{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "[]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas import json_normalize  \n",
    "# import pandas as pd\n",
    "# import sys\n",
    "# sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\custom_python\")\n",
    "# sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\src\")\n",
    "# from silvhua import *\n",
    "# from datetime import datetime\n",
    "# import json\n",
    "# from plotly.subplots import make_subplots\n",
    "# import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "import IPython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.json\") as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "# ig_user_id = credentials['am_ig_user_id']\n",
    "# access_token = credentials['am_ig_access_token']\n",
    "# ig_user_id = credentials['ig_user_id']\n",
    "# access_token = credentials['access_token']\n",
    "openai.api_key = credentials['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "module"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'api_openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\notebooks\\2023-03-21 ChatGPT API.ipynb Cell 7\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%20ChatGPT%20API.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m os\u001b[39m.\u001b[39;49menviron[\u001b[39m\"\u001b[39;49m\u001b[39mapi_openai\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\openaiEnv\\lib\\os.py:679\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    676\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencodekey(key)]\n\u001b[0;32m    677\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m    678\u001b[0m     \u001b[39m# raise KeyError with the original key value\u001b[39;00m\n\u001b[1;32m--> 679\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    680\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecodevalue(value)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'api_openai'"
     ]
    }
   ],
   "source": [
    "os.environ[\"api_openai\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Create the initial code with ChatGPT UI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 1: \"Use Python to write a function to use the ChatGPT API completions endpoint.\"\n",
    "\n",
    "### Response:\n",
    "Certainly, here's an example Python function that uses the OpenAI API to generate text completions based on a prompt:\n",
    "```python\n",
    "import openai\n",
    "import json\n",
    "\n",
    "# Set up the API key\n",
    "openai.api_key = \"YOUR_API_KEY\"\n",
    "\n",
    "# Define a function to generate completions for a given prompt\n",
    "def generate_completion(prompt, max_tokens=50, temperature=0.5):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"davinci\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    if response.choices[0].text:\n",
    "        return response.choices[0].text.strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Hello, I am ChatGPT. Please complete this sentence:\"\n",
    "completion = generate_completion(prompt, max_tokens=20, temperature=0.7)\n",
    "print(completion)\n",
    "```\n",
    "In this example, the function takes a prompt argument as input, along with optional arguments for max_tokens and temperature (which control the length and creativity of the generated text, respectively). The function then sends a request to the OpenAI API using the openai.Completion.create() method, which generates one possible completion for the given prompt using the specified parameters. Finally, the function returns the generated text as a string.\n",
    "\n",
    "Note that you'll need to replace YOUR_API_KEY with your actual OpenAI API key for this code to work. You can obtain an API key by signing up for the OpenAI API at https://beta.openai.com/signup/."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 2: \"The Engines endpoints are deprecated. Please use their replacement, Models, instead.\"\n",
    "\n",
    "### Response\n",
    "You're right, the Engines endpoints have been deprecated and replaced by the Models API in OpenAI. Here's an updated version of the code using the Models API:\n",
    "```python\n",
    "import openai\n",
    "import json\n",
    "\n",
    "# Set up the API key\n",
    "openai.api_key = \"YOUR_API_KEY\"\n",
    "\n",
    "# Define a function to generate completions for a given prompt\n",
    "def generate_completion(prompt, model, max_tokens=50, temperature=0.5):\n",
    "    response = openai.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    if response.choices[0].text:\n",
    "        return response.choices[0].text.strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Hello, I am ChatGPT. Please complete this sentence:\"\n",
    "completion = generate_completion(prompt, model=\"text-davinci-002\", max_tokens=20, temperature=0.7)\n",
    "print(completion)\n",
    "\n",
    "```\n",
    "In this updated code, we use the openai.Completion.create() method to send a request to the Models API, passing in the model argument instead of the deprecated engine argument. We also need to specify the ID of the model we want to use; in this example, we're using the \"text-davinci-002\" model, which is one of OpenAI's most advanced language models.\n",
    "\n",
    "Other than these changes, the function works the same way as before, generating text completions for a given prompt. Note that you'll still need to replace YOUR_API_KEY with your actual OpenAI API key for this code to work."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Test the code (`generate_completion` function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['babbage', 'davinci', 'gpt-3.5-turbo', 'babbage-code-search-code', 'text-similarity-babbage-001', 'text-davinci-001', 'ada', 'curie-instruct-beta', 'gpt-3.5-turbo-0301', 'babbage-code-search-text', 'babbage-similarity', 'whisper-1', 'canary-babbage', 'code-search-babbage-text-001', 'text-curie-001', 'canary-babbage-c6', 'code-cushman-001', 'code-search-babbage-code-001', 'text-davinci-insert-001', 'canary-ada-c6', 'text-ada-001', 'text-embedding-ada-002', 'text-similarity-ada-001', 'text-davinci-insert-002', 'code-davinci-002', 'ada-code-search-code', 'canary-ada', 'ada-similarity', 'text-davinci-003', 'code-search-ada-text-001', 'text-search-ada-query-001', 'davinci-search-document', 'ada-code-search-text', 'text-search-ada-doc-001', 'davinci-instruct-beta', 'text-similarity-curie-001', 'code-search-ada-code-001', 'ada-search-query', 'text-search-davinci-query-001', 'curie-search-query', 'davinci-search-query', 'babbage-search-document', 'ada-search-document', 'text-search-curie-query-001', 'text-search-babbage-doc-001', 'curie-search-document', 'text-search-curie-doc-001', 'babbage-search-query', 'text-babbage-001', 'code-davinci-edit-001', 'text-search-davinci-doc-001', 'text-search-babbage-query-001', 'curie-similarity', 'curie', 'text-davinci-edit-001', 'text-similarity-davinci-001', 'text-davinci-002', 'davinci-similarity', 'cushman:2020-05-03', 'ada:2020-05-03', 'babbage:2020-05-03', 'curie:2020-05-03', 'davinci:2020-05-03', 'if-davinci-v2', 'if-curie-v2', 'if-davinci:3.0.0', 'davinci-if:3.0.0', 'davinci-instruct-beta:2.0.0', 'text-ada:001', 'text-davinci:001', 'text-curie:001', 'text-babbage:001']\n",
      "',\n",
      "  'BotEngine.ChatGPT, what time will our chat session end today?\n"
     ]
    }
   ],
   "source": [
    "# Define a function to generate completions for a given prompt using the specified model\n",
    "def generate_completion(prompt, model, max_tokens=50, temperature=0.5):\n",
    "    response = openai.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    if response.choices[0].text:\n",
    "        return response.choices[0].text.strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Define a function to list the available models in the OpenAI API\n",
    "def list_models():\n",
    "    models = openai.Model.list()\n",
    "    return [model.id for model in models[\"data\"]]\n",
    "\n",
    "# Example usage\n",
    "available_models = list_models()\n",
    "print(\"Available models:\", available_models)\n",
    "\n",
    "prompt = \"ChatGPT, what time did we start our chat session today?\"\n",
    "model = \"text-davinci-002\"\n",
    "max_tokens = 20\n",
    "temperature = 0.7\n",
    "completion = generate_completion(prompt, model, max_tokens, temperature)\n",
    "print(completion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ada',\n",
       " 'ada-code-search-code',\n",
       " 'ada-code-search-text',\n",
       " 'ada-search-document',\n",
       " 'ada-search-query',\n",
       " 'ada-similarity',\n",
       " 'ada:2020-05-03',\n",
       " 'babbage',\n",
       " 'babbage-code-search-code',\n",
       " 'babbage-code-search-text',\n",
       " 'babbage-search-document',\n",
       " 'babbage-search-query',\n",
       " 'babbage-similarity',\n",
       " 'babbage:2020-05-03',\n",
       " 'code-cushman-001',\n",
       " 'code-davinci-002',\n",
       " 'code-davinci-edit-001',\n",
       " 'code-search-ada-code-001',\n",
       " 'code-search-ada-text-001',\n",
       " 'code-search-babbage-code-001',\n",
       " 'code-search-babbage-text-001',\n",
       " 'curie',\n",
       " 'curie-instruct-beta',\n",
       " 'curie-search-document',\n",
       " 'curie-search-query',\n",
       " 'curie-similarity',\n",
       " 'curie:2020-05-03',\n",
       " 'cushman:2020-05-03',\n",
       " 'davinci',\n",
       " 'davinci-if:3.0.0',\n",
       " 'davinci-instruct-beta',\n",
       " 'davinci-instruct-beta:2.0.0',\n",
       " 'davinci-search-document',\n",
       " 'davinci-search-query',\n",
       " 'davinci-similarity',\n",
       " 'davinci:2020-05-03',\n",
       " 'gpt-3.5-turbo',\n",
       " 'gpt-3.5-turbo-0301',\n",
       " 'if-curie-v2',\n",
       " 'if-davinci-v2',\n",
       " 'if-davinci:3.0.0',\n",
       " 'text-ada-001',\n",
       " 'text-ada:001',\n",
       " 'text-babbage-001',\n",
       " 'text-babbage:001',\n",
       " 'text-curie-001',\n",
       " 'text-curie:001',\n",
       " 'text-davinci-001',\n",
       " 'text-davinci-002',\n",
       " 'text-davinci-003',\n",
       " 'text-davinci-edit-001',\n",
       " 'text-davinci-insert-001',\n",
       " 'text-davinci-insert-002',\n",
       " 'text-davinci:001',\n",
       " 'text-embedding-ada-002',\n",
       " 'text-search-ada-doc-001',\n",
       " 'text-search-ada-query-001',\n",
       " 'text-search-babbage-doc-001',\n",
       " 'text-search-babbage-query-001',\n",
       " 'text-search-curie-doc-001',\n",
       " 'text-search-curie-query-001',\n",
       " 'text-search-davinci-doc-001',\n",
       " 'text-search-davinci-query-001',\n",
       " 'text-similarity-ada-001',\n",
       " 'text-similarity-babbage-001',\n",
       " 'text-similarity-curie-001',\n",
       " 'text-similarity-davinci-001',\n",
       " 'whisper-1']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model for model in sorted(available_models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\notebooks\\2023-03-21 1736 ChatGPT API.ipynb Cell 14\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m max_tokens \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m temperature \u001b[39m=\u001b[39m \u001b[39m0.7\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m completion \u001b[39m=\u001b[39m generate_completion(prompt, model, max_tokens, temperature)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(completion)\n",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\notebooks\\2023-03-21 1736 ChatGPT API.ipynb Cell 14\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_completion\u001b[39m(prompt, model, max_tokens\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, temperature\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         max_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         n\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         stop\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         frequency_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         presence_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\openaiEnv\\lib\\site-packages\\openai\\api_resources\\completion.py:31\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     33\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\openaiEnv\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:100\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m     92\u001b[0m requestor \u001b[39m=\u001b[39m api_requestor\u001b[39m.\u001b[39mAPIRequestor(\n\u001b[0;32m     93\u001b[0m     api_key,\n\u001b[0;32m     94\u001b[0m     api_base\u001b[39m=\u001b[39mapi_base,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m     organization\u001b[39m=\u001b[39morganization,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m     99\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[1;32m--> 100\u001b[0m response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    101\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    102\u001b[0m     url,\n\u001b[0;32m    103\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    104\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    105\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    106\u001b[0m     request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    107\u001b[0m )\n\u001b[0;32m    109\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    110\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)  \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\openaiEnv\\lib\\site-packages\\openai\\api_requestor.py:122\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    104\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    105\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m     request_id: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    112\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    113\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    114\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    115\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m         request_id\u001b[39m=\u001b[39mrequest_id,\n\u001b[0;32m    121\u001b[0m     )\n\u001b[1;32m--> 122\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    123\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\openaiEnv\\lib\\site-packages\\openai\\api_requestor.py:329\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    322\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    323\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    324\u001b[0m         )\n\u001b[0;32m    325\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    326\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    328\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 329\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    330\u001b[0m             result\u001b[39m.\u001b[39;49mcontent, result\u001b[39m.\u001b[39;49mstatus_code, result\u001b[39m.\u001b[39;49mheaders, stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    331\u001b[0m         ),\n\u001b[0;32m    332\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    333\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\openaiEnv\\lib\\site-packages\\openai\\api_requestor.py:362\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    360\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    361\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 362\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    363\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    364\u001b[0m     )\n\u001b[0;32m    365\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?"
     ]
    }
   ],
   "source": [
    "prompt = \"ChatGPT, what time did we start our chat session today?\"\n",
    "model = 'gpt-3.5-turbo'\n",
    "max_tokens = 20\n",
    "temperature = 0.7\n",
    "completion = generate_completion(prompt, model, max_tokens, temperature)\n",
    "print(completion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [the Open AI documentation](https://platform.openai.com/docs/models/codex):\n",
    "- The model that is optimized for generating code is `code-davinci-002`\n",
    "- The model that is recommended for most use cases is GPT-3.5. 'gpt-3.5-turbo' is the latest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Modify the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'ChatCompletion'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\notebooks\\2023-03-21 1736 ChatGPT API.ipynb Cell 17\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# # Define a function to list the available models in the OpenAI API\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# def list_models():\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m#     models = openai.Model.list()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# available_models = list_models()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# print(\"Available models:\", available_models)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mChatGPT, what is today\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms date?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m response_text, response_object \u001b[39m=\u001b[39m generate_chat_completion(prompt)\n",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\notebooks\\2023-03-21 1736 ChatGPT API.ipynb Cell 17\u001b[0m in \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_chat_completion\u001b[39m(prompt, max_tokens\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, temperature\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m, model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     message_list \u001b[39m=\u001b[39m [\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: prompt}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     ]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         messages\u001b[39m=\u001b[39mmessage_list,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         max_tokens\u001b[39m=\u001b[39mmax_tokens,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         temperature\u001b[39m=\u001b[39mtemperature,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         n\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         stop\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         frequency_penalty\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         presence_penalty\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X60sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         IPython\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mMarkdown(prompt)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'openai' has no attribute 'ChatCompletion'"
     ]
    }
   ],
   "source": [
    "# Define a function to generate completions for a given prompt using the specified model\n",
    "def generate_chat_completion(prompt, max_tokens=50, temperature=0.7, model='gpt-3.5-turbo'):\n",
    "\n",
    "    message_list = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=message_list,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    if response.choices[0].text:\n",
    "        IPython.display.Markdown(prompt)\n",
    "        return response.choices[0].text.strip(), response\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "# # Define a function to list the available models in the OpenAI API\n",
    "# def list_models():\n",
    "#     models = openai.Model.list()\n",
    "#     return [model.id for model in models[\"data\"]]\n",
    "\n",
    "# # Example usage\n",
    "# available_models = list_models()\n",
    "# print(\"Available models:\", available_models)\n",
    "\n",
    "prompt = \"ChatGPT, what is today's date?\"\n",
    "response_text, response_object = generate_chat_completion(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['babbage', 'davinci', 'gpt-3.5-turbo', 'babbage-code-search-code', 'text-similarity-babbage-001', 'text-davinci-001', 'ada', 'curie-instruct-beta', 'gpt-3.5-turbo-0301', 'babbage-code-search-text', 'babbage-similarity', 'whisper-1', 'canary-babbage', 'code-search-babbage-text-001', 'text-curie-001', 'canary-babbage-c6', 'code-cushman-001', 'code-search-babbage-code-001', 'text-davinci-insert-001', 'canary-ada-c6', 'text-ada-001', 'text-embedding-ada-002', 'text-similarity-ada-001', 'text-davinci-insert-002', 'code-davinci-002', 'ada-code-search-code', 'canary-ada', 'ada-similarity', 'text-davinci-003', 'code-search-ada-text-001', 'text-search-ada-query-001', 'davinci-search-document', 'ada-code-search-text', 'text-search-ada-doc-001', 'davinci-instruct-beta', 'text-similarity-curie-001', 'code-search-ada-code-001', 'ada-search-query', 'text-search-davinci-query-001', 'curie-search-query', 'davinci-search-query', 'babbage-search-document', 'ada-search-document', 'text-search-curie-query-001', 'text-search-babbage-doc-001', 'curie-search-document', 'text-search-curie-doc-001', 'babbage-search-query', 'text-babbage-001', 'code-davinci-edit-001', 'text-search-davinci-doc-001', 'text-search-babbage-query-001', 'curie-similarity', 'curie', 'text-davinci-edit-001', 'text-similarity-davinci-001', 'text-davinci-002', 'davinci-similarity', 'cushman:2020-05-03', 'ada:2020-05-03', 'babbage:2020-05-03', 'curie:2020-05-03', 'davinci:2020-05-03', 'if-davinci-v2', 'if-curie-v2', 'if-davinci:3.0.0', 'davinci-if:3.0.0', 'davinci-instruct-beta:2.0.0', 'text-ada:001', 'text-davinci:001', 'text-curie:001', 'text-babbage:001']\n"
     ]
    }
   ],
   "source": [
    "# Define a function to list the available models in the OpenAI API\n",
    "def list_models():\n",
    "    models = openai.Model.list()\n",
    "    return [model.id for model in models[\"data\"]]\n",
    "\n",
    "# Example usage\n",
    "available_models = list_models()\n",
    "print(\"Available models:\", available_models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 18:30:24.131985\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Today's date is October 10th."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to generate completions for a given prompt using the specified model\n",
    "from datetime import datetime\n",
    "def generate_completion(prompt, max_tokens=50, temperature=0.0, model=\"text-davinci-002\"):\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    if response.choices[0].text:\n",
    "        print('Time completed:', datetime.now())\n",
    "        response_string = response.choices[0].text.strip()\n",
    "        return response_string, response, IPython.display.Markdown(response_string)\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "# # Define a function to list the available models in the OpenAI API\n",
    "# def list_models():\n",
    "#     models = openai.Model.list()\n",
    "#     return [model.id for model in models[\"data\"]]\n",
    "\n",
    "# # Example usage\n",
    "# available_models = list_models()\n",
    "# print(\"Available models:\", available_models)\n",
    "\n",
    "prompt = \"ChatGPT, what is today's date?\"\n",
    "response_text, response_object, response_printout = generate_completion(prompt)\n",
    "response_printout\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 3\n",
    "I modified the function so that each API request would be appended to a dictionary of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 18:41:44.670862\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "?\n",
       "\n",
       "The capital city of Canada is Ottawa."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to generate completions for a given prompt using the specified model\n",
    "from datetime import datetime\n",
    "def generate_completion(prompt, history, max_tokens=50, temperature=0.0, model=\"text-davinci-002\"):\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    if response.choices[0].text:\n",
    "        print('Time completed:', datetime.now())\n",
    "        response_string = response.choices[0].text.strip()\n",
    "        try:\n",
    "            history['response_test'].append(response_string)\n",
    "            history['response'].append(response)\n",
    "            history['response_MD'].append(IPython.display.Markdown(response_string))\n",
    "        except:\n",
    "            history['response_test'] = [response_string]\n",
    "            history['response'] = [response]\n",
    "            history['response_MD'] = [IPython.display.Markdown(response_string)]\n",
    "        return history\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "\n",
    "prompt = \"What is the capital city of Canada\"\n",
    "history = generate_completion(prompt, dict())\n",
    "history['response_MD'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 18:42:27.047073\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital city of British Columbia\"\n",
    "history = generate_completion(prompt, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "?\n",
       "\n",
       "The capital city of Canada is Ottawa."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history['response_MD'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "?\n",
       "\n",
       "The capital city of British Columbia is Victoria."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history['response_MD'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(history['response_MD'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 4\n",
    "Modify the function to include the prompt in the output dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 18:45:03.592954\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Eileen Regina Edwards"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "def generate_completion(prompt, history, max_tokens=50, temperature=0.0, model=\"text-davinci-002\"):\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    if response.choices[0].text:\n",
    "        print('Time completed:', datetime.now())\n",
    "        response_string = response.choices[0].text.strip()\n",
    "        try:\n",
    "            history['prompt'].append(prompt)\n",
    "            history['response_test'].append(response_string)\n",
    "            history['response'].append(response)\n",
    "            history['response_MD'].append(IPython.display.Markdown(response_string))\n",
    "        except:\n",
    "            history['prompt'] = [prompt]\n",
    "            history['response_test'] = [response_string]\n",
    "            history['response'] = [response]\n",
    "            history['response_MD'] = [IPython.display.Markdown(response_string)]\n",
    "        return history\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "\n",
    "prompt = \"What is Shania Twain's birth name?\"\n",
    "history2 = generate_completion(prompt, dict())\n",
    "history2['response_MD'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The best exercise for back pain is the McKenzie exercise."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Explain in one sentence what is the best exercise for backpain?\"\n",
    "history2 = generate_completion(prompt, history2)\n",
    "history2['response_MD'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-6whq1X35Qz8hzcyXWTfdilGEaFInm at 0x2670cb47ef0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"\\n\\nThe best exercise for back pain is the McKenzie exercise.\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1679449557,\n",
       "  \"id\": \"cmpl-6whq1X35Qz8hzcyXWTfdilGEaFInm\",\n",
       "  \"model\": \"text-davinci-002\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 13,\n",
       "    \"prompt_tokens\": 14,\n",
       "    \"total_tokens\": 27\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history2['response'][1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 5\n",
    "Modify the max_tokens parameter to allow larger responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 19:15:48.037746\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Eileen Regina Edwards"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "def generate_completion(prompt, history, temperature=0.0, model=\"text-davinci-002\"):\n",
    "    max_tokens= int(4096 - len(prompt)/4)\n",
    "    response = openai.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    if response.choices[0].text:\n",
    "        print('Time completed:', datetime.now())\n",
    "        response_string = response.choices[0].text.strip()\n",
    "        try:\n",
    "            history['prompt'].append(prompt)\n",
    "            history['response_text'].append(response_string)\n",
    "            history['response'].append(response)\n",
    "            history['response_MD'].append(IPython.display.Markdown(response_string))\n",
    "        except:\n",
    "            history['prompt'] = [prompt]\n",
    "            history['response_text'] = [response_string]\n",
    "            history['response'] = [response]\n",
    "            history['response_MD'] = [IPython.display.Markdown(response_string)]\n",
    "        return history\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "\n",
    "prompt = \"What is Shania Twain's birth name?\"\n",
    "history2 = generate_completion(prompt, dict())\n",
    "history2['response_MD'][0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Use ChatGPT to complete a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 19:16:09.985057\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "reate a python function that will make a GET request to pull posts from a \n",
    "Facebook group using the Facebook API.\n",
    "\n",
    "The function should return the following:\n",
    "1. posts: A DataFrame with each row containining the post_id, created_time, message, comments_count, and \n",
    "reactions count for each post. Give each reaction its own column.\n",
    "2. comments: A DataFrame with the comments for each post_id in the posts DataFrame.\n",
    "Each row should have the post_id, comment time stamp, comment message, and comment id.\n",
    "3. response.json()\n",
    "\n",
    "Include parameters to limit the number of posts to retrieve.\n",
    "Use as few API requests as possible\n",
    "\n",
    "\"\"\"\n",
    "history3 = dict()\n",
    "history3 = generate_completion(prompt, history3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "def get_fb_posts(group_id, n_posts, access_token):\n",
       "    \"\"\"\n",
       "    Retrieves n_posts from a Facebook group with group_id.\n",
       "    \n",
       "    Parameters\n",
       "    ----------\n",
       "    group_id : str\n",
       "        The id of the Facebook group\n",
       "    n_posts : int\n",
       "        The number of posts to retrieve\n",
       "    access_token : str\n",
       "        The access token to use to make the request\n",
       "        \n",
       "    Returns\n",
       "    -------\n",
       "    posts : DataFrame\n",
       "        A DataFrame with each row containining the post_id, created_time, message, comments_count, and \n",
       "        reactions count for each post. Give each reaction its own column.\n",
       "    comments : DataFrame\n",
       "        A DataFrame with the comments for each post_id in the posts DataFrame.\n",
       "        Each row should have the post_id, comment time stamp, comment message, and comment id.\n",
       "    response.json()\n",
       "        The json response from the request\n",
       "    \"\"\"\n",
       "    \n",
       "    # YOUR CODE HERE\n",
       "    \n",
       "    return posts, comments, response.json()"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history3['response_MD'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 19:37:55.456280\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Fill in the code for the function\n",
    "\"\"\"\n",
    "\n",
    "history3 = generate_completion(prompt, history3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "def get_grade(score1, score2, score3):\n",
       "\n",
       "if score1 >= 90 and score2 >= 90 and score3 >= 90:\n",
       "\n",
       "return \"A\"\n",
       "\n",
       "elif score1 >= 80 and score2 >= 80 and score3 >= 80:\n",
       "\n",
       "return \"B\"\n",
       "\n",
       "elif score1 >= 70 and score2 >= 70 and score3 >= 70:\n",
       "\n",
       "return \"C\"\n",
       "\n",
       "else:\n",
       "\n",
       "return \"D\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history3['response_MD'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens, however you requested 4105 tokens (248 in your prompt; 3857 for the completion). Please reduce your prompt; or completion length.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\notebooks\\2023-03-21 1736 ChatGPT API.ipynb Cell 38\u001b[0m in \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#Y115sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#Y115sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mFill in the code for the function:\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#Y115sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m{\u001b[39;00mhistory3[\u001b[39m'\u001b[39m\u001b[39mresponse_text\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#Y115sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#Y115sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m history3 \u001b[39m=\u001b[39m generate_completion(prompt, history3)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#Y115sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m history3[\u001b[39m'\u001b[39m\u001b[39mresponse_MD\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m2\u001b[39m]\n",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\notebooks\\2023-03-21 1736 ChatGPT API.ipynb Cell 38\u001b[0m in \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#Y115sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_completion\u001b[39m(prompt, history, temperature\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext-davinci-002\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#Y115sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     max_tokens\u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m4096\u001b[39m \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(prompt)\u001b[39m/\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#Y115sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#Y115sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#Y115sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#Y115sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         max_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#Y115sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#Y115sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         n\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#Y115sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         stop\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#Y115sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         frequency_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#Y115sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         presence_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#Y115sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#Y115sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#Y115sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTime completed:\u001b[39m\u001b[39m'\u001b[39m, datetime\u001b[39m.\u001b[39mnow())\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\openaiEnv\\lib\\site-packages\\openai\\api_resources\\completion.py:31\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     33\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\openaiEnv\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:100\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m     92\u001b[0m requestor \u001b[39m=\u001b[39m api_requestor\u001b[39m.\u001b[39mAPIRequestor(\n\u001b[0;32m     93\u001b[0m     api_key,\n\u001b[0;32m     94\u001b[0m     api_base\u001b[39m=\u001b[39mapi_base,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m     organization\u001b[39m=\u001b[39morganization,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m     99\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[1;32m--> 100\u001b[0m response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    101\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    102\u001b[0m     url,\n\u001b[0;32m    103\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    104\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    105\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    106\u001b[0m     request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    107\u001b[0m )\n\u001b[0;32m    109\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    110\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)  \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\openaiEnv\\lib\\site-packages\\openai\\api_requestor.py:122\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    104\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    105\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m     request_id: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    112\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    113\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    114\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    115\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m         request_id\u001b[39m=\u001b[39mrequest_id,\n\u001b[0;32m    121\u001b[0m     )\n\u001b[1;32m--> 122\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    123\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\openaiEnv\\lib\\site-packages\\openai\\api_requestor.py:329\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    322\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    323\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    324\u001b[0m         )\n\u001b[0;32m    325\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    326\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    328\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 329\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    330\u001b[0m             result\u001b[39m.\u001b[39;49mcontent, result\u001b[39m.\u001b[39;49mstatus_code, result\u001b[39m.\u001b[39;49mheaders, stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    331\u001b[0m         ),\n\u001b[0;32m    332\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    333\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\openaiEnv\\lib\\site-packages\\openai\\api_requestor.py:362\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    360\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    361\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 362\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    363\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    364\u001b[0m     )\n\u001b[0;32m    365\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens, however you requested 4105 tokens (248 in your prompt; 3857 for the completion). Please reduce your prompt; or completion length."
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Fill in the code for the function:\n",
    "{history3['response_text'][0]}\n",
    "\"\"\"\n",
    "\n",
    "history3 = generate_completion(prompt, history3)\n",
    "history3['response_MD'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
