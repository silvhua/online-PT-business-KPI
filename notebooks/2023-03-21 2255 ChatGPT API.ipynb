{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The goal was to use the Open AI API to interface in lieu of the ChatGPT web app. \n",
    "\n",
    "I was able to use the chat completions endpoint, but ultimately decided to use the ChatGPT web app for the following reasons:\n",
    "* The [ChatCompletion endpoint](https://platform.openai.com/docs/api-reference/chat) is the most powerful endpoint so that the responses would account for chat history. However, I was unable to access this endpoint due to `AttributeError: module 'openai' has no attribute 'ChatCompletion'`. This is perhaps because I have a free Open AI account.\n",
    "* I ran into token limits when trying to generate code, which resulted in incomplete functions. The [Open AI documentation](https://platform.openai.com/docs/guides/chat/chat-vs-completions) \"Because gpt-3.5-turbo performs at a similar capability to text-davinci-003 but at 10% the price per token, we recommend gpt-3.5-turbo for most use cases.\" When I tried to use the gpt-3.5-turbo model, I got `InvalidRequestError: This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?`\n",
    "\n",
    "    With the ChatGPT web app, I was able to generate complete code without major issues.\n",
    "\n",
    "This notebook shows what I did to generate code to use the Open AI API. I used the ChatGPT web app to help me with this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "import IPython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.json\") as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "openai.api_key = credentials['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "module"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(openai)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Create the initial code with ChatGPT UI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 1: \"Use Python to write a function to use the ChatGPT API completions endpoint.\"\n",
    "\n",
    "### Response:\n",
    "Certainly, here's an example Python function that uses the OpenAI API to generate text completions based on a prompt:\n",
    "```python\n",
    "import openai\n",
    "import json\n",
    "\n",
    "# Set up the API key\n",
    "openai.api_key = \"YOUR_API_KEY\"\n",
    "\n",
    "# Define a function to generate completions for a given prompt\n",
    "def generate_completion(prompt, max_tokens=50, temperature=0.5):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"davinci\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    if response.choices[0].text:\n",
    "        return response.choices[0].text.strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Hello, I am ChatGPT. Please complete this sentence:\"\n",
    "completion = generate_completion(prompt, max_tokens=20, temperature=0.7)\n",
    "print(completion)\n",
    "```\n",
    "In this example, the function takes a prompt argument as input, along with optional arguments for max_tokens and temperature (which control the length and creativity of the generated text, respectively). The function then sends a request to the OpenAI API using the openai.Completion.create() method, which generates one possible completion for the given prompt using the specified parameters. Finally, the function returns the generated text as a string.\n",
    "\n",
    "Note that you'll need to replace YOUR_API_KEY with your actual OpenAI API key for this code to work. You can obtain an API key by signing up for the OpenAI API at https://beta.openai.com/signup/."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 2: \"The Engines endpoints are deprecated. Please use their replacement, Models, instead.\"\n",
    "\n",
    "### Response\n",
    "You're right, the Engines endpoints have been deprecated and replaced by the Models API in OpenAI. Here's an updated version of the code using the Models API:\n",
    "```python\n",
    "import openai\n",
    "import json\n",
    "\n",
    "# Set up the API key\n",
    "openai.api_key = \"YOUR_API_KEY\"\n",
    "\n",
    "# Define a function to generate completions for a given prompt\n",
    "def generate_completion(prompt, model, max_tokens=50, temperature=0.5):\n",
    "    response = openai.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    if response.choices[0].text:\n",
    "        return response.choices[0].text.strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Hello, I am ChatGPT. Please complete this sentence:\"\n",
    "completion = generate_completion(prompt, model=\"text-davinci-002\", max_tokens=20, temperature=0.7)\n",
    "print(completion)\n",
    "\n",
    "```\n",
    "In this updated code, we use the openai.Completion.create() method to send a request to the Models API, passing in the model argument instead of the deprecated engine argument. We also need to specify the ID of the model we want to use; in this example, we're using the \"text-davinci-002\" model, which is one of OpenAI's most advanced language models.\n",
    "\n",
    "Other than these changes, the function works the same way as before, generating text completions for a given prompt. Note that you'll still need to replace YOUR_API_KEY with your actual OpenAI API key for this code to work."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Test the code (`generate_completion` function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['babbage', 'davinci', 'gpt-3.5-turbo', 'babbage-code-search-code', 'text-similarity-babbage-001', 'text-davinci-001', 'ada', 'curie-instruct-beta', 'gpt-3.5-turbo-0301', 'babbage-code-search-text', 'babbage-similarity', 'whisper-1', 'canary-babbage', 'code-search-babbage-text-001', 'text-curie-001', 'canary-babbage-c6', 'code-cushman-001', 'code-search-babbage-code-001', 'text-davinci-insert-001', 'canary-ada-c6', 'text-ada-001', 'text-embedding-ada-002', 'text-similarity-ada-001', 'text-davinci-insert-002', 'code-davinci-002', 'ada-code-search-code', 'canary-ada', 'ada-similarity', 'text-davinci-003', 'code-search-ada-text-001', 'text-search-ada-query-001', 'davinci-search-document', 'ada-code-search-text', 'text-search-ada-doc-001', 'davinci-instruct-beta', 'text-similarity-curie-001', 'code-search-ada-code-001', 'ada-search-query', 'text-search-davinci-query-001', 'curie-search-query', 'davinci-search-query', 'babbage-search-document', 'ada-search-document', 'text-search-curie-query-001', 'text-search-babbage-doc-001', 'curie-search-document', 'text-search-curie-doc-001', 'babbage-search-query', 'text-babbage-001', 'code-davinci-edit-001', 'text-search-davinci-doc-001', 'text-search-babbage-query-001', 'curie-similarity', 'curie', 'text-davinci-edit-001', 'text-similarity-davinci-001', 'text-davinci-002', 'davinci-similarity', 'cushman:2020-05-03', 'ada:2020-05-03', 'babbage:2020-05-03', 'curie:2020-05-03', 'davinci:2020-05-03', 'if-davinci-v2', 'if-curie-v2', 'if-davinci:3.0.0', 'davinci-if:3.0.0', 'davinci-instruct-beta:2.0.0', 'text-ada:001', 'text-davinci:001', 'text-curie:001', 'text-babbage:001']\n",
      "',\n",
      "  'BotEngine.ChatGPT, what time will our chat session end today?\n"
     ]
    }
   ],
   "source": [
    "# Define a function to generate completions for a given prompt using the specified model\n",
    "def generate_completion(prompt, model, max_tokens=50, temperature=0.5):\n",
    "    response = openai.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    if response.choices[0].text:\n",
    "        return response.choices[0].text.strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Define a function to list the available models in the OpenAI API\n",
    "def list_models():\n",
    "    models = openai.Model.list()\n",
    "    return [model.id for model in models[\"data\"]]\n",
    "\n",
    "# Example usage\n",
    "available_models = list_models()\n",
    "print(\"Available models:\", available_models)\n",
    "\n",
    "prompt = \"ChatGPT, what time did we start our chat session today?\"\n",
    "model = \"text-davinci-002\"\n",
    "max_tokens = 20\n",
    "temperature = 0.7\n",
    "completion = generate_completion(prompt, model, max_tokens, temperature)\n",
    "print(completion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ada',\n",
       " 'ada-code-search-code',\n",
       " 'ada-code-search-text',\n",
       " 'ada-search-document',\n",
       " 'ada-search-query',\n",
       " 'ada-similarity',\n",
       " 'ada:2020-05-03',\n",
       " 'babbage',\n",
       " 'babbage-code-search-code',\n",
       " 'babbage-code-search-text',\n",
       " 'babbage-search-document',\n",
       " 'babbage-search-query',\n",
       " 'babbage-similarity',\n",
       " 'babbage:2020-05-03',\n",
       " 'code-cushman-001',\n",
       " 'code-davinci-002',\n",
       " 'code-davinci-edit-001',\n",
       " 'code-search-ada-code-001',\n",
       " 'code-search-ada-text-001',\n",
       " 'code-search-babbage-code-001',\n",
       " 'code-search-babbage-text-001',\n",
       " 'curie',\n",
       " 'curie-instruct-beta',\n",
       " 'curie-search-document',\n",
       " 'curie-search-query',\n",
       " 'curie-similarity',\n",
       " 'curie:2020-05-03',\n",
       " 'cushman:2020-05-03',\n",
       " 'davinci',\n",
       " 'davinci-if:3.0.0',\n",
       " 'davinci-instruct-beta',\n",
       " 'davinci-instruct-beta:2.0.0',\n",
       " 'davinci-search-document',\n",
       " 'davinci-search-query',\n",
       " 'davinci-similarity',\n",
       " 'davinci:2020-05-03',\n",
       " 'gpt-3.5-turbo',\n",
       " 'gpt-3.5-turbo-0301',\n",
       " 'if-curie-v2',\n",
       " 'if-davinci-v2',\n",
       " 'if-davinci:3.0.0',\n",
       " 'text-ada-001',\n",
       " 'text-ada:001',\n",
       " 'text-babbage-001',\n",
       " 'text-babbage:001',\n",
       " 'text-curie-001',\n",
       " 'text-curie:001',\n",
       " 'text-davinci-001',\n",
       " 'text-davinci-002',\n",
       " 'text-davinci-003',\n",
       " 'text-davinci-edit-001',\n",
       " 'text-davinci-insert-001',\n",
       " 'text-davinci-insert-002',\n",
       " 'text-davinci:001',\n",
       " 'text-embedding-ada-002',\n",
       " 'text-search-ada-doc-001',\n",
       " 'text-search-ada-query-001',\n",
       " 'text-search-babbage-doc-001',\n",
       " 'text-search-babbage-query-001',\n",
       " 'text-search-curie-doc-001',\n",
       " 'text-search-curie-query-001',\n",
       " 'text-search-davinci-doc-001',\n",
       " 'text-search-davinci-query-001',\n",
       " 'text-similarity-ada-001',\n",
       " 'text-similarity-babbage-001',\n",
       " 'text-similarity-curie-001',\n",
       " 'text-similarity-davinci-001',\n",
       " 'whisper-1']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model for model in sorted(available_models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\notebooks\\2023-03-21 1736 ChatGPT API.ipynb Cell 14\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m max_tokens \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m temperature \u001b[39m=\u001b[39m \u001b[39m0.7\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m completion \u001b[39m=\u001b[39m generate_completion(prompt, model, max_tokens, temperature)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(completion)\n",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\notebooks\\2023-03-21 1736 ChatGPT API.ipynb Cell 14\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_completion\u001b[39m(prompt, model, max_tokens\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, temperature\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         max_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         n\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         stop\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         frequency_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         presence_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%201736%20ChatGPT%20API.ipynb#X56sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\openaiEnv\\lib\\site-packages\\openai\\api_resources\\completion.py:31\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     33\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\openaiEnv\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:100\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m     92\u001b[0m requestor \u001b[39m=\u001b[39m api_requestor\u001b[39m.\u001b[39mAPIRequestor(\n\u001b[0;32m     93\u001b[0m     api_key,\n\u001b[0;32m     94\u001b[0m     api_base\u001b[39m=\u001b[39mapi_base,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m     organization\u001b[39m=\u001b[39morganization,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m     99\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[1;32m--> 100\u001b[0m response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    101\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    102\u001b[0m     url,\n\u001b[0;32m    103\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    104\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    105\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    106\u001b[0m     request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    107\u001b[0m )\n\u001b[0;32m    109\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    110\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)  \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\openaiEnv\\lib\\site-packages\\openai\\api_requestor.py:122\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    104\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    105\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m     request_id: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    112\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    113\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    114\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    115\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m         request_id\u001b[39m=\u001b[39mrequest_id,\n\u001b[0;32m    121\u001b[0m     )\n\u001b[1;32m--> 122\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    123\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\openaiEnv\\lib\\site-packages\\openai\\api_requestor.py:329\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    322\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    323\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    324\u001b[0m         )\n\u001b[0;32m    325\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    326\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    328\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 329\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    330\u001b[0m             result\u001b[39m.\u001b[39;49mcontent, result\u001b[39m.\u001b[39;49mstatus_code, result\u001b[39m.\u001b[39;49mheaders, stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    331\u001b[0m         ),\n\u001b[0;32m    332\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    333\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\openaiEnv\\lib\\site-packages\\openai\\api_requestor.py:362\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    360\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    361\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 362\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    363\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    364\u001b[0m     )\n\u001b[0;32m    365\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?"
     ]
    }
   ],
   "source": [
    "prompt = \"ChatGPT, what time did we start our chat session today?\"\n",
    "model = 'gpt-3.5-turbo'\n",
    "max_tokens = 20\n",
    "temperature = 0.7\n",
    "completion = generate_completion(prompt, model, max_tokens, temperature)\n",
    "print(completion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [the Open AI documentation](https://platform.openai.com/docs/models/codex):\n",
    "- The model that is optimized for generating code is `code-davinci-002`\n",
    "- The model that is recommended for most use cases is GPT-3.5. 'gpt-3.5-turbo' is the latest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Modify the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'ChatCompletion'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\notebooks\\2023-03-21 2255 ChatGPT API.ipynb Cell 16\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# # Define a function to list the available models in the OpenAI API\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# def list_models():\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m#     models = openai.Model.list()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# available_models = list_models()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# print(\"Available models:\", available_models)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mChatGPT, what is today\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms date?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m response_text, response_object \u001b[39m=\u001b[39m generate_chat_completion(prompt)\n",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\notebooks\\2023-03-21 2255 ChatGPT API.ipynb Cell 16\u001b[0m in \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_chat_completion\u001b[39m(prompt, max_tokens\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, temperature\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m, model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     message_list \u001b[39m=\u001b[39m [\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: prompt}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     ]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         messages\u001b[39m=\u001b[39mmessage_list,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         max_tokens\u001b[39m=\u001b[39mmax_tokens,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         temperature\u001b[39m=\u001b[39mtemperature,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         n\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         stop\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         frequency_penalty\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         presence_penalty\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         IPython\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mMarkdown(prompt)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'openai' has no attribute 'ChatCompletion'"
     ]
    }
   ],
   "source": [
    "# Define a function to generate completions for a given prompt using the specified model\n",
    "def generate_chat_completion(prompt, max_tokens=50, temperature=0.7, model='gpt-3.5-turbo'):\n",
    "\n",
    "    message_list = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=message_list,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    if response.choices[0].text:\n",
    "        IPython.display.Markdown(prompt)\n",
    "        return response.choices[0].text.strip(), response\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "# # Define a function to list the available models in the OpenAI API\n",
    "# def list_models():\n",
    "#     models = openai.Model.list()\n",
    "#     return [model.id for model in models[\"data\"]]\n",
    "\n",
    "# # Example usage\n",
    "# available_models = list_models()\n",
    "# print(\"Available models:\", available_models)\n",
    "\n",
    "prompt = \"ChatGPT, what is today's date?\"\n",
    "response_text, response_object = generate_chat_completion(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['babbage', 'davinci', 'gpt-3.5-turbo', 'babbage-code-search-code', 'text-similarity-babbage-001', 'text-davinci-001', 'ada', 'curie-instruct-beta', 'gpt-3.5-turbo-0301', 'babbage-code-search-text', 'babbage-similarity', 'whisper-1', 'canary-babbage', 'code-search-babbage-text-001', 'text-curie-001', 'canary-babbage-c6', 'code-cushman-001', 'code-search-babbage-code-001', 'text-davinci-insert-001', 'canary-ada-c6', 'text-ada-001', 'text-embedding-ada-002', 'text-similarity-ada-001', 'text-davinci-insert-002', 'code-davinci-002', 'ada-code-search-code', 'canary-ada', 'ada-similarity', 'text-davinci-003', 'code-search-ada-text-001', 'text-search-ada-query-001', 'davinci-search-document', 'ada-code-search-text', 'text-search-ada-doc-001', 'davinci-instruct-beta', 'text-similarity-curie-001', 'code-search-ada-code-001', 'ada-search-query', 'text-search-davinci-query-001', 'curie-search-query', 'davinci-search-query', 'babbage-search-document', 'ada-search-document', 'text-search-curie-query-001', 'text-search-babbage-doc-001', 'curie-search-document', 'text-search-curie-doc-001', 'babbage-search-query', 'text-babbage-001', 'code-davinci-edit-001', 'text-search-davinci-doc-001', 'text-search-babbage-query-001', 'curie-similarity', 'curie', 'text-davinci-edit-001', 'text-similarity-davinci-001', 'text-davinci-002', 'davinci-similarity', 'cushman:2020-05-03', 'ada:2020-05-03', 'babbage:2020-05-03', 'curie:2020-05-03', 'davinci:2020-05-03', 'if-davinci-v2', 'if-curie-v2', 'if-davinci:3.0.0', 'davinci-if:3.0.0', 'davinci-instruct-beta:2.0.0', 'text-ada:001', 'text-davinci:001', 'text-curie:001', 'text-babbage:001']\n"
     ]
    }
   ],
   "source": [
    "# Define a function to list the available models in the OpenAI API\n",
    "def list_models():\n",
    "    models = openai.Model.list()\n",
    "    return [model.id for model in models[\"data\"]]\n",
    "\n",
    "# Example usage\n",
    "available_models = list_models()\n",
    "print(\"Available models:\", available_models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 18:30:24.131985\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Today's date is October 10th."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to generate completions for a given prompt using the specified model\n",
    "from datetime import datetime\n",
    "def generate_completion(prompt, max_tokens=50, temperature=0.0, model=\"text-davinci-002\"):\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    if response.choices[0].text:\n",
    "        print('Time completed:', datetime.now())\n",
    "        response_string = response.choices[0].text.strip()\n",
    "        return response_string, response, IPython.display.Markdown(response_string)\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "# # Define a function to list the available models in the OpenAI API\n",
    "# def list_models():\n",
    "#     models = openai.Model.list()\n",
    "#     return [model.id for model in models[\"data\"]]\n",
    "\n",
    "# # Example usage\n",
    "# available_models = list_models()\n",
    "# print(\"Available models:\", available_models)\n",
    "\n",
    "prompt = \"ChatGPT, what is today's date?\"\n",
    "response_text, response_object, response_printout = generate_completion(prompt)\n",
    "response_printout\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 3\n",
    "I modified the function so that each API request would be appended to a dictionary of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 18:41:44.670862\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "?\n",
       "\n",
       "The capital city of Canada is Ottawa."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to generate completions for a given prompt using the specified model\n",
    "from datetime import datetime\n",
    "def generate_completion(prompt, history, max_tokens=50, temperature=0.0, model=\"text-davinci-002\"):\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    if response.choices[0].text:\n",
    "        print('Time completed:', datetime.now())\n",
    "        response_string = response.choices[0].text.strip()\n",
    "        try:\n",
    "            history['response_test'].append(response_string)\n",
    "            history['response'].append(response)\n",
    "            history['response_MD'].append(IPython.display.Markdown(response_string))\n",
    "        except:\n",
    "            history['response_test'] = [response_string]\n",
    "            history['response'] = [response]\n",
    "            history['response_MD'] = [IPython.display.Markdown(response_string)]\n",
    "        return history\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "\n",
    "prompt = \"What is the capital city of Canada\"\n",
    "history = generate_completion(prompt, dict())\n",
    "history['response_MD'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 18:42:27.047073\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital city of British Columbia\"\n",
    "history = generate_completion(prompt, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "?\n",
       "\n",
       "The capital city of Canada is Ottawa."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history['response_MD'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "?\n",
       "\n",
       "The capital city of British Columbia is Victoria."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history['response_MD'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(history['response_MD'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 4\n",
    "Modify the function to include the prompt in the output dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 18:45:03.592954\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Eileen Regina Edwards"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "def generate_completion(prompt, history, max_tokens=50, temperature=0.0, model=\"text-davinci-002\"):\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    if response.choices[0].text:\n",
    "        print('Time completed:', datetime.now())\n",
    "        response_string = response.choices[0].text.strip()\n",
    "        try:\n",
    "            history['prompt'].append(prompt)\n",
    "            history['response_test'].append(response_string)\n",
    "            history['response'].append(response)\n",
    "            history['response_MD'].append(IPython.display.Markdown(response_string))\n",
    "        except:\n",
    "            history['prompt'] = [prompt]\n",
    "            history['response_test'] = [response_string]\n",
    "            history['response'] = [response]\n",
    "            history['response_MD'] = [IPython.display.Markdown(response_string)]\n",
    "        return history\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "\n",
    "prompt = \"What is Shania Twain's birth name?\"\n",
    "history2 = generate_completion(prompt, dict())\n",
    "history2['response_MD'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The best exercise for back pain is the McKenzie exercise."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Explain in one sentence what is the best exercise for backpain?\"\n",
    "history2 = generate_completion(prompt, history2)\n",
    "history2['response_MD'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-6whq1X35Qz8hzcyXWTfdilGEaFInm at 0x2670cb47ef0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"\\n\\nThe best exercise for back pain is the McKenzie exercise.\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1679449557,\n",
       "  \"id\": \"cmpl-6whq1X35Qz8hzcyXWTfdilGEaFInm\",\n",
       "  \"model\": \"text-davinci-002\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 13,\n",
       "    \"prompt_tokens\": 14,\n",
       "    \"total_tokens\": 27\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history2['response'][1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 5\n",
    "Modify the max_tokens parameter to allow larger responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 22:56:54.107364\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Eileen Regina Edwards"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "def generate_completion(prompt, history, temperature=0.0, model=\"text-davinci-002\"):\n",
    "    max_tokens= int(4096 - len(prompt)/4)\n",
    "    response = openai.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    if response.choices[0].text:\n",
    "        print('Time completed:', datetime.now())\n",
    "        response_string = response.choices[0].text.strip()\n",
    "        try:\n",
    "            history['prompt'].append(prompt)\n",
    "            history['response_text'].append(response_string)\n",
    "            history['response'].append(response)\n",
    "            history['response_MD'].append(IPython.display.Markdown(response_string))\n",
    "        except:\n",
    "            history['prompt'] = [prompt]\n",
    "            history['response_text'] = [response_string]\n",
    "            history['response'] = [response]\n",
    "            history['response_MD'] = [IPython.display.Markdown(response_string)]\n",
    "        return history\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "\n",
    "prompt = \"What is Shania Twain's birth name?\"\n",
    "history2 = generate_completion(prompt, dict())\n",
    "history2['response_MD'][0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Use ChatGPT to complete a task\n",
    "I will use ChatGPT's API to generate code to send GET requests to the Facebook API to pull data from a Facebook group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 23:04:48.451106\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "I will be using Python to make requests .\n",
    "Define the API endpoint and parameters for sending requests the Facebook Graph API.\n",
    "The response should have the following information:\n",
    "- Posts from a Facebook group, including post_id, created_time, message, comments_count, \n",
    "and count of each type of reaction.\n",
    "- Comments from these posts from the Facebook group, including comment time stamp, comment message, and comment id\n",
    "\n",
    "Include a parameter to limit the number of posts to retrieve.\n",
    "Use as few API requests as possible\n",
    "\n",
    "\"\"\"\n",
    "history3 = dict()\n",
    "history3 = generate_completion(prompt, history3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://developers.facebook.com/docs/graph-api/reference/v3.2/group/feed#read'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history3['response_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 23:08:04.038154\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"limit\": 100,\n",
       "    \"fields\": \"id,created_time,message,comments.limit(0).summary(true),reactions.limit(0).summary(true)\"\n",
       "\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "I will be using Python to make requests .\n",
    "Define the API endpoint and parameters for sending requests the Facebook Graph API.\n",
    "The response should have the following information:\n",
    "- Posts from a Facebook group, including post_id, created_time, message, comments_count, \n",
    "and count of each type of reaction.\n",
    "- Comments from these posts from the Facebook group, including comment time stamp, comment message, and comment id\n",
    "\n",
    "Include a parameter to limit the number of posts to retrieve.\n",
    "Use as few API requests as possible\n",
    "\n",
    "url = f\"https://graph.facebook.com/{group_id}/feed\"\n",
    "params = {\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "history3 = generate_completion(prompt, history3)\n",
    "history3['response_MD'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 23:11:16.728722\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"fields\": \"id, created_time, message, comments.limit(300).summary(true), reactions.limit(0).summary(true)\",\n",
       "    \"limit\": number_of_posts,\n",
       "    \"access_token\": access_token\n",
       "\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "I will be using Python to make requests .\n",
    "Define the API endpoint and parameters for sending requests the Facebook Graph API.\n",
    "The response should have the following information:\n",
    "- Posts from a Facebook group, including post_id, created_time, message, comments_count, \n",
    "and reaction count (a separate count per reaction). \n",
    "- Up to 300 comments per post from the Facebook group, including comment time stamp, comment message, and comment id\n",
    "\n",
    "Include a parameter to limit the number of posts to retrieve.\n",
    "Use as few API requests as possible\n",
    "\n",
    "url = f\"https://graph.facebook.com/{group_id}/feed\"\n",
    "params = {\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "history3 = generate_completion(prompt, history3)\n",
    "history3['response_MD'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 23:12:31.849609\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"limit\": 300,\n",
       "    \"fields\": \"id,created_time,message,comments.limit(300).summary(true),reactions.limit(0).summary(true)\"\n",
       "\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "I will be using Python to make requests .\n",
    "Define the API endpoint and parameters for sending requests the Facebook Graph API.\n",
    "The response should have the following information:\n",
    "- Posts from a Facebook group, including post_id, created_time, message, comments_count, \n",
    "and reaction count (a separate count per reaction type, e.g. like, love, sad). \n",
    "- Up to 300 comments per post from the Facebook group, including comment time stamp, comment message, and comment id\n",
    "\n",
    "Include a parameter to limit the number of posts to retrieve.\n",
    "Use as few API requests as possible\n",
    "\n",
    "url = f\"https://graph.facebook.com/{group_id}/feed\"\n",
    "params = {\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "history3 = generate_completion(prompt, history3)\n",
    "history3['response_MD'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 23:14:56.847309\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"access_token\": access_token,\n",
       "    \"limit\": 300,\n",
       "    \"fields\": \"id, created_time, message, comments.limit(300).summary(true), reactions.limit(0).summary(true)\"\n",
       "\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "I will be using Python to make requests .\n",
    "Define the API endpoint and parameters for sending requests the Facebook Graph API.\n",
    "The response should have the following information:\n",
    "- Posts from a Facebook group, including post_id, created_time, message, comments_count, \n",
    "and reactions count (a separate count for each of like, love, care, haha, wow, sad, angry). \n",
    "- Up to 300 comments per post from the Facebook group, including comment time stamp, comment message, and comment id\n",
    "\n",
    "Include a parameter to limit the number of posts to retrieve.\n",
    "Use as few API requests as possible\n",
    "\n",
    "url = f\"https://graph.facebook.com/{group_id}/feed\"\n",
    "params = {\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "history3 = generate_completion(prompt, history3)\n",
    "history3['response_MD'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://developers.facebook.com/docs/graph-api/reference/v3.2/group/feed#read',\n",
       " '\"limit\": 100,\\n    \"fields\": \"id,created_time,message,comments.limit(0).summary(true),reactions.limit(0).summary(true)\"\\n\\n}',\n",
       " '\"fields\": \"id, created_time, message, comments.limit(300).summary(true), reactions.limit(0).summary(true)\",\\n    \"limit\": number_of_posts,\\n    \"access_token\": access_token\\n\\n}',\n",
       " '\"limit\": 300,\\n    \"fields\": \"id,created_time,message,comments.limit(300).summary(true),reactions.limit(0).summary(true)\"\\n\\n}',\n",
       " '\"access_token\": access_token,\\n    \"limit\": 300,\\n    \"fields\": \"id, created_time, message, comments.limit(300).summary(true), reactions.limit(0).summary(true)\"\\n\\n}']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[text for text in history3['response_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nI will be using Python to make requests .\\nDefine the API endpoint and parameters for sending requests the Facebook Graph API.\\nThe response should have the following information:\\n- Posts from a Facebook group, including post_id, created_time, message, comments_count, \\nand reactions count (a separate count for each of like, love, care, haha, wow, sad, angry). \\n- Up to 300 comments per post from the Facebook group, including comment time stamp, comment message, and comment id\\n\\nInclude a parameter to limit the number of posts to retrieve.\\nUse as few API requests as possible\\n\\nurl = f\"https://graph.facebook.com/{group_id}/feed\"\\nparams = {\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history3['prompt'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When is the best time to go to sleep?\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_prompt_number = 4\n",
    "prompt = f\"\"\"When is the best time to go to sleep?\n",
    "\"\"\"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 23:39:49.652441\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The best time to sleep is at night."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history3 = generate_completion(prompt, history3)\n",
    "last_prompt_number += 1\n",
    "history3['response_MD'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 23:42:01.527119\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "def get_fb_data(token, endpoint, params=None):\n",
       "    if not params:\n",
       "        params = {}\n",
       "    params['access_token'] = token\n",
       "    url = 'https://graph.facebook.com/{}'.format(endpoint)\n",
       "    return requests.get(url, params=params).json()"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt = f\"\"\"Use this to write a Python function to request data from the Facebook graph API.\n",
    "Minimize characters.\n",
    "\"\"\"\n",
    "history3 = generate_completion(prompt, history3)\n",
    "last_prompt_number += 1\n",
    "history3['response_MD'][last_prompt_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 23:43:36.613386\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "def get_fb_group_data(group_id):\n",
       "    graph = facebook.GraphAPI(access_token)\n",
       "    return graph.get_object(group_id + \"/feed\")"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt = f\"\"\"Use this to write a Python function to retrieve data from a Facebook group\n",
    "using the most recent version of the Facebook graph API.\n",
    "Minimize characters.\n",
    "\"\"\"\n",
    "history3 = generate_completion(prompt, history3)\n",
    "last_prompt_number += 1\n",
    "history3['response_MD'][last_prompt_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 23:44:31.700238\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "def get_fb_data(token, endpoint, params=''):\n",
       "    return requests.get('https://graph.facebook.com/v2.8/' + endpoint + '?access_token=' + token + params).json()"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt = f\"\"\"Write a Python function to request data from the Facebook graph API.\n",
    "Minimize characters.\n",
    "\"\"\"\n",
    "history3 = generate_completion(prompt, history3)\n",
    "last_prompt_number += 1\n",
    "history3['response_MD'][last_prompt_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens, however you requested 4101 tokens (69 in your prompt; 4032 for the completion). Please reduce your prompt; or completion length.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\notebooks\\2023-03-21 2255 ChatGPT API.ipynb Cell 46\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X64sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39mUse this to write a Python function to request data from the Facebook graph API:\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X64sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39maccess_token\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: access_token, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlimit\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: 300, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfields\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mid, created_time, message, comments.limit(300).summary(true), reactions.limit(0).summary(true)\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X64sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mMinimize characters.\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X64sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X64sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m history3 \u001b[39m=\u001b[39m generate_completion(prompt, history3)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X64sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m last_prompt_number \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X64sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m history3[\u001b[39m'\u001b[39m\u001b[39mresponse_MD\u001b[39m\u001b[39m'\u001b[39m][last_prompt_number]\n",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\notebooks\\2023-03-21 2255 ChatGPT API.ipynb Cell 46\u001b[0m in \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X64sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_completion\u001b[39m(prompt, history, temperature\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext-davinci-002\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X64sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     max_tokens\u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m4096\u001b[39m \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(prompt)\u001b[39m/\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X64sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X64sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X64sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X64sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         max_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X64sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X64sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         n\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X64sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         stop\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X64sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         frequency_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X64sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         presence_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X64sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X64sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-21%202255%20ChatGPT%20API.ipynb#X64sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTime completed:\u001b[39m\u001b[39m'\u001b[39m, datetime\u001b[39m.\u001b[39mnow())\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\openaiEnv\\lib\\site-packages\\openai\\api_resources\\completion.py:31\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     33\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\openaiEnv\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:100\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m     92\u001b[0m requestor \u001b[39m=\u001b[39m api_requestor\u001b[39m.\u001b[39mAPIRequestor(\n\u001b[0;32m     93\u001b[0m     api_key,\n\u001b[0;32m     94\u001b[0m     api_base\u001b[39m=\u001b[39mapi_base,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m     organization\u001b[39m=\u001b[39morganization,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m     99\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[1;32m--> 100\u001b[0m response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    101\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    102\u001b[0m     url,\n\u001b[0;32m    103\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    104\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    105\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    106\u001b[0m     request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    107\u001b[0m )\n\u001b[0;32m    109\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    110\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)  \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\openaiEnv\\lib\\site-packages\\openai\\api_requestor.py:122\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    104\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    105\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m     request_id: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    112\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    113\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    114\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    115\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m         request_id\u001b[39m=\u001b[39mrequest_id,\n\u001b[0;32m    121\u001b[0m     )\n\u001b[1;32m--> 122\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    123\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\openaiEnv\\lib\\site-packages\\openai\\api_requestor.py:329\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    322\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    323\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    324\u001b[0m         )\n\u001b[0;32m    325\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    326\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    328\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 329\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    330\u001b[0m             result\u001b[39m.\u001b[39;49mcontent, result\u001b[39m.\u001b[39;49mstatus_code, result\u001b[39m.\u001b[39;49mheaders, stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    331\u001b[0m         ),\n\u001b[0;32m    332\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    333\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\openaiEnv\\lib\\site-packages\\openai\\api_requestor.py:362\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    360\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    361\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 362\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    363\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    364\u001b[0m     )\n\u001b[0;32m    365\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens, however you requested 4101 tokens (69 in your prompt; 4032 for the completion). Please reduce your prompt; or completion length."
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = f\"\"\"Use this to write a Python function to request data from the Facebook graph API:\n",
    "'access_token\": access_token, \"limit\": 300, \"fields\": \"id, created_time, message, comments.limit(300).summary(true), reactions.limit(0).summary(true)'\n",
    "Minimize characters.\n",
    "\"\"\"\n",
    "history3 = generate_completion(prompt, history3)\n",
    "last_prompt_number += 1\n",
    "history3['response_MD'][last_prompt_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 23:49:49.713596\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "def get_data_from_facebook(access_token, fields):\n",
       "    return requests.get('https://graph.facebook.com/v2.8/me',\n",
       "        params={\n",
       "            'access_token': access_token,\n",
       "            'fields': fields\n",
       "        }\n",
       "    ).json()"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt = f\"\"\"Use this to write a Python function to request data from Facebook:\n",
    "'access_token\": access_token, \"fields\": \"id, created_time, message, comments'\n",
    "Minimize characters.\n",
    "\"\"\"\n",
    "history3 = generate_completion(prompt, history3)\n",
    "last_prompt_number += 1\n",
    "history3['response_MD'][last_prompt_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-21 23:51:06.703543\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "def get_fb_posts(group_id, n_posts):\n",
       "    # construct the URL string\n",
       "    base = \"https://graph.facebook.com/v2.8\"\n",
       "    node = \"/\" + group_id\n",
       "    fields = \"/posts?fields=created_time,message,permalink_url,comments.limit(0).summary(true)\"\n",
       "    parameters = \"&limit={}&access_token={}\".format(n_posts, ACCESS_TOKEN)\n",
       "    url = base + node + fields + parameters\n",
       " \n",
       "    # make the GET request\n",
       "    response = requests.get(url)\n",
       "    print(\"request made\")\n",
       " \n",
       "    # return the response object\n",
       "    return response"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\"Create a python function that will make a GET request to pull the posts from a Facebook group using the Facebook API. Output the results into a DataFrame so that each post is a row along with its time stamp, message, permalink, and comments. Also output the response.json(). Include parameters for number of posts.\n",
    "\"\"\"\n",
    "history3 = generate_completion(prompt, history3)\n",
    "last_prompt_number += 1\n",
    "history3['response_MD'][last_prompt_number]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My thoughts\n",
    "This code is incomplete, likely due to token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
