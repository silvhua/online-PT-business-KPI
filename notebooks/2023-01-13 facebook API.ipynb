{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "[]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pandas import json_normalize  \n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\custom_python\")\n",
    "from silvhua import *\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.json\") as f:\n",
    "    access_token = json.load(f)['access_token']\n",
    "user_id = os.environ['fb_user_id']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_post(user_id, access_token, pages=5, filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    user_id = str(user_id)\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url = f'{url_root}{user_id}/posts?access_token={access_token}'\n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    for page in range(1,pages+1):\n",
    "        response = requests.get(url)\n",
    "        print('Response status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        df_list.append(json_normalize(response_json_dict[page], record_path='data'))\n",
    "        try:\n",
    "            url = response_json_dict[page]['paging']['next']\n",
    "        except: \n",
    "            pass\n",
    "    df = pd.concat(df_list)\n",
    "    print('Number of posts:',len(df))\n",
    "    if filename:\n",
    "        try:\n",
    "            save_csv(df,filename,csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict\n",
    "\n",
    "# SH 2023-01-16 16:58 Need to update this so that:\n",
    "    # The URL of the last request is returned in case you want to request posts further back\n",
    "    # If it is the last page, it won't just keep making request with the same endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5p, response_json_5p = get_user_post(user_id, access_token)\n",
    "df_5p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/my_fb_posts_5page_2023-01-12.csv\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/my_fb_posts_5page_2023-01-12.sav\n"
     ]
    }
   ],
   "source": [
    "json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw'\n",
    "csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'\n",
    "filename='my_fb_posts_5page_2023-01-12'\n",
    "save_csv(df_5p,filename,csv_path)\n",
    "savepickle(response_json_5p,filename,'sav',json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape:  (119, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_time</th>\n",
       "      <th>message</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-15T20:21:48+0000</td>\n",
       "      <td>Too good to share. Too bad there's none on exe...</td>\n",
       "      <td>10104327314119821_10104331500165951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-13T19:50:47+0000</td>\n",
       "      <td>It’s been 1 month since I finished my data sci...</td>\n",
       "      <td>10104327314119821_10104329950905681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-11T03:17:40+0000</td>\n",
       "      <td>There was a time when my only exercise was run...</td>\n",
       "      <td>10104327314119821_10104327972240941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-09T21:30:18+0000</td>\n",
       "      <td>For the US, \"Wearable technology (#1), strengt...</td>\n",
       "      <td>10104327314119821_10104326999794731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-09T17:06:33+0000</td>\n",
       "      <td>Excited to see developments in wearable tech!\\...</td>\n",
       "      <td>10104327314119821_10104326849426071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-12-21T23:40:00+0000</td>\n",
       "      <td>Why the scale goes crazy during the holidays: ...</td>\n",
       "      <td>10104327314119821_10103960734697921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-12-17T05:59:42+0000</td>\n",
       "      <td>Feels like I am at a plateau with hand balanci...</td>\n",
       "      <td>10104327314119821_10103957435309921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2021-12-14T19:36:06+0000</td>\n",
       "      <td>New toy</td>\n",
       "      <td>10104327314119821_10103955698231041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021-12-05T06:57:42+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104327314119821_10103949690400781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2021-12-05T06:57:26+0000</td>\n",
       "      <td>💛</td>\n",
       "      <td>10104327314119821_10103949690325931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                created_time  \\\n",
       "0   2023-01-15T20:21:48+0000   \n",
       "1   2023-01-13T19:50:47+0000   \n",
       "2   2023-01-11T03:17:40+0000   \n",
       "3   2023-01-09T21:30:18+0000   \n",
       "4   2023-01-09T17:06:33+0000   \n",
       "..                       ...   \n",
       "18  2021-12-21T23:40:00+0000   \n",
       "19  2021-12-17T05:59:42+0000   \n",
       "20  2021-12-14T19:36:06+0000   \n",
       "21  2021-12-05T06:57:42+0000   \n",
       "22  2021-12-05T06:57:26+0000   \n",
       "\n",
       "                                              message  \\\n",
       "0   Too good to share. Too bad there's none on exe...   \n",
       "1   It’s been 1 month since I finished my data sci...   \n",
       "2   There was a time when my only exercise was run...   \n",
       "3   For the US, \"Wearable technology (#1), strengt...   \n",
       "4   Excited to see developments in wearable tech!\\...   \n",
       "..                                                ...   \n",
       "18  Why the scale goes crazy during the holidays: ...   \n",
       "19  Feels like I am at a plateau with hand balanci...   \n",
       "20                                            New toy   \n",
       "21                                                NaN   \n",
       "22                                                  💛   \n",
       "\n",
       "                                     id  \n",
       "0   10104327314119821_10104331500165951  \n",
       "1   10104327314119821_10104329950905681  \n",
       "2   10104327314119821_10104327972240941  \n",
       "3   10104327314119821_10104326999794731  \n",
       "4   10104327314119821_10104326849426071  \n",
       "..                                  ...  \n",
       "18  10104327314119821_10103960734697921  \n",
       "19  10104327314119821_10103957435309921  \n",
       "20  10104327314119821_10103955698231041  \n",
       "21  10104327314119821_10103949690400781  \n",
       "22  10104327314119821_10103949690325931  \n",
       "\n",
       "[119 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'\n",
    "df_5p = load_csv('my_fb_posts_5page_2023-01-12.csv', csv_path, column1_as_index=True)\n",
    "df_5p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw'\n",
    "filename='my_fb_posts_5page_2023-01-12.sav'\n",
    "response_json_5p = loadpickle(filename, json_path)\n",
    "response_json_5p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>time</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-15 20:21:48+00:00</td>\n",
       "      <td>Too good to share. Too bad there's none on exe...</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20:21:48</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-13 19:50:47+00:00</td>\n",
       "      <td>It’s been 1 month since I finished my data sci...</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>19:50:47</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-11 03:17:40+00:00</td>\n",
       "      <td>There was a time when my only exercise was run...</td>\n",
       "      <td>2023-01-11</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>03:17:40</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp  \\\n",
       "0 2023-01-15 20:21:48+00:00   \n",
       "1 2023-01-13 19:50:47+00:00   \n",
       "2 2023-01-11 03:17:40+00:00   \n",
       "\n",
       "                                                text        date  year  month  \\\n",
       "0  Too good to share. Too bad there's none on exe...  2023-01-15  2023      1   \n",
       "1  It’s been 1 month since I finished my data sci...  2023-01-13  2023      1   \n",
       "2  There was a time when my only exercise was run...  2023-01-11  2023      1   \n",
       "\n",
       "   day_of_week      time  hour  \n",
       "0            6  20:21:48    20  \n",
       "1            4  19:50:47    19  \n",
       "2            2  03:17:40     3  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def process_timestamp(df):\n",
    "    \"\"\"\n",
    "    Convert dates in the json-derived dataframe into different formats.\n",
    "    \"\"\"\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    regex_date = r'.+T'\n",
    "    df2 = pd.DataFrame()\n",
    "    df2['timestamp'] = pd.to_datetime(df['created_time'])\n",
    "    df2['text'] = df['message']\n",
    "    df2['date'] = df2['timestamp'].dt.date\n",
    "    df2['year'] = df2['timestamp'].dt.year\n",
    "    df2['month'] = df2['timestamp'].dt.month\n",
    "    df2['day_of_week'] = df2['timestamp'].dt.dayofweek\n",
    "    df2['time'] = df2['timestamp'].dt.time\n",
    "    df2['hour'] = df2['timestamp'].dt.hour\n",
    "    return df2\n",
    "\n",
    "process_timestamp(df_5p.head(3).copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp      datetime64[ns, UTC]\n",
       "text                        object\n",
       "date                        object\n",
       "year                         int64\n",
       "month                        int64\n",
       "day_of_week                  int64\n",
       "time                        object\n",
       "hour                         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_timestamp(df_5p.head(3).copy()).dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Photos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `get_user_photos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_photos(user_id, access_token, pages=5, filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    user_id = str(user_id)\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url = f'{url_root}{user_id}/photos?type=uploaded&access_token={access_token}'\n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    for page in range(1,pages+1):\n",
    "        response = requests.get(url)\n",
    "        print('Response status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        df_list.append(json_normalize(response_json_dict[page], record_path='data'))\n",
    "        try:\n",
    "            url = response_json_dict[page]['paging']['next']\n",
    "        except: \n",
    "            pass\n",
    "    df = pd.concat(df_list)\n",
    "    print('Number of photos:',len(df))\n",
    "    if filename:\n",
    "        try:\n",
    "            save_csv(df,filename,csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict\n",
    "# SH 2023-01-16 16:58 Need to update this so that:\n",
    "    # The URL of the last request is returned in case you want to request posts further back\n",
    "    # If it is the last page, it won't just keep making request with the same endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status code:  200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [138], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_photos, response_json_photos \u001b[39m=\u001b[39m get_user_photos(user_id, access_token, pages\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn [137], line 10\u001b[0m, in \u001b[0;36mget_user_photos\u001b[1;34m(user_id, access_token, pages, filename, json_path, csv_path)\u001b[0m\n\u001b[0;32m      8\u001b[0m df_list \u001b[39m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m page \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,pages\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m---> 10\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url)\n\u001b[0;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mResponse status code: \u001b[39m\u001b[39m'\u001b[39m,response\u001b[39m.\u001b[39mstatus_code)\n\u001b[0;32m     12\u001b[0m     response_json_dict[page] \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    500\u001b[0m         )\n\u001b[0;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\cloudEnv\\lib\\http\\client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1377\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1378\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1379\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\cloudEnv\\lib\\http\\client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\cloudEnv\\lib\\http\\client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 281\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\cloudEnv\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\cloudEnv\\lib\\ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\cloudEnv\\lib\\ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_photos, response_json_photos = get_user_photos(user_id, access_token, pages=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_time</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-13T19:50:47+0000</td>\n",
       "      <td>It’s been 1 month since I finished my data sci...</td>\n",
       "      <td>10104329950835821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-07T21:36:58+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104325301812501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-07T21:36:58+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104325301787551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-07T21:36:58+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104325301737651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-07T21:36:58+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104325301687751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-01-06T19:39:51+0000</td>\n",
       "      <td>What's your favourite app or wearable that hel...</td>\n",
       "      <td>10104324466511451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-01-03T20:34:34+0000</td>\n",
       "      <td>Gamify your goals\\n\\nDuolingo was one of the p...</td>\n",
       "      <td>10104322209175171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-12-30T02:06:59+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104318386994851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-12-30T02:06:59+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104318386959921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-12-27T07:56:46+0000</td>\n",
       "      <td>While my podcast feed is mostly filled with po...</td>\n",
       "      <td>10104316147467881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               created_time  \\\n",
       "0  2023-01-13T19:50:47+0000   \n",
       "1  2023-01-07T21:36:58+0000   \n",
       "2  2023-01-07T21:36:58+0000   \n",
       "3  2023-01-07T21:36:58+0000   \n",
       "4  2023-01-07T21:36:58+0000   \n",
       "5  2023-01-06T19:39:51+0000   \n",
       "6  2023-01-03T20:34:34+0000   \n",
       "7  2022-12-30T02:06:59+0000   \n",
       "8  2022-12-30T02:06:59+0000   \n",
       "9  2022-12-27T07:56:46+0000   \n",
       "\n",
       "                                                name                 id  \n",
       "0  It’s been 1 month since I finished my data sci...  10104329950835821  \n",
       "1                                                NaN  10104325301812501  \n",
       "2                                                NaN  10104325301787551  \n",
       "3                                                NaN  10104325301737651  \n",
       "4                                                NaN  10104325301687751  \n",
       "5  What's your favourite app or wearable that hel...  10104324466511451  \n",
       "6  Gamify your goals\\n\\nDuolingo was one of the p...  10104322209175171  \n",
       "7                                                NaN  10104318386994851  \n",
       "8                                                NaN  10104318386959921  \n",
       "9  While my podcast feed is mostly filled with po...  10104316147467881  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_photos.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/my_fb_photos_2023-01-14.csv\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/my_fb_photos_2023-01-14.sav\n"
     ]
    }
   ],
   "source": [
    "json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw'\n",
    "csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'\n",
    "filename='my_fb_photos_2023-01-14'\n",
    "save_csv(df_photos,filename,csv_path)\n",
    "savepickle(response_json_photos,filename,'sav',json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_response(df, response_json, filename,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\"\n",
    "    Save the data frame and json_response from the Facebook API request.\n",
    "    \"\"\"\n",
    "    save_csv(df,filename,csv_path)\n",
    "    savepickle(response_json,filename,'sav',json_path)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling AM's Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status code:  200\n",
      "Number of posts: 24\n"
     ]
    }
   ],
   "source": [
    "with open(\"credentials.json\") as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "access_token = credentials['am_access_token'] # SH 2023-01-25 23:59 long lived access token\n",
    "am_user_id = credentials['am_user_id'] \n",
    "df, response_json = get_user_post(am_user_id, access_token, pages=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status code:  400\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Key 'data' not found. If specifying a record_path, all elements of data should have the path.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\json\\_normalize.py:399\u001b[0m, in \u001b[0;36m_json_normalize.<locals>._pull_field\u001b[1;34m(js, spec, extract_record)\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m         result \u001b[39m=\u001b[39m result[spec]\n\u001b[0;32m    400\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'data'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [52], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m access_token \u001b[39m=\u001b[39m credentials[\u001b[39m'\u001b[39m\u001b[39mam2_access_token\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m am_user_id \u001b[39m=\u001b[39m credentials[\u001b[39m'\u001b[39m\u001b[39mam_user_id\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m df, response_json \u001b[39m=\u001b[39m get_user_post(am_user_id, access_token, pages\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn [2], line 13\u001b[0m, in \u001b[0;36mget_user_post\u001b[1;34m(user_id, access_token, pages, filename, json_path, csv_path)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mResponse status code: \u001b[39m\u001b[39m'\u001b[39m,response\u001b[39m.\u001b[39mstatus_code)\n\u001b[0;32m     12\u001b[0m response_json_dict[page] \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n\u001b[1;32m---> 13\u001b[0m df_list\u001b[39m.\u001b[39mappend(json_normalize(response_json_dict[page], record_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     14\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     url \u001b[39m=\u001b[39m response_json_dict[page][\u001b[39m'\u001b[39m\u001b[39mpaging\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mnext\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\json\\_normalize.py:518\u001b[0m, in \u001b[0;36m_json_normalize\u001b[1;34m(data, record_path, meta, meta_prefix, record_prefix, errors, sep, max_level)\u001b[0m\n\u001b[0;32m    515\u001b[0m                 meta_vals[key]\u001b[39m.\u001b[39mappend(meta_val)\n\u001b[0;32m    516\u001b[0m             records\u001b[39m.\u001b[39mextend(recs)\n\u001b[1;32m--> 518\u001b[0m _recursive_extract(data, record_path, {}, level\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m    520\u001b[0m result \u001b[39m=\u001b[39m DataFrame(records)\n\u001b[0;32m    522\u001b[0m \u001b[39mif\u001b[39;00m record_prefix \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\json\\_normalize.py:500\u001b[0m, in \u001b[0;36m_json_normalize.<locals>._recursive_extract\u001b[1;34m(data, path, seen_meta, level)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    499\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m data:\n\u001b[1;32m--> 500\u001b[0m         recs \u001b[39m=\u001b[39m _pull_records(obj, path[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m    501\u001b[0m         recs \u001b[39m=\u001b[39m [\n\u001b[0;32m    502\u001b[0m             nested_to_record(r, sep\u001b[39m=\u001b[39msep, max_level\u001b[39m=\u001b[39mmax_level)\n\u001b[0;32m    503\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(r, \u001b[39mdict\u001b[39m)\n\u001b[0;32m    504\u001b[0m             \u001b[39melse\u001b[39;00m r\n\u001b[0;32m    505\u001b[0m             \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m recs\n\u001b[0;32m    506\u001b[0m         ]\n\u001b[0;32m    508\u001b[0m         \u001b[39m# For repeating the metadata later\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\json\\_normalize.py:422\u001b[0m, in \u001b[0;36m_json_normalize.<locals>._pull_records\u001b[1;34m(js, spec)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_pull_records\u001b[39m(js: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any], spec: \u001b[39mlist\u001b[39m \u001b[39m|\u001b[39m \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[0;32m    417\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[39m    Internal function to pull field for records, and similar to\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[39m    _pull_field, but require to return list. And will raise error\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[39m    if has non iterable value.\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 422\u001b[0m     result \u001b[39m=\u001b[39m _pull_field(js, spec, extract_record\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    424\u001b[0m     \u001b[39m# GH 31507 GH 30145, GH 26284 if result is not list, raise TypeError if not\u001b[39;00m\n\u001b[0;32m    425\u001b[0m     \u001b[39m# null, otherwise return an empty list\u001b[39;00m\n\u001b[0;32m    426\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mlist\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\json\\_normalize.py:402\u001b[0m, in \u001b[0;36m_json_normalize.<locals>._pull_field\u001b[1;34m(js, spec, extract_record)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    401\u001b[0m     \u001b[39mif\u001b[39;00m extract_record:\n\u001b[1;32m--> 402\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[0;32m    403\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m not found. If specifying a record_path, all elements of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    404\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdata should have the path.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    405\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     \u001b[39melif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    407\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mnan\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'data' not found. If specifying a record_path, all elements of data should have the path.\""
     ]
    }
   ],
   "source": [
    "with open(\"credentials.json\") as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "access_token = credentials['am2_access_token'] # SH 2023-01-25 23:59 short-lived access token\n",
    "am_user_id = credentials['am_user_id']\n",
    "df, response_json = get_user_post(am_user_id, access_token, pages=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/AM_fb_posts_50page_2023-01-16.csv\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/AM_fb_posts_50page_2023-01-16.sav\n"
     ]
    }
   ],
   "source": [
    "df_50p, response_json_50p = get_user_post(am_user_id, access_token, pages=50, \n",
    "    filename='AM_fb_posts_50page_2023-01-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape:  (53, 3)\n"
     ]
    }
   ],
   "source": [
    "csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'\n",
    "df_50p = load_csv('AM_fb_posts_50page_2023-01-16.csv', csv_path, column1_as_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 3)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_50p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_time</th>\n",
       "      <th>message</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-04T15:06:57+0000</td>\n",
       "      <td>Charity event in Roscommon - Saturday 7th Janu...</td>\n",
       "      <td>545505737609234_537318481761293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-29T18:55:49+0000</td>\n",
       "      <td>Nutrition and Training Workshop 💪\\n\\nMy ethos ...</td>\n",
       "      <td>545505737609234_509802061179602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-04T22:39:50+0000</td>\n",
       "      <td>‘Why your metabolism is not broken’\\n\\nReally ...</td>\n",
       "      <td>545505737609234_467470882079387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-18T20:34:20+0000</td>\n",
       "      <td>Anyone else on TikTok? 🎯</td>\n",
       "      <td>545505737609234_455027709990371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-18T06:19:36+0000</td>\n",
       "      <td>Completed it mate ✅</td>\n",
       "      <td>545505737609234_454597556700053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-09-15T19:20:59+0000</td>\n",
       "      <td>Anyone doing Blackmores - marathon/half or 10km?</td>\n",
       "      <td>545505737609234_452894803536995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-08-23T08:49:02+0000</td>\n",
       "      <td>We had the most stunning time in Fiji 🇫🇯\\n\\nHa...</td>\n",
       "      <td>545505737609234_436768241816318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-08-16T02:04:54+0000</td>\n",
       "      <td>If you can’t beat ‘em, join em 🍸</td>\n",
       "      <td>545505737609234_431993685627107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-08-14T23:22:47+0000</td>\n",
       "      <td>Bingo Loco 🫶</td>\n",
       "      <td>545505737609234_431270869032722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-08-10T21:13:31+0000</td>\n",
       "      <td>First night out out in awhile 🫶</td>\n",
       "      <td>545505737609234_428638645962611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-08-01T05:12:00+0000</td>\n",
       "      <td>Christmas in July 🎄❤️</td>\n",
       "      <td>545505737609234_422429943250148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022-06-02T20:18:43+0000</td>\n",
       "      <td>❤️❤️❤️</td>\n",
       "      <td>545505737609234_382682757224867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-05-29T08:12:02+0000</td>\n",
       "      <td>I’d love to see what you’d caption this 😅\\n\\n⬇...</td>\n",
       "      <td>545505737609234_379552780871198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022-02-07T19:29:39+0000</td>\n",
       "      <td>Last of the comp Spam I promise 🤣</td>\n",
       "      <td>545505737609234_307769374716206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022-02-02T19:07:05+0000</td>\n",
       "      <td>Preparing for our first external comp 🥳</td>\n",
       "      <td>545505737609234_304784511681359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-01-29T10:16:48+0000</td>\n",
       "      <td>Me and Lillie Allen representing the ladies in...</td>\n",
       "      <td>545505737609234_302093805283763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-01-29T10:16:04+0000</td>\n",
       "      <td>First In House Comp of the year 🙌</td>\n",
       "      <td>545505737609234_302093515283792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-01-27T20:33:16+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>545505737609234_301203155372828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022-01-03T06:40:22+0000</td>\n",
       "      <td>Expires Tonight. Invitation for women who are ...</td>\n",
       "      <td>545505737609234_286353056857838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-12-19T19:15:22+0000</td>\n",
       "      <td>Run up to Christmas in Sydney 🙏</td>\n",
       "      <td>545505737609234_277533934406417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2021-11-16T07:47:31+0000</td>\n",
       "      <td>Right who here Squats???\\n\\nI'm going to be ho...</td>\n",
       "      <td>545505737609234_257020856457725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021-11-09T06:31:04+0000</td>\n",
       "      <td>Hey All,\\n\\n- Are you clueless about resistanc...</td>\n",
       "      <td>545505737609234_252527030240441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2021-11-03T19:17:52+0000</td>\n",
       "      <td>Any goals you'd like to achieve by the end of ...</td>\n",
       "      <td>545505737609234_249141407245670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2021-11-02T23:38:46+0000</td>\n",
       "      <td>Anyone out there still use a written diary or ...</td>\n",
       "      <td>545505737609234_248645757295235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-11-01T18:41:07+0000</td>\n",
       "      <td>What have you done recently that made you real...</td>\n",
       "      <td>545505737609234_247962924030185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-06T23:21:49+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>545505737609234_230971169062694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-09-29T05:20:58+0000</td>\n",
       "      <td>Last Chance people, if you're interested in my...</td>\n",
       "      <td>545505737609234_226206536205824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-09-20T20:20:52+0000</td>\n",
       "      <td>If anyone is interested in getting some help a...</td>\n",
       "      <td>545505737609234_220738340085977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-09-09T06:51:17+0000</td>\n",
       "      <td>𝙄𝙢𝙖𝙜𝙞𝙣𝙚 𝙉𝙊𝙏 𝙛𝙚𝙖𝙧𝙞𝙣𝙜 𝙖 𝙗𝙞𝙠𝙞𝙣𝙞 👙 ✖\\n.\\n.\\n𝐈𝐦𝐚𝐠𝐢𝐧...</td>\n",
       "      <td>545505737609234_213187764174368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-09-02T06:56:40+0000</td>\n",
       "      <td>One Space left for Monday 6th. If you're inter...</td>\n",
       "      <td>545505737609234_208650987961379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-08-31T05:28:24+0000</td>\n",
       "      <td>28 Day Kickstarter \\n\\n‼️I'm looking for 5 lad...</td>\n",
       "      <td>545505737609234_207336971426114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-08-30T05:55:19+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>545505737609234_206695284823616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-08-28T10:04:22+0000</td>\n",
       "      <td>Crossfit or body-building? 🤔</td>\n",
       "      <td>545505737609234_205475808278897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-08-18T04:21:26+0000</td>\n",
       "      <td>I did Front Foot Elevated Split Squats today f...</td>\n",
       "      <td>545505737609234_198907928935685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-08-16T09:27:13+0000</td>\n",
       "      <td>If you're not already involved, get into this ...</td>\n",
       "      <td>545505737609234_197756439050834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-08-13T01:30:43+0000</td>\n",
       "      <td>Anyone interested in why you don't achieve res...</td>\n",
       "      <td>545505737609234_195604925932652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021-08-10T23:06:29+0000</td>\n",
       "      <td>Favourite high protein breakfast? Minus eggs.....</td>\n",
       "      <td>545505737609234_194233939403084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021-08-01T07:53:38+0000</td>\n",
       "      <td>‼️I'm looking for 5 ladies who want to become ...</td>\n",
       "      <td>545505737609234_187816276711517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-07-30T07:57:14+0000</td>\n",
       "      <td>Movie Suggestions? 🤔</td>\n",
       "      <td>545505737609234_186517523508059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021-07-19T04:38:56+0000</td>\n",
       "      <td>https://www.coachmcloone.com/\\n\\nFinally have ...</td>\n",
       "      <td>545505737609234_178816784278133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021-07-16T10:00:08+0000</td>\n",
       "      <td>If you knew Covid was coming.\\n\\nWhat would yo...</td>\n",
       "      <td>545505737609234_176622584497553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2021-07-01T06:13:39+0000</td>\n",
       "      <td>‼️I'm looking for 5 ladies who want to become ...</td>\n",
       "      <td>545505737609234_166417258851419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-06-30T22:53:38+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>545505737609234_166240302202448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-06-26T11:51:06+0000</td>\n",
       "      <td>Anyone interested in gaining some nutrition kn...</td>\n",
       "      <td>545505737609234_163057309187414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2021-06-16T04:22:08+0000</td>\n",
       "      <td>What are people's favourite FB Groups to be ap...</td>\n",
       "      <td>545505737609234_156435016516310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021-06-07T10:48:27+0000</td>\n",
       "      <td>https://www.facebook.com/100000504382136/posts...</td>\n",
       "      <td>545505737609234_150885450404600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2021-05-11T05:30:40+0000</td>\n",
       "      <td>Aoibheann's Experience:\\n.\\n.\\nI started the p...</td>\n",
       "      <td>545505737609234_133207925505686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2021-05-06T04:43:38+0000</td>\n",
       "      <td>If you could eat one food and still achieve yo...</td>\n",
       "      <td>545505737609234_129813195845159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-03T00:12:56+0000</td>\n",
       "      <td>‼️I'm looking for 5 ladies who want to become ...</td>\n",
       "      <td>545505737609234_128188466007632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-27T04:45:39+0000</td>\n",
       "      <td>Snack Options ✅\\n.\\n.\\nA question I get so fre...</td>\n",
       "      <td>545505737609234_126493706177108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-12T00:33:05+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>545505737609234_109633854529760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-12T00:25:29+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>545505737609234_109630651196747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1991-02-12T08:00:00+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>545505737609234_109626264530519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                created_time  \\\n",
       "0   2023-01-04T15:06:57+0000   \n",
       "1   2022-11-29T18:55:49+0000   \n",
       "2   2022-10-04T22:39:50+0000   \n",
       "3   2022-09-18T20:34:20+0000   \n",
       "4   2022-09-18T06:19:36+0000   \n",
       "5   2022-09-15T19:20:59+0000   \n",
       "6   2022-08-23T08:49:02+0000   \n",
       "7   2022-08-16T02:04:54+0000   \n",
       "8   2022-08-14T23:22:47+0000   \n",
       "9   2022-08-10T21:13:31+0000   \n",
       "10  2022-08-01T05:12:00+0000   \n",
       "11  2022-06-02T20:18:43+0000   \n",
       "12  2022-05-29T08:12:02+0000   \n",
       "13  2022-02-07T19:29:39+0000   \n",
       "14  2022-02-02T19:07:05+0000   \n",
       "15  2022-01-29T10:16:48+0000   \n",
       "16  2022-01-29T10:16:04+0000   \n",
       "17  2022-01-27T20:33:16+0000   \n",
       "18  2022-01-03T06:40:22+0000   \n",
       "19  2021-12-19T19:15:22+0000   \n",
       "20  2021-11-16T07:47:31+0000   \n",
       "21  2021-11-09T06:31:04+0000   \n",
       "22  2021-11-03T19:17:52+0000   \n",
       "23  2021-11-02T23:38:46+0000   \n",
       "0   2021-11-01T18:41:07+0000   \n",
       "1   2021-10-06T23:21:49+0000   \n",
       "2   2021-09-29T05:20:58+0000   \n",
       "3   2021-09-20T20:20:52+0000   \n",
       "4   2021-09-09T06:51:17+0000   \n",
       "5   2021-09-02T06:56:40+0000   \n",
       "6   2021-08-31T05:28:24+0000   \n",
       "7   2021-08-30T05:55:19+0000   \n",
       "8   2021-08-28T10:04:22+0000   \n",
       "9   2021-08-18T04:21:26+0000   \n",
       "10  2021-08-16T09:27:13+0000   \n",
       "11  2021-08-13T01:30:43+0000   \n",
       "12  2021-08-10T23:06:29+0000   \n",
       "13  2021-08-01T07:53:38+0000   \n",
       "14  2021-07-30T07:57:14+0000   \n",
       "15  2021-07-19T04:38:56+0000   \n",
       "16  2021-07-16T10:00:08+0000   \n",
       "17  2021-07-01T06:13:39+0000   \n",
       "18  2021-06-30T22:53:38+0000   \n",
       "19  2021-06-26T11:51:06+0000   \n",
       "20  2021-06-16T04:22:08+0000   \n",
       "21  2021-06-07T10:48:27+0000   \n",
       "22  2021-05-11T05:30:40+0000   \n",
       "23  2021-05-06T04:43:38+0000   \n",
       "0   2021-05-03T00:12:56+0000   \n",
       "1   2021-04-27T04:45:39+0000   \n",
       "2   2021-03-12T00:33:05+0000   \n",
       "3   2021-03-12T00:25:29+0000   \n",
       "4   1991-02-12T08:00:00+0000   \n",
       "\n",
       "                                              message  \\\n",
       "0   Charity event in Roscommon - Saturday 7th Janu...   \n",
       "1   Nutrition and Training Workshop 💪\\n\\nMy ethos ...   \n",
       "2   ‘Why your metabolism is not broken’\\n\\nReally ...   \n",
       "3                            Anyone else on TikTok? 🎯   \n",
       "4                                 Completed it mate ✅   \n",
       "5    Anyone doing Blackmores - marathon/half or 10km?   \n",
       "6   We had the most stunning time in Fiji 🇫🇯\\n\\nHa...   \n",
       "7                    If you can’t beat ‘em, join em 🍸   \n",
       "8                                        Bingo Loco 🫶   \n",
       "9                     First night out out in awhile 🫶   \n",
       "10                              Christmas in July 🎄❤️   \n",
       "11                                             ❤️❤️❤️   \n",
       "12  I’d love to see what you’d caption this 😅\\n\\n⬇...   \n",
       "13                  Last of the comp Spam I promise 🤣   \n",
       "14            Preparing for our first external comp 🥳   \n",
       "15  Me and Lillie Allen representing the ladies in...   \n",
       "16                  First In House Comp of the year 🙌   \n",
       "17                                                NaN   \n",
       "18  Expires Tonight. Invitation for women who are ...   \n",
       "19                    Run up to Christmas in Sydney 🙏   \n",
       "20  Right who here Squats???\\n\\nI'm going to be ho...   \n",
       "21  Hey All,\\n\\n- Are you clueless about resistanc...   \n",
       "22  Any goals you'd like to achieve by the end of ...   \n",
       "23  Anyone out there still use a written diary or ...   \n",
       "0   What have you done recently that made you real...   \n",
       "1                                                 NaN   \n",
       "2   Last Chance people, if you're interested in my...   \n",
       "3   If anyone is interested in getting some help a...   \n",
       "4   𝙄𝙢𝙖𝙜𝙞𝙣𝙚 𝙉𝙊𝙏 𝙛𝙚𝙖𝙧𝙞𝙣𝙜 𝙖 𝙗𝙞𝙠𝙞𝙣𝙞 👙 ✖\\n.\\n.\\n𝐈𝐦𝐚𝐠𝐢𝐧...   \n",
       "5   One Space left for Monday 6th. If you're inter...   \n",
       "6   28 Day Kickstarter \\n\\n‼️I'm looking for 5 lad...   \n",
       "7                                                 NaN   \n",
       "8                        Crossfit or body-building? 🤔   \n",
       "9   I did Front Foot Elevated Split Squats today f...   \n",
       "10  If you're not already involved, get into this ...   \n",
       "11  Anyone interested in why you don't achieve res...   \n",
       "12  Favourite high protein breakfast? Minus eggs.....   \n",
       "13  ‼️I'm looking for 5 ladies who want to become ...   \n",
       "14                               Movie Suggestions? 🤔   \n",
       "15  https://www.coachmcloone.com/\\n\\nFinally have ...   \n",
       "16  If you knew Covid was coming.\\n\\nWhat would yo...   \n",
       "17  ‼️I'm looking for 5 ladies who want to become ...   \n",
       "18                                                NaN   \n",
       "19  Anyone interested in gaining some nutrition kn...   \n",
       "20  What are people's favourite FB Groups to be ap...   \n",
       "21  https://www.facebook.com/100000504382136/posts...   \n",
       "22  Aoibheann's Experience:\\n.\\n.\\nI started the p...   \n",
       "23  If you could eat one food and still achieve yo...   \n",
       "0   ‼️I'm looking for 5 ladies who want to become ...   \n",
       "1   Snack Options ✅\\n.\\n.\\nA question I get so fre...   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "\n",
       "                                 id  \n",
       "0   545505737609234_537318481761293  \n",
       "1   545505737609234_509802061179602  \n",
       "2   545505737609234_467470882079387  \n",
       "3   545505737609234_455027709990371  \n",
       "4   545505737609234_454597556700053  \n",
       "5   545505737609234_452894803536995  \n",
       "6   545505737609234_436768241816318  \n",
       "7   545505737609234_431993685627107  \n",
       "8   545505737609234_431270869032722  \n",
       "9   545505737609234_428638645962611  \n",
       "10  545505737609234_422429943250148  \n",
       "11  545505737609234_382682757224867  \n",
       "12  545505737609234_379552780871198  \n",
       "13  545505737609234_307769374716206  \n",
       "14  545505737609234_304784511681359  \n",
       "15  545505737609234_302093805283763  \n",
       "16  545505737609234_302093515283792  \n",
       "17  545505737609234_301203155372828  \n",
       "18  545505737609234_286353056857838  \n",
       "19  545505737609234_277533934406417  \n",
       "20  545505737609234_257020856457725  \n",
       "21  545505737609234_252527030240441  \n",
       "22  545505737609234_249141407245670  \n",
       "23  545505737609234_248645757295235  \n",
       "0   545505737609234_247962924030185  \n",
       "1   545505737609234_230971169062694  \n",
       "2   545505737609234_226206536205824  \n",
       "3   545505737609234_220738340085977  \n",
       "4   545505737609234_213187764174368  \n",
       "5   545505737609234_208650987961379  \n",
       "6   545505737609234_207336971426114  \n",
       "7   545505737609234_206695284823616  \n",
       "8   545505737609234_205475808278897  \n",
       "9   545505737609234_198907928935685  \n",
       "10  545505737609234_197756439050834  \n",
       "11  545505737609234_195604925932652  \n",
       "12  545505737609234_194233939403084  \n",
       "13  545505737609234_187816276711517  \n",
       "14  545505737609234_186517523508059  \n",
       "15  545505737609234_178816784278133  \n",
       "16  545505737609234_176622584497553  \n",
       "17  545505737609234_166417258851419  \n",
       "18  545505737609234_166240302202448  \n",
       "19  545505737609234_163057309187414  \n",
       "20  545505737609234_156435016516310  \n",
       "21  545505737609234_150885450404600  \n",
       "22  545505737609234_133207925505686  \n",
       "23  545505737609234_129813195845159  \n",
       "0   545505737609234_128188466007632  \n",
       "1   545505737609234_126493706177108  \n",
       "2   545505737609234_109633854529760  \n",
       "3   545505737609234_109630651196747  \n",
       "4   545505737609234_109626264530519  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_50p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_post(user_id, access_token, pages=5, filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    user_id = str(user_id)\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url = f'{url_root}{user_id}/posts?access_token={access_token}'\n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    for page in range(1,pages+1):\n",
    "        response = requests.get(url)\n",
    "        print('Request URL:', url)\n",
    "        print('Response status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        df_list.append(json_normalize(response_json_dict[page], record_path='data'))\n",
    "        try:\n",
    "            url = response_json_dict[page]['paging']['next']\n",
    "        except: \n",
    "            pass\n",
    "    df = pd.concat(df_list)\n",
    "    print('Number of posts:',len(df))\n",
    "    if filename:\n",
    "        try:\n",
    "            save_csv(df,filename,csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_photos_am, response_json_photos_am = get_user_photos(am_user_id, access_token, pages=50,\n",
    "    filename='AM_fb_photos_50page_2023-01-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_time</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-04T15:06:57+0000</td>\n",
       "      <td>Charity event in Roscommon - Saturday 7th Janu...</td>\n",
       "      <td>537318171761324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-29T18:55:49+0000</td>\n",
       "      <td>Nutrition and Training Workshop 💪\\n\\nMy ethos ...</td>\n",
       "      <td>509802031179605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-04T22:39:50+0000</td>\n",
       "      <td>‘Why your metabolism is not broken’\\n\\nReally ...</td>\n",
       "      <td>467470492079426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-18T06:19:36+0000</td>\n",
       "      <td>Completed it mate ✅</td>\n",
       "      <td>454597496700059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-08-23T08:49:02+0000</td>\n",
       "      <td>We had the most stunning time in Fiji 🇫🇯\\n\\nHa...</td>\n",
       "      <td>436768148482994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-12T00:33:05+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109633777863101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-12T00:25:31+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109630624530083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-27T04:45:39+0000</td>\n",
       "      <td>Snack Options ✅\\n.\\n.\\nA question I get so fre...</td>\n",
       "      <td>126493682843777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-12T00:33:05+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109633777863101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-12T00:25:31+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109630624530083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                created_time  \\\n",
       "0   2023-01-04T15:06:57+0000   \n",
       "1   2022-11-29T18:55:49+0000   \n",
       "2   2022-10-04T22:39:50+0000   \n",
       "3   2022-09-18T06:19:36+0000   \n",
       "4   2022-08-23T08:49:02+0000   \n",
       "..                       ...   \n",
       "1   2021-03-12T00:33:05+0000   \n",
       "2   2021-03-12T00:25:31+0000   \n",
       "0   2021-04-27T04:45:39+0000   \n",
       "1   2021-03-12T00:33:05+0000   \n",
       "2   2021-03-12T00:25:31+0000   \n",
       "\n",
       "                                                 name               id  \n",
       "0   Charity event in Roscommon - Saturday 7th Janu...  537318171761324  \n",
       "1   Nutrition and Training Workshop 💪\\n\\nMy ethos ...  509802031179605  \n",
       "2   ‘Why your metabolism is not broken’\\n\\nReally ...  467470492079426  \n",
       "3                                 Completed it mate ✅  454597496700059  \n",
       "4   We had the most stunning time in Fiji 🇫🇯\\n\\nHa...  436768148482994  \n",
       "..                                                ...              ...  \n",
       "1                                                 NaN  109633777863101  \n",
       "2                                                 NaN  109630624530083  \n",
       "0   Snack Options ✅\\n.\\n.\\nA question I get so fre...  126493682843777  \n",
       "1                                                 NaN  109633777863101  \n",
       "2                                                 NaN  109630624530083  \n",
       "\n",
       "[194 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_photos_am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'537318171761324'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "# df_photos_am.loc[index,\"id\"]\n",
    "df_photos_am.reset_index(drop=True).loc[0,'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_photo(df_photos, index, access_token=access_token, filename=None,\n",
    "    path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim\\individual_photo_data'):\n",
    "    \"\"\"\n",
    "    Get the url for the photo. This requires that the access token be active for the user of the photo.\n",
    "    Parameters:\n",
    "        - df_photos: DataFrame containing the API response.\n",
    "        - index (int or list): Index in df_photos for which to obtain the url.\n",
    "    Returns: URL for the photo.\n",
    "    \"\"\"\n",
    "    df_photos.reset_index(drop=True, inplace=True)\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    index_list = []\n",
    "    url_list = []\n",
    "    index_list.append(index)\n",
    "    for index in index_list:\n",
    "        request_url = f'{url_root}{str(df_photos.reset_index(drop=True).loc[index,\"id\"])}/picture?access_token='\n",
    "        print('Request URL without access token:', request_url)\n",
    "        request_url += access_token\n",
    "        response = requests.get(request_url)\n",
    "        print('Response status code: ',response.status_code)\n",
    "        url_list.append(response.request.url)\n",
    "    if filename:\n",
    "        try:\n",
    "            savepickle(url_list,filename+'_picture_url_'+str(index),'sav',json_path)\n",
    "            savepickle(url_list,filename+'_api_response_'+str(index),'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "\n",
    "    return url_list, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list, response = get_photo(df_photos, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://scontent.fcxh3-1.fna.fbcdn.net/v/t39.30808-6/325443079_709033990789318_5257377445815459026_n.jpg?stp=cp1_dst-jpg_p960x960&_nc_cat=110&ccb=1-7&_nc_sid=453a68&_nc_ohc=UhhHUM1TfuQAX95CI46&_nc_ht=scontent.fcxh3-1.fna&edm=AIv30VUEAAAA&oh=00_AfAzjFiZEmfvHgXEU29sj6ue-bcNRBP9rxz4JEsFkbYgtw&oe=63CB06AC']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request URL without access token: https://graph.facebook.com/v15.0/537318171761324/picture?access_token=\n",
      "Response status code:  400\n"
     ]
    }
   ],
   "source": [
    "url_list_am0, response_am0 = get_photo(df_photos_am, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get BQ's posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.json\") as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "bq_access_token = credentials['bq_access_token']\n",
    "bq_user_id = credentials['bq_user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status code:  400\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Key 'data' not found. If specifying a record_path, all elements of data should have the path.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\json\\_normalize.py:399\u001b[0m, in \u001b[0;36m_json_normalize.<locals>._pull_field\u001b[1;34m(js, spec, extract_record)\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m         result \u001b[39m=\u001b[39m result[spec]\n\u001b[0;32m    400\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'data'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_bq, response_json_bq \u001b[39m=\u001b[39m get_user_post(bq_user_id, bq_access_token, pages\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, \n\u001b[0;32m      2\u001b[0m     filename\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mBQ_fb_posts_50page_2023-01-22\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn [4], line 13\u001b[0m, in \u001b[0;36mget_user_post\u001b[1;34m(user_id, access_token, pages, filename, json_path, csv_path)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mResponse status code: \u001b[39m\u001b[39m'\u001b[39m,response\u001b[39m.\u001b[39mstatus_code)\n\u001b[0;32m     12\u001b[0m response_json_dict[page] \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n\u001b[1;32m---> 13\u001b[0m df_list\u001b[39m.\u001b[39mappend(json_normalize(response_json_dict[page], record_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     14\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     url \u001b[39m=\u001b[39m response_json_dict[page][\u001b[39m'\u001b[39m\u001b[39mpaging\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mnext\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\json\\_normalize.py:518\u001b[0m, in \u001b[0;36m_json_normalize\u001b[1;34m(data, record_path, meta, meta_prefix, record_prefix, errors, sep, max_level)\u001b[0m\n\u001b[0;32m    515\u001b[0m                 meta_vals[key]\u001b[39m.\u001b[39mappend(meta_val)\n\u001b[0;32m    516\u001b[0m             records\u001b[39m.\u001b[39mextend(recs)\n\u001b[1;32m--> 518\u001b[0m _recursive_extract(data, record_path, {}, level\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m    520\u001b[0m result \u001b[39m=\u001b[39m DataFrame(records)\n\u001b[0;32m    522\u001b[0m \u001b[39mif\u001b[39;00m record_prefix \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\json\\_normalize.py:500\u001b[0m, in \u001b[0;36m_json_normalize.<locals>._recursive_extract\u001b[1;34m(data, path, seen_meta, level)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    499\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m data:\n\u001b[1;32m--> 500\u001b[0m         recs \u001b[39m=\u001b[39m _pull_records(obj, path[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m    501\u001b[0m         recs \u001b[39m=\u001b[39m [\n\u001b[0;32m    502\u001b[0m             nested_to_record(r, sep\u001b[39m=\u001b[39msep, max_level\u001b[39m=\u001b[39mmax_level)\n\u001b[0;32m    503\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(r, \u001b[39mdict\u001b[39m)\n\u001b[0;32m    504\u001b[0m             \u001b[39melse\u001b[39;00m r\n\u001b[0;32m    505\u001b[0m             \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m recs\n\u001b[0;32m    506\u001b[0m         ]\n\u001b[0;32m    508\u001b[0m         \u001b[39m# For repeating the metadata later\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\json\\_normalize.py:422\u001b[0m, in \u001b[0;36m_json_normalize.<locals>._pull_records\u001b[1;34m(js, spec)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_pull_records\u001b[39m(js: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any], spec: \u001b[39mlist\u001b[39m \u001b[39m|\u001b[39m \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[0;32m    417\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[39m    Internal function to pull field for records, and similar to\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[39m    _pull_field, but require to return list. And will raise error\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[39m    if has non iterable value.\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 422\u001b[0m     result \u001b[39m=\u001b[39m _pull_field(js, spec, extract_record\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    424\u001b[0m     \u001b[39m# GH 31507 GH 30145, GH 26284 if result is not list, raise TypeError if not\u001b[39;00m\n\u001b[0;32m    425\u001b[0m     \u001b[39m# null, otherwise return an empty list\u001b[39;00m\n\u001b[0;32m    426\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mlist\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\json\\_normalize.py:402\u001b[0m, in \u001b[0;36m_json_normalize.<locals>._pull_field\u001b[1;34m(js, spec, extract_record)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    401\u001b[0m     \u001b[39mif\u001b[39;00m extract_record:\n\u001b[1;32m--> 402\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[0;32m    403\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m not found. If specifying a record_path, all elements of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    404\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdata should have the path.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    405\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     \u001b[39melif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    407\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mnan\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'data' not found. If specifying a record_path, all elements of data should have the path.\""
     ]
    }
   ],
   "source": [
    "df_bq, response_json_bq = get_user_post(bq_user_id, bq_access_token, pages=50, \n",
    "    filename='BQ_fb_posts_50page_2023-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_media(user_id, access_token, pages=5, type='videos',filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "\n",
    "    \"\"\"\n",
    "    Parmeters:\n",
    "        - type ('videos' (default) or 'photos')\n",
    "    \"\"\"\n",
    "    user_id = str(user_id)\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url = f'{url_root}{user_id}/{type}?type=uploaded&access_token={access_token}'\n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    for page in range(1,pages+1):\n",
    "        response = requests.get(url)\n",
    "        print('Request URL:', url)\n",
    "        print('Response status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        df_list.append(json_normalize(response_json_dict[page], record_path='data'))\n",
    "        try:\n",
    "            url = response_json_dict[page]['paging']['next']\n",
    "        except: \n",
    "            pass\n",
    "    df = pd.concat(df_list)\n",
    "    print('Number of photos:',len(df))\n",
    "    if filename:\n",
    "        try:\n",
    "            save_csv(df,filename,csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict\n",
    "\n",
    "df_videos_am, response_json_videos_am = get_user_media(am_user_id, access_token, pages=50,\n",
    "    filename='AM_fb_videos_50page_2023-01-16')\n",
    "df_videos_am\n",
    "\n",
    "# SH 2023-01-16 16:01 Error message using Graph API explorer:\n",
    "    # \"(#10) This endpoint requires the 'pages_read_engagement' permission or the 'Page Public Content Access' feature. Refer to https://developers.facebook.com/docs/apps/review/login-permissions#manage-pages and https://developers.facebook.com/docs/apps/review/feature#reference-PAGES_ACCESS for details.\","
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a Long-Lived User Access Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.json\") as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "access_token = credentials['access_token']\n",
    "app_id = credentials['app_id']\n",
    "app_secret = credentials['app_secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "def prolong_access_token(credentials_json='credentials.json', access_token_key='access_token', \n",
    "    new_credentials_filename='credentials_long_lived.json'):\n",
    "    \n",
    "    \"\"\" SH 2023-01-17 23:04\n",
    "    - Convert an access token to a Long-Lived User Access Token, which should last 60 days instead\n",
    "    of 2 hours. \n",
    "    - Print the date and time of updated access_token expiry.\n",
    "    - Create a new credentials JSON file with the Long-Lived User Access Token.\n",
    "\n",
    "    Parameters:\n",
    "        - credentials.json : JSON file containing the following fields:\n",
    "            - 'access_token' or other key: User access token.\n",
    "            - 'app_id'\n",
    "            - 'app_secret'\n",
    "        - access_token_key (str): Key to the relevant access_token in the \n",
    "            JSON file if different from 'access_token'\n",
    "        - new_credentials_filename (str): Filename for saving the credentials file \n",
    "            with the long-lived user access token.\n",
    "    Returns:\n",
    "        - token_response: JSON object containing API GET response.\n",
    "\n",
    "    Relevant API documentation:\n",
    "    https://developers.facebook.com/docs/facebook-login/guides/access-tokens/get-long-lived\n",
    "    \"\"\"\n",
    "    # Retrieve credentials\n",
    "    with open(\"credentials.json\") as f:\n",
    "        credentials = json.load(f)\n",
    "    access_token = credentials[access_token_key]\n",
    "    app_id = credentials['app_id']\n",
    "    app_secret = credentials['app_secret']\n",
    "\n",
    "    # Make API request: Query the GET oauth/access_token endpoint\n",
    "    url_root = \"https://graph.facebook.com/v15.0/oauth/access_token\"\n",
    "    request_url = f'{url_root}?grant_type=fb_exchange_token&client_id={app_id}&client_secret={app_secret}&fb_exchange_token={access_token}'\n",
    "    response = requests.get(request_url)\n",
    "    print('Response status code: ',response.status_code)\n",
    "    try:\n",
    "        response_json = response.json()\n",
    "        response_json['request_url'] = request_url\n",
    "        try:\n",
    "            new_access_token = response_json['access_token']\n",
    "            credentials[access_token_key] = new_access_token\n",
    "            time_to_expiry = timedelta(seconds=response_json['expires_in'])\n",
    "            now = datetime.now()\n",
    "            credentials['token_expiry'] = (datetime.now() + time_to_expiry).strftime(\"%Y-%m-%d %H:%M\")\n",
    "            print('Updated token expiry:', credentials['token_expiry'])\n",
    "            with open(new_credentials_filename,'w') as json_file:\n",
    "                json.dump(credentials, json_file)\n",
    "                print('New credentials file created:', new_credentials_filename)\n",
    "        except:\n",
    "            print('Unable to save new credentials; check request response')\n",
    "        return response_json\n",
    "    except:\n",
    "        print('Unable to get response JSON; check request response')\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status code:  200\n",
      "Updated token expiry: 2023-03-18 21:09\n",
      "New credentials file created: credentials_long_lived.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "token_response = prolong_access_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the new credentials file loads properly\n",
    "new_credentials_filename='credentials_long_lived.json'\n",
    "\n",
    "with open(new_credentials_filename) as json_file:\n",
    "    credentials2 = json.load(json_file)\n",
    "credentials2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status code:  400\n",
      "Unable to save new credentials; check request response\n"
     ]
    }
   ],
   "source": [
    "token_response = prolong_access_token(access_token_key='bq_access_token')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get group feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_response = prolong_access_token(access_token_key='am_access_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint for Strong and Sassy group\n",
    "group_feed_url = \"https://graph.facebook.com/v15.0/2139238999669147/feed?access_token=\"+access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_feed(credentials_json='credentials_long_lived.json', group_id_key='SSC_group_id', \n",
    "    access_token_key='am_access_token',\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - credentials.json : JSON file containing the required credentials:\n",
    "            - group_id for the Facebook group.\n",
    "            - access_token for the user.\n",
    "        - group_id_key and access_token_key (str): Keys to the relevant group_id and access_token in the \n",
    "            JSON file.\n",
    "    Returns:\n",
    "        - \n",
    "    \"\"\"\n",
    "    # Retrieve credentials\n",
    "    with open(\"credentials.json\") as f:\n",
    "        credentials = json.load(f)\n",
    "    access_token = credentials[access_token_key]\n",
    "    group_id = credentials[group_id_key]\n",
    "\n",
    "    # Make API request:\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url = f'{url_root}{group_id}/feed?access_token={access_token}'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_time</th>\n",
       "      <th>message</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-04T15:06:57+0000</td>\n",
       "      <td>Charity event in Roscommon - Saturday 7th Janu...</td>\n",
       "      <td>545505737609234_537318481761293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-29T18:55:49+0000</td>\n",
       "      <td>Nutrition and Training Workshop 💪\\n\\nMy ethos ...</td>\n",
       "      <td>545505737609234_509802061179602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-04T22:39:50+0000</td>\n",
       "      <td>‘Why your metabolism is not broken’\\n\\nReally ...</td>\n",
       "      <td>545505737609234_467470882079387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-18T20:34:20+0000</td>\n",
       "      <td>Anyone else on TikTok? 🎯</td>\n",
       "      <td>545505737609234_455027709990371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-18T06:19:36+0000</td>\n",
       "      <td>Completed it mate ✅</td>\n",
       "      <td>545505737609234_454597556700053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               created_time  \\\n",
       "0  2023-01-04T15:06:57+0000   \n",
       "1  2022-11-29T18:55:49+0000   \n",
       "2  2022-10-04T22:39:50+0000   \n",
       "3  2022-09-18T20:34:20+0000   \n",
       "4  2022-09-18T06:19:36+0000   \n",
       "\n",
       "                                             message  \\\n",
       "0  Charity event in Roscommon - Saturday 7th Janu...   \n",
       "1  Nutrition and Training Workshop 💪\\n\\nMy ethos ...   \n",
       "2  ‘Why your metabolism is not broken’\\n\\nReally ...   \n",
       "3                           Anyone else on TikTok? 🎯   \n",
       "4                                Completed it mate ✅   \n",
       "\n",
       "                                id  \n",
       "0  545505737609234_537318481761293  \n",
       "1  545505737609234_509802061179602  \n",
       "2  545505737609234_467470882079387  \n",
       "3  545505737609234_455027709990371  \n",
       "4  545505737609234_454597556700053  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_50p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_time</th>\n",
       "      <th>message</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2021-06-07T10:48:27+0000</td>\n",
       "      <td>https://www.facebook.com/100000504382136/posts...</td>\n",
       "      <td>545505737609234_150885450404600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2021-05-11T05:30:40+0000</td>\n",
       "      <td>Aoibheann's Experience:\\n.\\n.\\nI started the p...</td>\n",
       "      <td>545505737609234_133207925505686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2021-05-06T04:43:38+0000</td>\n",
       "      <td>If you could eat one food and still achieve yo...</td>\n",
       "      <td>545505737609234_129813195845159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2021-05-03T00:12:56+0000</td>\n",
       "      <td>‼️I'm looking for 5 ladies who want to become ...</td>\n",
       "      <td>545505737609234_128188466007632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2021-04-27T04:45:39+0000</td>\n",
       "      <td>Snack Options ✅\\n.\\n.\\nA question I get so fre...</td>\n",
       "      <td>545505737609234_126493706177108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                created_time  \\\n",
       "45  2021-06-07T10:48:27+0000   \n",
       "46  2021-05-11T05:30:40+0000   \n",
       "47  2021-05-06T04:43:38+0000   \n",
       "48  2021-05-03T00:12:56+0000   \n",
       "49  2021-04-27T04:45:39+0000   \n",
       "\n",
       "                                              message  \\\n",
       "45  https://www.facebook.com/100000504382136/posts...   \n",
       "46  Aoibheann's Experience:\\n.\\n.\\nI started the p...   \n",
       "47  If you could eat one food and still achieve yo...   \n",
       "48  ‼️I'm looking for 5 ladies who want to become ...   \n",
       "49  Snack Options ✅\\n.\\n.\\nA question I get so fre...   \n",
       "\n",
       "                                 id  \n",
       "45  545505737609234_150885450404600  \n",
       "46  545505737609234_133207925505686  \n",
       "47  545505737609234_129813195845159  \n",
       "48  545505737609234_128188466007632  \n",
       "49  545505737609234_126493706177108  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_50p.iloc[-8:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45    https://www.facebook.com/100000504382136/posts...\n",
       "46    Aoibheann's Experience:\\n.\\n.\\nI started the p...\n",
       "47    If you could eat one food and still achieve yo...\n",
       "48    ‼️I'm looking for 5 ladies who want to become ...\n",
       "49    Snack Options ✅\\n.\\n.\\nA question I get so fre...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_50p.iloc[-8:-3]['message']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options for feature transformation:\n",
    "* bag of words / CountVectorizer\n",
    "* vectorization using pre-trained model\n",
    "\n",
    "Given the small amount of data and the lack of labels (i.e. no business metrics for this time period), it doesn't make to:\n",
    "* Perform topic modelling\n",
    "* Perform any modelling\n",
    "\n",
    "For now, just do EDA; look at bag of words because word embeddings cannot be interpreted by humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  (5, 214)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def preprocess_post_text(docs):\n",
    "    \"\"\"\n",
    "    Prepare data from text documents for NLP:\n",
    "    - Remove all the special characters\n",
    "    - Remove all single characters\n",
    "    - Substitute multiple spaces with single space\n",
    "    - Convert to lowercase\n",
    "\n",
    "    Parameters:\n",
    "    docs (n x 1 array or string): Documents.\n",
    "\n",
    "    Returns: Array of processed docs.\n",
    "    \"\"\"\n",
    "    clean_docs = []\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            # Split text into single words (also gets rid of extra white spaces)\n",
    "            words = word_tokenize(doc)\n",
    "\n",
    "            # Convert to lower case\n",
    "            words = [word.lower() for word in words]\n",
    "\n",
    "            # Lemmatize words (must be done after conversion to lower case)\n",
    "            words = [wnl.lemmatize(word) for word in words]\n",
    "\n",
    "            # Remove all single characters\n",
    "            words = [word for word in words if len(word) > 2]\n",
    "            \n",
    "            # join words back together as a string\n",
    "            words = ''.join([word+' ' for word in words])\n",
    "\n",
    "            # Remove any URLs \n",
    "            words = re.sub(r'\\w*\\.+\\w*', '', words) # Remove periods in middle of word\n",
    "            words = re.sub(r'\\w*/+\\w*', '', words) # remove forward slash in middle of word\n",
    "            words = re.sub(r'\\w*/+\\w*', '', words)\n",
    "\n",
    "            # remove special characters\n",
    "            words = ''.join([char for char in words if char not in string.punctuation])\n",
    "            clean_docs.append(words)\n",
    "        except: # In case value is nan\n",
    "            clean_docs.append(doc)\n",
    "        \n",
    "    return np.array(clean_docs)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from custom_nlp import *\n",
    "def post_eda(df):\n",
    "    df = process_timestamp(df)\n",
    "    df['text'] = preprocess_post_text(df['text'])\n",
    "    vect = CountVectorizer()\n",
    "    vect.fit(df['text'])\n",
    "    vector = vect.transform(df['text'])\n",
    "    print('shape: ', vector.shape)\n",
    "    vector_df = pd.DataFrame(vector.toarray(), columns=vect.get_feature_names_out())\n",
    "\n",
    "    return df, vect, vector_df\n",
    "\n",
    "df, vector, vector_df = post_eda(df_50p.iloc[-8:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>time</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-06-07 10:48:27+00:00</td>\n",
       "      <td>http  who want make change like this 👇👇👇</td>\n",
       "      <td>2021-06-07</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>10:48:27</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-11 05:30:40+00:00</td>\n",
       "      <td>aoibheann experience started the programme wan...</td>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>05:30:40</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-06 04:43:38+00:00</td>\n",
       "      <td>you could eat one food and still achieve your ...</td>\n",
       "      <td>2021-05-06</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>04:43:38</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-03 00:12:56+00:00</td>\n",
       "      <td>‼️i looking for lady who want become strong an...</td>\n",
       "      <td>2021-05-03</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>00:12:56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-27 04:45:39+00:00</td>\n",
       "      <td>snack option question get frequently and area ...</td>\n",
       "      <td>2021-04-27</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>04:45:39</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp  \\\n",
       "0 2021-06-07 10:48:27+00:00   \n",
       "1 2021-05-11 05:30:40+00:00   \n",
       "2 2021-05-06 04:43:38+00:00   \n",
       "3 2021-05-03 00:12:56+00:00   \n",
       "4 2021-04-27 04:45:39+00:00   \n",
       "\n",
       "                                                text        date  year  month  \\\n",
       "0          http  who want make change like this 👇👇👇   2021-06-07  2021      6   \n",
       "1  aoibheann experience started the programme wan...  2021-05-11  2021      5   \n",
       "2  you could eat one food and still achieve your ...  2021-05-06  2021      5   \n",
       "3  ‼️i looking for lady who want become strong an...  2021-05-03  2021      5   \n",
       "4  snack option question get frequently and area ...  2021-04-27  2021      4   \n",
       "\n",
       "   day_of_week      time  hour  \n",
       "0            0  10:48:27    10  \n",
       "1            1  05:30:40     5  \n",
       "2            3  04:43:38     4  \n",
       "3            0  00:12:56     0  \n",
       "4            1  04:45:39     4  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2021</th>\n",
       "      <th>able</th>\n",
       "      <th>achieve</th>\n",
       "      <th>actually</th>\n",
       "      <th>advocate</th>\n",
       "      <th>after</th>\n",
       "      <th>all</th>\n",
       "      <th>also</th>\n",
       "      <th>amanda</th>\n",
       "      <th>amazing</th>\n",
       "      <th>...</th>\n",
       "      <th>without</th>\n",
       "      <th>woman</th>\n",
       "      <th>work</th>\n",
       "      <th>working</th>\n",
       "      <th>would</th>\n",
       "      <th>wouldn</th>\n",
       "      <th>year</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>yourself</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 214 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   2021  able  achieve  actually  advocate  after  all  also  amanda  amazing  \\\n",
       "0     0     0        0         0         0      0    0     0       0        0   \n",
       "1     1     0        0         0         0      1    1     0       3        0   \n",
       "2     0     0        1         0         0      0    0     0       0        0   \n",
       "3     0     1        0         2         1      0    0     1       0        1   \n",
       "4     0     0        0         0         0      0    0     0       0        0   \n",
       "\n",
       "   ...  without  woman  work  working  would  wouldn  year  you  your  \\\n",
       "0  ...        0      0     0        0      0       0     0    0     0   \n",
       "1  ...        0      0     0        1      1       1     1    1     1   \n",
       "2  ...        0      0     0        0      1       0     0    1     1   \n",
       "3  ...        0      4     1        0      0       0     0    7     2   \n",
       "4  ...        1      0     0        0      0       0     0    0     1   \n",
       "\n",
       "   yourself  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         1  \n",
       "4         0  \n",
       "\n",
       "[5 rows x 214 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86576fc1f72bb8252e2f1578cc878ed2c12b40840637cdef083c8fb979cf67d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
