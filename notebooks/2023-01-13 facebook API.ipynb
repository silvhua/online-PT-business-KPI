{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "[]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pandas import json_normalize  \n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\custom_python\")\n",
    "from silvhua import *\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append(r'C:\\ProgramData\\Anaconda3\\Scripts')\n",
    "# sys.path.append(r'C:\\ProgramData\\Anaconda3')\n",
    "# sys.path.append(r'C:\\ProgramData\\Anaconda3\\Library\\bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.json\") as f:\n",
    "    access_token = json.load(f)['access_token']\n",
    "user_id = os.environ['fb_user_id']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_post(user_id, access_token, pages=5, filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    user_id = str(user_id)\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url = f'{url_root}{user_id}/posts?access_token={access_token}'\n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    for page in range(1,pages+1):\n",
    "        response = requests.get(url)\n",
    "        print('Response status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        df_list.append(json_normalize(response_json_dict[page], record_path='data'))\n",
    "        try:\n",
    "            url = response_json_dict[page]['paging']['next']\n",
    "        except: \n",
    "            pass\n",
    "    df = pd.concat(df_list)\n",
    "    print('Number of posts:',len(df))\n",
    "    if filename:\n",
    "        try:\n",
    "            save_csv(df,filename,csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict\n",
    "\n",
    "# SH 2023-01-16 16:58 Need to update this so that:\n",
    "    # The URL of the last request is returned in case you want to request posts further back\n",
    "    # If it is the last page, it won't just keep making request with the same endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5p, response_json_5p = get_user_post(user_id, access_token)\n",
    "df_5p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/my_fb_posts_5page_2023-01-12.csv\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/my_fb_posts_5page_2023-01-12.sav\n"
     ]
    }
   ],
   "source": [
    "json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw'\n",
    "csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'\n",
    "filename='my_fb_posts_5page_2023-01-12'\n",
    "save_csv(df_5p,filename,csv_path)\n",
    "savepickle(response_json_5p,filename,'sav',json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape:  (119, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_time</th>\n",
       "      <th>message</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-11T03:17:40+0000</td>\n",
       "      <td>There was a time when my only exercise was run...</td>\n",
       "      <td>10104327314119821_10104327972240941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-09T21:30:18+0000</td>\n",
       "      <td>For the US, \"Wearable technology (#1), strengt...</td>\n",
       "      <td>10104327314119821_10104326999794731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-09T17:06:33+0000</td>\n",
       "      <td>Excited to see developments in wearable tech!\\...</td>\n",
       "      <td>10104327314119821_10104326849426071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-07T21:36:58+0000</td>\n",
       "      <td>Pomelos are one of my favourite fruits. It is ...</td>\n",
       "      <td>10104327314119821_10104325303319481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-06T19:39:51+0000</td>\n",
       "      <td>What's your favourite app or wearable that hel...</td>\n",
       "      <td>10104327314119821_10104324466536401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-12-14T19:36:06+0000</td>\n",
       "      <td>New toy</td>\n",
       "      <td>10104327314119821_10103955698231041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-12-05T06:57:42+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104327314119821_10103949690400781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2021-12-05T06:57:26+0000</td>\n",
       "      <td>💛</td>\n",
       "      <td>10104327314119821_10103949690325931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021-12-03T22:47:08+0000</td>\n",
       "      <td>Did you know that the only way to get better i...</td>\n",
       "      <td>10104327314119821_10103948774531191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2021-12-03T00:13:54+0000</td>\n",
       "      <td>Paleo pumpkin bread with pumpkin puree, coconu...</td>\n",
       "      <td>10104327314119821_10103948206983561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                created_time  \\\n",
       "0   2023-01-11T03:17:40+0000   \n",
       "1   2023-01-09T21:30:18+0000   \n",
       "2   2023-01-09T17:06:33+0000   \n",
       "3   2023-01-07T21:36:58+0000   \n",
       "4   2023-01-06T19:39:51+0000   \n",
       "..                       ...   \n",
       "18  2021-12-14T19:36:06+0000   \n",
       "19  2021-12-05T06:57:42+0000   \n",
       "20  2021-12-05T06:57:26+0000   \n",
       "21  2021-12-03T22:47:08+0000   \n",
       "22  2021-12-03T00:13:54+0000   \n",
       "\n",
       "                                              message  \\\n",
       "0   There was a time when my only exercise was run...   \n",
       "1   For the US, \"Wearable technology (#1), strengt...   \n",
       "2   Excited to see developments in wearable tech!\\...   \n",
       "3   Pomelos are one of my favourite fruits. It is ...   \n",
       "4   What's your favourite app or wearable that hel...   \n",
       "..                                                ...   \n",
       "18                                            New toy   \n",
       "19                                                NaN   \n",
       "20                                                  💛   \n",
       "21  Did you know that the only way to get better i...   \n",
       "22  Paleo pumpkin bread with pumpkin puree, coconu...   \n",
       "\n",
       "                                     id  \n",
       "0   10104327314119821_10104327972240941  \n",
       "1   10104327314119821_10104326999794731  \n",
       "2   10104327314119821_10104326849426071  \n",
       "3   10104327314119821_10104325303319481  \n",
       "4   10104327314119821_10104324466536401  \n",
       "..                                  ...  \n",
       "18  10104327314119821_10103955698231041  \n",
       "19  10104327314119821_10103949690400781  \n",
       "20  10104327314119821_10103949690325931  \n",
       "21  10104327314119821_10103948774531191  \n",
       "22  10104327314119821_10103948206983561  \n",
       "\n",
       "[119 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'\n",
    "df_5p = load_csv('my_fb_posts_5page_2023-01-12.csv', csv_path, column1_as_index=True)\n",
    "df_5p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw'\n",
    "filename='my_fb_posts_5page_2023-01-12.sav'\n",
    "response_json_5p = loadpickle(filename, json_path)\n",
    "response_json_5p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-11 03:17:40+00:00</td>\n",
       "      <td>There was a time when my only exercise was run...</td>\n",
       "      <td>2023-01-11</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>03:17:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-09 21:30:18+00:00</td>\n",
       "      <td>For the US, \"Wearable technology (#1), strengt...</td>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21:30:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-09 17:06:33+00:00</td>\n",
       "      <td>Excited to see developments in wearable tech!\\...</td>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17:06:33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp  \\\n",
       "0 2023-01-11 03:17:40+00:00   \n",
       "1 2023-01-09 21:30:18+00:00   \n",
       "2 2023-01-09 17:06:33+00:00   \n",
       "\n",
       "                                                text        date  year  month  \\\n",
       "0  There was a time when my only exercise was run...  2023-01-11  2023      1   \n",
       "1  For the US, \"Wearable technology (#1), strengt...  2023-01-09  2023      1   \n",
       "2  Excited to see developments in wearable tech!\\...  2023-01-09  2023      1   \n",
       "\n",
       "   day_of_week      time  \n",
       "0            2  03:17:40  \n",
       "1            0  21:30:18  \n",
       "2            0  17:06:33  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def process_post_df(df):\n",
    "    \"\"\"\n",
    "    Convert dates in the json-derived dataframe into different formats.\n",
    "    \"\"\"\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    regex_date = r'.+T'\n",
    "    df2 = pd.DataFrame()\n",
    "    df2['timestamp'] = pd.to_datetime(df['created_time'])\n",
    "    df2['text'] = df['message']\n",
    "    df2['date'] = df2['timestamp'].dt.date\n",
    "    df2['year'] = df2['timestamp'].dt.year\n",
    "    df2['month'] = df2['timestamp'].dt.month\n",
    "    df2['day_of_week'] = df2['timestamp'].dt.dayofweek\n",
    "    df2['time'] = df2['timestamp'].dt.time\n",
    "    df2['time'] = df2['timestamp'].dt.time\n",
    "    return df2\n",
    "\n",
    "process_post_df(df_5p.head(3).copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp      datetime64[ns, UTC]\n",
       "text                        object\n",
       "date                        object\n",
       "month                        int64\n",
       "day_of_week                  int64\n",
       "time                        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_post_df(df_5p.head(3).copy()).dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Photos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `get_user_photos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_photos(user_id, access_token, pages=5, filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    user_id = str(user_id)\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url = f'{url_root}{user_id}/photos?type=uploaded&access_token={access_token}'\n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    for page in range(1,pages+1):\n",
    "        response = requests.get(url)\n",
    "        print('Response status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        df_list.append(json_normalize(response_json_dict[page], record_path='data'))\n",
    "        try:\n",
    "            url = response_json_dict[page]['paging']['next']\n",
    "        except: \n",
    "            pass\n",
    "    df = pd.concat(df_list)\n",
    "    print('Number of photos:',len(df))\n",
    "    if filename:\n",
    "        try:\n",
    "            save_csv(df,filename,csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict\n",
    "# SH 2023-01-16 16:58 Need to update this so that:\n",
    "    # The URL of the last request is returned in case you want to request posts further back\n",
    "    # If it is the last page, it won't just keep making request with the same endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_photos, response_json_photos = get_user_photos(user_id, access_token, pages=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_time</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-13T19:50:47+0000</td>\n",
       "      <td>It’s been 1 month since I finished my data sci...</td>\n",
       "      <td>10104329950835821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-07T21:36:58+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104325301812501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-07T21:36:58+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104325301787551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-07T21:36:58+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104325301737651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-07T21:36:58+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104325301687751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-01-06T19:39:51+0000</td>\n",
       "      <td>What's your favourite app or wearable that hel...</td>\n",
       "      <td>10104324466511451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-01-03T20:34:34+0000</td>\n",
       "      <td>Gamify your goals\\n\\nDuolingo was one of the p...</td>\n",
       "      <td>10104322209175171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-12-30T02:06:59+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104318386994851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-12-30T02:06:59+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104318386959921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-12-27T07:56:46+0000</td>\n",
       "      <td>While my podcast feed is mostly filled with po...</td>\n",
       "      <td>10104316147467881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               created_time  \\\n",
       "0  2023-01-13T19:50:47+0000   \n",
       "1  2023-01-07T21:36:58+0000   \n",
       "2  2023-01-07T21:36:58+0000   \n",
       "3  2023-01-07T21:36:58+0000   \n",
       "4  2023-01-07T21:36:58+0000   \n",
       "5  2023-01-06T19:39:51+0000   \n",
       "6  2023-01-03T20:34:34+0000   \n",
       "7  2022-12-30T02:06:59+0000   \n",
       "8  2022-12-30T02:06:59+0000   \n",
       "9  2022-12-27T07:56:46+0000   \n",
       "\n",
       "                                                name                 id  \n",
       "0  It’s been 1 month since I finished my data sci...  10104329950835821  \n",
       "1                                                NaN  10104325301812501  \n",
       "2                                                NaN  10104325301787551  \n",
       "3                                                NaN  10104325301737651  \n",
       "4                                                NaN  10104325301687751  \n",
       "5  What's your favourite app or wearable that hel...  10104324466511451  \n",
       "6  Gamify your goals\\n\\nDuolingo was one of the p...  10104322209175171  \n",
       "7                                                NaN  10104318386994851  \n",
       "8                                                NaN  10104318386959921  \n",
       "9  While my podcast feed is mostly filled with po...  10104316147467881  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_photos.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/my_fb_photos_2023-01-14.csv\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/my_fb_photos_2023-01-14.sav\n"
     ]
    }
   ],
   "source": [
    "json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw'\n",
    "csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'\n",
    "filename='my_fb_photos_2023-01-14'\n",
    "save_csv(df_photos,filename,csv_path)\n",
    "savepickle(response_json_photos,filename,'sav',json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_response(df, response_json, filename,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\"\n",
    "    Save the data frame and json_response from the Facebook API request.\n",
    "    \"\"\"\n",
    "    save_csv(df,filename,csv_path)\n",
    "    savepickle(response_json,filename,'sav',json_path)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling AM's Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.json\") as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "access_token = credentials['access_token']\n",
    "am_user_id = credentials['am_user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/AM_fb_posts_50page_2023-01-16.csv\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/AM_fb_posts_50page_2023-01-16.sav\n"
     ]
    }
   ],
   "source": [
    "df_50p, response_json_50p = get_user_post(am_user_id, access_token, pages=50, \n",
    "    filename='AM_fb_posts_50page_2023-01-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_time</th>\n",
       "      <th>message</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-04T15:06:57+0000</td>\n",
       "      <td>Charity event in Roscommon - Saturday 7th Janu...</td>\n",
       "      <td>545505737609234_537318481761293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-29T18:55:49+0000</td>\n",
       "      <td>Nutrition and Training Workshop 💪\\n\\nMy ethos ...</td>\n",
       "      <td>545505737609234_509802061179602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-04T22:39:50+0000</td>\n",
       "      <td>‘Why your metabolism is not broken’\\n\\nReally ...</td>\n",
       "      <td>545505737609234_467470882079387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-18T20:34:20+0000</td>\n",
       "      <td>Anyone else on TikTok? 🎯</td>\n",
       "      <td>545505737609234_455027709990371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-18T06:19:36+0000</td>\n",
       "      <td>Completed it mate ✅</td>\n",
       "      <td>545505737609234_454597556700053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-09-15T19:20:59+0000</td>\n",
       "      <td>Anyone doing Blackmores - marathon/half or 10km?</td>\n",
       "      <td>545505737609234_452894803536995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-08-23T08:49:02+0000</td>\n",
       "      <td>We had the most stunning time in Fiji 🇫🇯\\n\\nHa...</td>\n",
       "      <td>545505737609234_436768241816318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-08-16T02:04:54+0000</td>\n",
       "      <td>If you can’t beat ‘em, join em 🍸</td>\n",
       "      <td>545505737609234_431993685627107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-08-14T23:22:47+0000</td>\n",
       "      <td>Bingo Loco 🫶</td>\n",
       "      <td>545505737609234_431270869032722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-08-10T21:13:31+0000</td>\n",
       "      <td>First night out out in awhile 🫶</td>\n",
       "      <td>545505737609234_428638645962611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-08-01T05:12:00+0000</td>\n",
       "      <td>Christmas in July 🎄❤️</td>\n",
       "      <td>545505737609234_422429943250148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022-06-02T20:18:43+0000</td>\n",
       "      <td>❤️❤️❤️</td>\n",
       "      <td>545505737609234_382682757224867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-05-29T08:12:02+0000</td>\n",
       "      <td>I’d love to see what you’d caption this 😅\\n\\n⬇...</td>\n",
       "      <td>545505737609234_379552780871198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022-02-07T19:29:39+0000</td>\n",
       "      <td>Last of the comp Spam I promise 🤣</td>\n",
       "      <td>545505737609234_307769374716206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022-02-02T19:07:05+0000</td>\n",
       "      <td>Preparing for our first external comp 🥳</td>\n",
       "      <td>545505737609234_304784511681359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-01-29T10:16:48+0000</td>\n",
       "      <td>Me and Lillie Allen representing the ladies in...</td>\n",
       "      <td>545505737609234_302093805283763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-01-29T10:16:04+0000</td>\n",
       "      <td>First In House Comp of the year 🙌</td>\n",
       "      <td>545505737609234_302093515283792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-01-27T20:33:16+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>545505737609234_301203155372828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022-01-03T06:40:22+0000</td>\n",
       "      <td>Expires Tonight. Invitation for women who are ...</td>\n",
       "      <td>545505737609234_286353056857838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-12-19T19:15:22+0000</td>\n",
       "      <td>Run up to Christmas in Sydney 🙏</td>\n",
       "      <td>545505737609234_277533934406417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2021-11-16T07:47:31+0000</td>\n",
       "      <td>Right who here Squats???\\n\\nI'm going to be ho...</td>\n",
       "      <td>545505737609234_257020856457725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021-11-09T06:31:04+0000</td>\n",
       "      <td>Hey All,\\n\\n- Are you clueless about resistanc...</td>\n",
       "      <td>545505737609234_252527030240441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2021-11-03T19:17:52+0000</td>\n",
       "      <td>Any goals you'd like to achieve by the end of ...</td>\n",
       "      <td>545505737609234_249141407245670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2021-11-02T23:38:46+0000</td>\n",
       "      <td>Anyone out there still use a written diary or ...</td>\n",
       "      <td>545505737609234_248645757295235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-11-01T18:41:07+0000</td>\n",
       "      <td>What have you done recently that made you real...</td>\n",
       "      <td>545505737609234_247962924030185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-06T23:21:49+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>545505737609234_230971169062694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-09-29T05:20:58+0000</td>\n",
       "      <td>Last Chance people, if you're interested in my...</td>\n",
       "      <td>545505737609234_226206536205824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-09-20T20:20:52+0000</td>\n",
       "      <td>If anyone is interested in getting some help a...</td>\n",
       "      <td>545505737609234_220738340085977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-09-09T06:51:17+0000</td>\n",
       "      <td>𝙄𝙢𝙖𝙜𝙞𝙣𝙚 𝙉𝙊𝙏 𝙛𝙚𝙖𝙧𝙞𝙣𝙜 𝙖 𝙗𝙞𝙠𝙞𝙣𝙞 👙 ✖\\n.\\n.\\n𝐈𝐦𝐚𝐠𝐢𝐧...</td>\n",
       "      <td>545505737609234_213187764174368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-09-02T06:56:40+0000</td>\n",
       "      <td>One Space left for Monday 6th. If you're inter...</td>\n",
       "      <td>545505737609234_208650987961379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-08-31T05:28:24+0000</td>\n",
       "      <td>28 Day Kickstarter \\n\\n‼️I'm looking for 5 lad...</td>\n",
       "      <td>545505737609234_207336971426114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-08-30T05:55:19+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>545505737609234_206695284823616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-08-28T10:04:22+0000</td>\n",
       "      <td>Crossfit or body-building? 🤔</td>\n",
       "      <td>545505737609234_205475808278897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-08-18T04:21:26+0000</td>\n",
       "      <td>I did Front Foot Elevated Split Squats today f...</td>\n",
       "      <td>545505737609234_198907928935685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-08-16T09:27:13+0000</td>\n",
       "      <td>If you're not already involved, get into this ...</td>\n",
       "      <td>545505737609234_197756439050834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-08-13T01:30:43+0000</td>\n",
       "      <td>Anyone interested in why you don't achieve res...</td>\n",
       "      <td>545505737609234_195604925932652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021-08-10T23:06:29+0000</td>\n",
       "      <td>Favourite high protein breakfast? Minus eggs.....</td>\n",
       "      <td>545505737609234_194233939403084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021-08-01T07:53:38+0000</td>\n",
       "      <td>‼️I'm looking for 5 ladies who want to become ...</td>\n",
       "      <td>545505737609234_187816276711517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-07-30T07:57:14+0000</td>\n",
       "      <td>Movie Suggestions? 🤔</td>\n",
       "      <td>545505737609234_186517523508059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021-07-19T04:38:56+0000</td>\n",
       "      <td>https://www.coachmcloone.com/\\n\\nFinally have ...</td>\n",
       "      <td>545505737609234_178816784278133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021-07-16T10:00:08+0000</td>\n",
       "      <td>If you knew Covid was coming.\\n\\nWhat would yo...</td>\n",
       "      <td>545505737609234_176622584497553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2021-07-01T06:13:39+0000</td>\n",
       "      <td>‼️I'm looking for 5 ladies who want to become ...</td>\n",
       "      <td>545505737609234_166417258851419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-06-30T22:53:38+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>545505737609234_166240302202448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-06-26T11:51:06+0000</td>\n",
       "      <td>Anyone interested in gaining some nutrition kn...</td>\n",
       "      <td>545505737609234_163057309187414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2021-06-16T04:22:08+0000</td>\n",
       "      <td>What are people's favourite FB Groups to be ap...</td>\n",
       "      <td>545505737609234_156435016516310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021-06-07T10:48:27+0000</td>\n",
       "      <td>https://www.facebook.com/100000504382136/posts...</td>\n",
       "      <td>545505737609234_150885450404600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2021-05-11T05:30:40+0000</td>\n",
       "      <td>Aoibheann's Experience:\\n.\\n.\\nI started the p...</td>\n",
       "      <td>545505737609234_133207925505686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2021-05-06T04:43:38+0000</td>\n",
       "      <td>If you could eat one food and still achieve yo...</td>\n",
       "      <td>545505737609234_129813195845159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-03T00:12:56+0000</td>\n",
       "      <td>‼️I'm looking for 5 ladies who want to become ...</td>\n",
       "      <td>545505737609234_128188466007632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-27T04:45:39+0000</td>\n",
       "      <td>Snack Options ✅\\n.\\n.\\nA question I get so fre...</td>\n",
       "      <td>545505737609234_126493706177108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-12T00:33:05+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>545505737609234_109633854529760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-12T00:25:29+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>545505737609234_109630651196747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1991-02-12T08:00:00+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>545505737609234_109626264530519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                created_time  \\\n",
       "0   2023-01-04T15:06:57+0000   \n",
       "1   2022-11-29T18:55:49+0000   \n",
       "2   2022-10-04T22:39:50+0000   \n",
       "3   2022-09-18T20:34:20+0000   \n",
       "4   2022-09-18T06:19:36+0000   \n",
       "5   2022-09-15T19:20:59+0000   \n",
       "6   2022-08-23T08:49:02+0000   \n",
       "7   2022-08-16T02:04:54+0000   \n",
       "8   2022-08-14T23:22:47+0000   \n",
       "9   2022-08-10T21:13:31+0000   \n",
       "10  2022-08-01T05:12:00+0000   \n",
       "11  2022-06-02T20:18:43+0000   \n",
       "12  2022-05-29T08:12:02+0000   \n",
       "13  2022-02-07T19:29:39+0000   \n",
       "14  2022-02-02T19:07:05+0000   \n",
       "15  2022-01-29T10:16:48+0000   \n",
       "16  2022-01-29T10:16:04+0000   \n",
       "17  2022-01-27T20:33:16+0000   \n",
       "18  2022-01-03T06:40:22+0000   \n",
       "19  2021-12-19T19:15:22+0000   \n",
       "20  2021-11-16T07:47:31+0000   \n",
       "21  2021-11-09T06:31:04+0000   \n",
       "22  2021-11-03T19:17:52+0000   \n",
       "23  2021-11-02T23:38:46+0000   \n",
       "0   2021-11-01T18:41:07+0000   \n",
       "1   2021-10-06T23:21:49+0000   \n",
       "2   2021-09-29T05:20:58+0000   \n",
       "3   2021-09-20T20:20:52+0000   \n",
       "4   2021-09-09T06:51:17+0000   \n",
       "5   2021-09-02T06:56:40+0000   \n",
       "6   2021-08-31T05:28:24+0000   \n",
       "7   2021-08-30T05:55:19+0000   \n",
       "8   2021-08-28T10:04:22+0000   \n",
       "9   2021-08-18T04:21:26+0000   \n",
       "10  2021-08-16T09:27:13+0000   \n",
       "11  2021-08-13T01:30:43+0000   \n",
       "12  2021-08-10T23:06:29+0000   \n",
       "13  2021-08-01T07:53:38+0000   \n",
       "14  2021-07-30T07:57:14+0000   \n",
       "15  2021-07-19T04:38:56+0000   \n",
       "16  2021-07-16T10:00:08+0000   \n",
       "17  2021-07-01T06:13:39+0000   \n",
       "18  2021-06-30T22:53:38+0000   \n",
       "19  2021-06-26T11:51:06+0000   \n",
       "20  2021-06-16T04:22:08+0000   \n",
       "21  2021-06-07T10:48:27+0000   \n",
       "22  2021-05-11T05:30:40+0000   \n",
       "23  2021-05-06T04:43:38+0000   \n",
       "0   2021-05-03T00:12:56+0000   \n",
       "1   2021-04-27T04:45:39+0000   \n",
       "2   2021-03-12T00:33:05+0000   \n",
       "3   2021-03-12T00:25:29+0000   \n",
       "4   1991-02-12T08:00:00+0000   \n",
       "\n",
       "                                              message  \\\n",
       "0   Charity event in Roscommon - Saturday 7th Janu...   \n",
       "1   Nutrition and Training Workshop 💪\\n\\nMy ethos ...   \n",
       "2   ‘Why your metabolism is not broken’\\n\\nReally ...   \n",
       "3                            Anyone else on TikTok? 🎯   \n",
       "4                                 Completed it mate ✅   \n",
       "5    Anyone doing Blackmores - marathon/half or 10km?   \n",
       "6   We had the most stunning time in Fiji 🇫🇯\\n\\nHa...   \n",
       "7                    If you can’t beat ‘em, join em 🍸   \n",
       "8                                        Bingo Loco 🫶   \n",
       "9                     First night out out in awhile 🫶   \n",
       "10                              Christmas in July 🎄❤️   \n",
       "11                                             ❤️❤️❤️   \n",
       "12  I’d love to see what you’d caption this 😅\\n\\n⬇...   \n",
       "13                  Last of the comp Spam I promise 🤣   \n",
       "14            Preparing for our first external comp 🥳   \n",
       "15  Me and Lillie Allen representing the ladies in...   \n",
       "16                  First In House Comp of the year 🙌   \n",
       "17                                                NaN   \n",
       "18  Expires Tonight. Invitation for women who are ...   \n",
       "19                    Run up to Christmas in Sydney 🙏   \n",
       "20  Right who here Squats???\\n\\nI'm going to be ho...   \n",
       "21  Hey All,\\n\\n- Are you clueless about resistanc...   \n",
       "22  Any goals you'd like to achieve by the end of ...   \n",
       "23  Anyone out there still use a written diary or ...   \n",
       "0   What have you done recently that made you real...   \n",
       "1                                                 NaN   \n",
       "2   Last Chance people, if you're interested in my...   \n",
       "3   If anyone is interested in getting some help a...   \n",
       "4   𝙄𝙢𝙖𝙜𝙞𝙣𝙚 𝙉𝙊𝙏 𝙛𝙚𝙖𝙧𝙞𝙣𝙜 𝙖 𝙗𝙞𝙠𝙞𝙣𝙞 👙 ✖\\n.\\n.\\n𝐈𝐦𝐚𝐠𝐢𝐧...   \n",
       "5   One Space left for Monday 6th. If you're inter...   \n",
       "6   28 Day Kickstarter \\n\\n‼️I'm looking for 5 lad...   \n",
       "7                                                 NaN   \n",
       "8                        Crossfit or body-building? 🤔   \n",
       "9   I did Front Foot Elevated Split Squats today f...   \n",
       "10  If you're not already involved, get into this ...   \n",
       "11  Anyone interested in why you don't achieve res...   \n",
       "12  Favourite high protein breakfast? Minus eggs.....   \n",
       "13  ‼️I'm looking for 5 ladies who want to become ...   \n",
       "14                               Movie Suggestions? 🤔   \n",
       "15  https://www.coachmcloone.com/\\n\\nFinally have ...   \n",
       "16  If you knew Covid was coming.\\n\\nWhat would yo...   \n",
       "17  ‼️I'm looking for 5 ladies who want to become ...   \n",
       "18                                                NaN   \n",
       "19  Anyone interested in gaining some nutrition kn...   \n",
       "20  What are people's favourite FB Groups to be ap...   \n",
       "21  https://www.facebook.com/100000504382136/posts...   \n",
       "22  Aoibheann's Experience:\\n.\\n.\\nI started the p...   \n",
       "23  If you could eat one food and still achieve yo...   \n",
       "0   ‼️I'm looking for 5 ladies who want to become ...   \n",
       "1   Snack Options ✅\\n.\\n.\\nA question I get so fre...   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "\n",
       "                                 id  \n",
       "0   545505737609234_537318481761293  \n",
       "1   545505737609234_509802061179602  \n",
       "2   545505737609234_467470882079387  \n",
       "3   545505737609234_455027709990371  \n",
       "4   545505737609234_454597556700053  \n",
       "5   545505737609234_452894803536995  \n",
       "6   545505737609234_436768241816318  \n",
       "7   545505737609234_431993685627107  \n",
       "8   545505737609234_431270869032722  \n",
       "9   545505737609234_428638645962611  \n",
       "10  545505737609234_422429943250148  \n",
       "11  545505737609234_382682757224867  \n",
       "12  545505737609234_379552780871198  \n",
       "13  545505737609234_307769374716206  \n",
       "14  545505737609234_304784511681359  \n",
       "15  545505737609234_302093805283763  \n",
       "16  545505737609234_302093515283792  \n",
       "17  545505737609234_301203155372828  \n",
       "18  545505737609234_286353056857838  \n",
       "19  545505737609234_277533934406417  \n",
       "20  545505737609234_257020856457725  \n",
       "21  545505737609234_252527030240441  \n",
       "22  545505737609234_249141407245670  \n",
       "23  545505737609234_248645757295235  \n",
       "0   545505737609234_247962924030185  \n",
       "1   545505737609234_230971169062694  \n",
       "2   545505737609234_226206536205824  \n",
       "3   545505737609234_220738340085977  \n",
       "4   545505737609234_213187764174368  \n",
       "5   545505737609234_208650987961379  \n",
       "6   545505737609234_207336971426114  \n",
       "7   545505737609234_206695284823616  \n",
       "8   545505737609234_205475808278897  \n",
       "9   545505737609234_198907928935685  \n",
       "10  545505737609234_197756439050834  \n",
       "11  545505737609234_195604925932652  \n",
       "12  545505737609234_194233939403084  \n",
       "13  545505737609234_187816276711517  \n",
       "14  545505737609234_186517523508059  \n",
       "15  545505737609234_178816784278133  \n",
       "16  545505737609234_176622584497553  \n",
       "17  545505737609234_166417258851419  \n",
       "18  545505737609234_166240302202448  \n",
       "19  545505737609234_163057309187414  \n",
       "20  545505737609234_156435016516310  \n",
       "21  545505737609234_150885450404600  \n",
       "22  545505737609234_133207925505686  \n",
       "23  545505737609234_129813195845159  \n",
       "0   545505737609234_128188466007632  \n",
       "1   545505737609234_126493706177108  \n",
       "2   545505737609234_109633854529760  \n",
       "3   545505737609234_109630651196747  \n",
       "4   545505737609234_109626264530519  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_50p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_post(user_id, access_token, pages=5, filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    user_id = str(user_id)\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url = f'{url_root}{user_id}/posts?access_token={access_token}'\n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    for page in range(1,pages+1):\n",
    "        response = requests.get(url)\n",
    "        print('Request URL:', url)\n",
    "        print('Response status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        df_list.append(json_normalize(response_json_dict[page], record_path='data'))\n",
    "        try:\n",
    "            url = response_json_dict[page]['paging']['next']\n",
    "        except: \n",
    "            pass\n",
    "    df = pd.concat(df_list)\n",
    "    print('Number of posts:',len(df))\n",
    "    if filename:\n",
    "        try:\n",
    "            save_csv(df,filename,csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_photos_am, response_json_photos_am = get_user_photos(am_user_id, access_token, pages=50,\n",
    "    filename='AM_fb_photos_50page_2023-01-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_time</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-04T15:06:57+0000</td>\n",
       "      <td>Charity event in Roscommon - Saturday 7th Janu...</td>\n",
       "      <td>537318171761324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-29T18:55:49+0000</td>\n",
       "      <td>Nutrition and Training Workshop 💪\\n\\nMy ethos ...</td>\n",
       "      <td>509802031179605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-04T22:39:50+0000</td>\n",
       "      <td>‘Why your metabolism is not broken’\\n\\nReally ...</td>\n",
       "      <td>467470492079426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-18T06:19:36+0000</td>\n",
       "      <td>Completed it mate ✅</td>\n",
       "      <td>454597496700059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-08-23T08:49:02+0000</td>\n",
       "      <td>We had the most stunning time in Fiji 🇫🇯\\n\\nHa...</td>\n",
       "      <td>436768148482994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-12T00:33:05+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109633777863101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-12T00:25:31+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109630624530083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-27T04:45:39+0000</td>\n",
       "      <td>Snack Options ✅\\n.\\n.\\nA question I get so fre...</td>\n",
       "      <td>126493682843777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-12T00:33:05+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109633777863101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-12T00:25:31+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109630624530083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                created_time  \\\n",
       "0   2023-01-04T15:06:57+0000   \n",
       "1   2022-11-29T18:55:49+0000   \n",
       "2   2022-10-04T22:39:50+0000   \n",
       "3   2022-09-18T06:19:36+0000   \n",
       "4   2022-08-23T08:49:02+0000   \n",
       "..                       ...   \n",
       "1   2021-03-12T00:33:05+0000   \n",
       "2   2021-03-12T00:25:31+0000   \n",
       "0   2021-04-27T04:45:39+0000   \n",
       "1   2021-03-12T00:33:05+0000   \n",
       "2   2021-03-12T00:25:31+0000   \n",
       "\n",
       "                                                 name               id  \n",
       "0   Charity event in Roscommon - Saturday 7th Janu...  537318171761324  \n",
       "1   Nutrition and Training Workshop 💪\\n\\nMy ethos ...  509802031179605  \n",
       "2   ‘Why your metabolism is not broken’\\n\\nReally ...  467470492079426  \n",
       "3                                 Completed it mate ✅  454597496700059  \n",
       "4   We had the most stunning time in Fiji 🇫🇯\\n\\nHa...  436768148482994  \n",
       "..                                                ...              ...  \n",
       "1                                                 NaN  109633777863101  \n",
       "2                                                 NaN  109630624530083  \n",
       "0   Snack Options ✅\\n.\\n.\\nA question I get so fre...  126493682843777  \n",
       "1                                                 NaN  109633777863101  \n",
       "2                                                 NaN  109630624530083  \n",
       "\n",
       "[194 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_photos_am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'537318171761324'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "# df_photos_am.loc[index,\"id\"]\n",
    "df_photos_am.reset_index(drop=True).loc[0,'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_photo(df_photos, index, access_token=access_token, filename=None,\n",
    "    path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim\\individual_photo_data'):\n",
    "    \"\"\"\n",
    "    Get the url for the photo. This requires that the access token be active for the user of the photo.\n",
    "    Parameters:\n",
    "        - df_photos: DataFrame containing the API response.\n",
    "        - index (int or list): Index in df_photos for which to obtain the url.\n",
    "    Returns: URL for the photo.\n",
    "    \"\"\"\n",
    "    df_photos.reset_index(drop=True, inplace=True)\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    index_list = []\n",
    "    url_list = []\n",
    "    index_list.append(index)\n",
    "    for index in index_list:\n",
    "        request_url = f'{url_root}{str(df_photos.reset_index(drop=True).loc[index,\"id\"])}/picture?access_token='\n",
    "        print('Request URL without access token:', request_url)\n",
    "        request_url += access_token\n",
    "        response = requests.get(request_url)\n",
    "        print('Response status code: ',response.status_code)\n",
    "        url_list.append(response.request.url)\n",
    "    if filename:\n",
    "        try:\n",
    "            savepickle(url_list,filename+'_picture_url_'+str(index),'sav',json_path)\n",
    "            savepickle(url_list,filename+'_api_response_'+str(index),'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "\n",
    "    return url_list, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list, response = get_photo(df_photos, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://scontent.fcxh3-1.fna.fbcdn.net/v/t39.30808-6/325443079_709033990789318_5257377445815459026_n.jpg?stp=cp1_dst-jpg_p960x960&_nc_cat=110&ccb=1-7&_nc_sid=453a68&_nc_ohc=UhhHUM1TfuQAX95CI46&_nc_ht=scontent.fcxh3-1.fna&edm=AIv30VUEAAAA&oh=00_AfAzjFiZEmfvHgXEU29sj6ue-bcNRBP9rxz4JEsFkbYgtw&oe=63CB06AC']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request URL without access token: https://graph.facebook.com/v15.0/537318171761324/picture?access_token=\n",
      "Response status code:  400\n"
     ]
    }
   ],
   "source": [
    "url_list_am0, response_am0 = get_photo(df_photos_am, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_media(user_id, access_token, pages=5, type='videos',filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "\n",
    "    \"\"\"\n",
    "    Parmeters:\n",
    "        - type ('videos' (default) or 'photos')\n",
    "    \"\"\"\n",
    "    user_id = str(user_id)\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url = f'{url_root}{user_id}/{type}?type=uploaded&access_token={access_token}'\n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    for page in range(1,pages+1):\n",
    "        response = requests.get(url)\n",
    "        print('Request URL:', url)\n",
    "        print('Response status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        df_list.append(json_normalize(response_json_dict[page], record_path='data'))\n",
    "        try:\n",
    "            url = response_json_dict[page]['paging']['next']\n",
    "        except: \n",
    "            pass\n",
    "    df = pd.concat(df_list)\n",
    "    print('Number of photos:',len(df))\n",
    "    if filename:\n",
    "        try:\n",
    "            save_csv(df,filename,csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict\n",
    "\n",
    "df_videos_am, response_json_videos_am = get_user_media(am_user_id, access_token, pages=50,\n",
    "    filename='AM_fb_videos_50page_2023-01-16')\n",
    "df_videos_am\n",
    "\n",
    "# SH 2023-01-16 16:01 Error message using Graph API explorer:\n",
    "    # \"(#10) This endpoint requires the 'pages_read_engagement' permission or the 'Page Public Content Access' feature. Refer to https://developers.facebook.com/docs/apps/review/login-permissions#manage-pages and https://developers.facebook.com/docs/apps/review/feature#reference-PAGES_ACCESS for details.\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86576fc1f72bb8252e2f1578cc878ed2c12b40840637cdef083c8fb979cf67d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
