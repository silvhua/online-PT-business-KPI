{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "[]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pandas import json_normalize  \n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\custom_python\")\n",
    "from silvhua import *\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append(r'C:\\ProgramData\\Anaconda3\\Scripts')\n",
    "# sys.path.append(r'C:\\ProgramData\\Anaconda3')\n",
    "# sys.path.append(r'C:\\ProgramData\\Anaconda3\\Library\\bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.json\") as f:\n",
    "    access_token = json.load(f)['access_token']\n",
    "user_id = os.environ['fb_user_id']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_post(user_id, access_token, pages=5, filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    user_id = str(user_id)\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url = f'{url_root}{user_id}/posts?access_token={access_token}'\n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    for page in range(1,pages+1):\n",
    "        response = requests.get(url)\n",
    "        print('Response status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        df_list.append(json_normalize(response_json_dict[page], record_path='data'))\n",
    "        try:\n",
    "            url = response_json_dict[page]['paging']['next']\n",
    "        except: \n",
    "            pass\n",
    "    df = pd.concat(df_list)\n",
    "    print('Number of posts:',len(df))\n",
    "    if filename:\n",
    "        try:\n",
    "            save_csv(df,filename,csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict\n",
    "\n",
    "# SH 2023-01-16 16:58 Need to update this so that:\n",
    "    # The URL of the last request is returned in case you want to request posts further back\n",
    "    # If it is the last page, it won't just keep making request with the same endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5p, response_json_5p = get_user_post(user_id, access_token)\n",
    "df_5p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/my_fb_posts_5page_2023-01-12.csv\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/my_fb_posts_5page_2023-01-12.sav\n"
     ]
    }
   ],
   "source": [
    "json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw'\n",
    "csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'\n",
    "filename='my_fb_posts_5page_2023-01-12'\n",
    "save_csv(df_5p,filename,csv_path)\n",
    "savepickle(response_json_5p,filename,'sav',json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape:  (119, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_time</th>\n",
       "      <th>message</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-15T20:21:48+0000</td>\n",
       "      <td>Too good to share. Too bad there's none on exe...</td>\n",
       "      <td>10104327314119821_10104331500165951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-13T19:50:47+0000</td>\n",
       "      <td>It‚Äôs been 1 month since I finished my data sci...</td>\n",
       "      <td>10104327314119821_10104329950905681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-11T03:17:40+0000</td>\n",
       "      <td>There was a time when my only exercise was run...</td>\n",
       "      <td>10104327314119821_10104327972240941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-09T21:30:18+0000</td>\n",
       "      <td>For the US, \"Wearable technology (#1), strengt...</td>\n",
       "      <td>10104327314119821_10104326999794731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-09T17:06:33+0000</td>\n",
       "      <td>Excited to see developments in wearable tech!\\...</td>\n",
       "      <td>10104327314119821_10104326849426071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-12-21T23:40:00+0000</td>\n",
       "      <td>Why the scale goes crazy during the holidays: ...</td>\n",
       "      <td>10104327314119821_10103960734697921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-12-17T05:59:42+0000</td>\n",
       "      <td>Feels like I am at a plateau with hand balanci...</td>\n",
       "      <td>10104327314119821_10103957435309921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2021-12-14T19:36:06+0000</td>\n",
       "      <td>New toy</td>\n",
       "      <td>10104327314119821_10103955698231041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021-12-05T06:57:42+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104327314119821_10103949690400781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2021-12-05T06:57:26+0000</td>\n",
       "      <td>üíõ</td>\n",
       "      <td>10104327314119821_10103949690325931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                created_time  \\\n",
       "0   2023-01-15T20:21:48+0000   \n",
       "1   2023-01-13T19:50:47+0000   \n",
       "2   2023-01-11T03:17:40+0000   \n",
       "3   2023-01-09T21:30:18+0000   \n",
       "4   2023-01-09T17:06:33+0000   \n",
       "..                       ...   \n",
       "18  2021-12-21T23:40:00+0000   \n",
       "19  2021-12-17T05:59:42+0000   \n",
       "20  2021-12-14T19:36:06+0000   \n",
       "21  2021-12-05T06:57:42+0000   \n",
       "22  2021-12-05T06:57:26+0000   \n",
       "\n",
       "                                              message  \\\n",
       "0   Too good to share. Too bad there's none on exe...   \n",
       "1   It‚Äôs been 1 month since I finished my data sci...   \n",
       "2   There was a time when my only exercise was run...   \n",
       "3   For the US, \"Wearable technology (#1), strengt...   \n",
       "4   Excited to see developments in wearable tech!\\...   \n",
       "..                                                ...   \n",
       "18  Why the scale goes crazy during the holidays: ...   \n",
       "19  Feels like I am at a plateau with hand balanci...   \n",
       "20                                            New toy   \n",
       "21                                                NaN   \n",
       "22                                                  üíõ   \n",
       "\n",
       "                                     id  \n",
       "0   10104327314119821_10104331500165951  \n",
       "1   10104327314119821_10104329950905681  \n",
       "2   10104327314119821_10104327972240941  \n",
       "3   10104327314119821_10104326999794731  \n",
       "4   10104327314119821_10104326849426071  \n",
       "..                                  ...  \n",
       "18  10104327314119821_10103960734697921  \n",
       "19  10104327314119821_10103957435309921  \n",
       "20  10104327314119821_10103955698231041  \n",
       "21  10104327314119821_10103949690400781  \n",
       "22  10104327314119821_10103949690325931  \n",
       "\n",
       "[119 rows x 3 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'\n",
    "df_5p = load_csv('my_fb_posts_5page_2023-01-12.csv', csv_path, column1_as_index=True)\n",
    "df_5p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw'\n",
    "filename='my_fb_posts_5page_2023-01-12.sav'\n",
    "response_json_5p = loadpickle(filename, json_path)\n",
    "response_json_5p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>time</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-15 20:21:48+00:00</td>\n",
       "      <td>Too good to share. Too bad there's none on exe...</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20:21:48</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-13 19:50:47+00:00</td>\n",
       "      <td>It‚Äôs been 1 month since I finished my data sci...</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>19:50:47</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-11 03:17:40+00:00</td>\n",
       "      <td>There was a time when my only exercise was run...</td>\n",
       "      <td>2023-01-11</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>03:17:40</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp  \\\n",
       "0 2023-01-15 20:21:48+00:00   \n",
       "1 2023-01-13 19:50:47+00:00   \n",
       "2 2023-01-11 03:17:40+00:00   \n",
       "\n",
       "                                                text        date  year  month  \\\n",
       "0  Too good to share. Too bad there's none on exe...  2023-01-15  2023      1   \n",
       "1  It‚Äôs been 1 month since I finished my data sci...  2023-01-13  2023      1   \n",
       "2  There was a time when my only exercise was run...  2023-01-11  2023      1   \n",
       "\n",
       "   day_of_week      time  hour  \n",
       "0            6  20:21:48    20  \n",
       "1            4  19:50:47    19  \n",
       "2            2  03:17:40     3  "
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def process_post_df(df):\n",
    "    \"\"\"\n",
    "    Convert dates in the json-derived dataframe into different formats.\n",
    "    \"\"\"\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    regex_date = r'.+T'\n",
    "    df2 = pd.DataFrame()\n",
    "    df2['timestamp'] = pd.to_datetime(df['created_time'])\n",
    "    df2['text'] = df['message']\n",
    "    df2['date'] = df2['timestamp'].dt.date\n",
    "    df2['year'] = df2['timestamp'].dt.year\n",
    "    df2['month'] = df2['timestamp'].dt.month\n",
    "    df2['day_of_week'] = df2['timestamp'].dt.dayofweek\n",
    "    df2['time'] = df2['timestamp'].dt.time\n",
    "    df2['hour'] = df2['timestamp'].dt.hour\n",
    "    return df2\n",
    "\n",
    "process_post_df(df_5p.head(3).copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp      datetime64[ns, UTC]\n",
       "text                        object\n",
       "date                        object\n",
       "year                         int64\n",
       "month                        int64\n",
       "day_of_week                  int64\n",
       "time                        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_post_df(df_5p.head(3).copy()).dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Photos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `get_user_photos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_photos(user_id, access_token, pages=5, filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    user_id = str(user_id)\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url = f'{url_root}{user_id}/photos?type=uploaded&access_token={access_token}'\n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    for page in range(1,pages+1):\n",
    "        response = requests.get(url)\n",
    "        print('Response status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        df_list.append(json_normalize(response_json_dict[page], record_path='data'))\n",
    "        try:\n",
    "            url = response_json_dict[page]['paging']['next']\n",
    "        except: \n",
    "            pass\n",
    "    df = pd.concat(df_list)\n",
    "    print('Number of photos:',len(df))\n",
    "    if filename:\n",
    "        try:\n",
    "            save_csv(df,filename,csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict\n",
    "# SH 2023-01-16 16:58 Need to update this so that:\n",
    "    # The URL of the last request is returned in case you want to request posts further back\n",
    "    # If it is the last page, it won't just keep making request with the same endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status code:  200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [138], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_photos, response_json_photos \u001b[39m=\u001b[39m get_user_photos(user_id, access_token, pages\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn [137], line 10\u001b[0m, in \u001b[0;36mget_user_photos\u001b[1;34m(user_id, access_token, pages, filename, json_path, csv_path)\u001b[0m\n\u001b[0;32m      8\u001b[0m df_list \u001b[39m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m page \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,pages\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m---> 10\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url)\n\u001b[0;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mResponse status code: \u001b[39m\u001b[39m'\u001b[39m,response\u001b[39m.\u001b[39mstatus_code)\n\u001b[0;32m     12\u001b[0m     response_json_dict[page] \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    500\u001b[0m         )\n\u001b[0;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\cloudEnv\\lib\\http\\client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1377\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1378\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1379\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\cloudEnv\\lib\\http\\client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\cloudEnv\\lib\\http\\client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 281\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\cloudEnv\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\cloudEnv\\lib\\ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\cloudEnv\\lib\\ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_photos, response_json_photos = get_user_photos(user_id, access_token, pages=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_time</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-13T19:50:47+0000</td>\n",
       "      <td>It‚Äôs been 1 month since I finished my data sci...</td>\n",
       "      <td>10104329950835821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-07T21:36:58+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104325301812501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-07T21:36:58+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104325301787551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-07T21:36:58+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104325301737651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-07T21:36:58+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104325301687751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-01-06T19:39:51+0000</td>\n",
       "      <td>What's your favourite app or wearable that hel...</td>\n",
       "      <td>10104324466511451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-01-03T20:34:34+0000</td>\n",
       "      <td>Gamify your goals\\n\\nDuolingo was one of the p...</td>\n",
       "      <td>10104322209175171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-12-30T02:06:59+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104318386994851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-12-30T02:06:59+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104318386959921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-12-27T07:56:46+0000</td>\n",
       "      <td>While my podcast feed is mostly filled with po...</td>\n",
       "      <td>10104316147467881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               created_time  \\\n",
       "0  2023-01-13T19:50:47+0000   \n",
       "1  2023-01-07T21:36:58+0000   \n",
       "2  2023-01-07T21:36:58+0000   \n",
       "3  2023-01-07T21:36:58+0000   \n",
       "4  2023-01-07T21:36:58+0000   \n",
       "5  2023-01-06T19:39:51+0000   \n",
       "6  2023-01-03T20:34:34+0000   \n",
       "7  2022-12-30T02:06:59+0000   \n",
       "8  2022-12-30T02:06:59+0000   \n",
       "9  2022-12-27T07:56:46+0000   \n",
       "\n",
       "                                                name                 id  \n",
       "0  It‚Äôs been 1 month since I finished my data sci...  10104329950835821  \n",
       "1                                                NaN  10104325301812501  \n",
       "2                                                NaN  10104325301787551  \n",
       "3                                                NaN  10104325301737651  \n",
       "4                                                NaN  10104325301687751  \n",
       "5  What's your favourite app or wearable that hel...  10104324466511451  \n",
       "6  Gamify your goals\\n\\nDuolingo was one of the p...  10104322209175171  \n",
       "7                                                NaN  10104318386994851  \n",
       "8                                                NaN  10104318386959921  \n",
       "9  While my podcast feed is mostly filled with po...  10104316147467881  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_photos.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/my_fb_photos_2023-01-14.csv\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/my_fb_photos_2023-01-14.sav\n"
     ]
    }
   ],
   "source": [
    "json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw'\n",
    "csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'\n",
    "filename='my_fb_photos_2023-01-14'\n",
    "save_csv(df_photos,filename,csv_path)\n",
    "savepickle(response_json_photos,filename,'sav',json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_response(df, response_json, filename,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\"\n",
    "    Save the data frame and json_response from the Facebook API request.\n",
    "    \"\"\"\n",
    "    save_csv(df,filename,csv_path)\n",
    "    savepickle(response_json,filename,'sav',json_path)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling AM's Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.json\") as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "access_token = credentials['access_token']\n",
    "am_user_id = credentials['am_user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "Response status code:  200\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/AM_fb_posts_50page_2023-01-16.csv\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/AM_fb_posts_50page_2023-01-16.sav\n"
     ]
    }
   ],
   "source": [
    "df_50p, response_json_50p = get_user_post(am_user_id, access_token, pages=50, \n",
    "    filename='AM_fb_posts_50page_2023-01-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 3)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_50p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_time</th>\n",
       "      <th>message</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-04T15:06:57+0000</td>\n",
       "      <td>Charity event in Roscommon - Saturday 7th Janu...</td>\n",
       "      <td>545505737609234_537318481761293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-29T18:55:49+0000</td>\n",
       "      <td>Nutrition and Training Workshop üí™\\n\\nMy ethos ...</td>\n",
       "      <td>545505737609234_509802061179602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-04T22:39:50+0000</td>\n",
       "      <td>‚ÄòWhy your metabolism is not broken‚Äô\\n\\nReally ...</td>\n",
       "      <td>545505737609234_467470882079387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-18T20:34:20+0000</td>\n",
       "      <td>Anyone else on TikTok? üéØ</td>\n",
       "      <td>545505737609234_455027709990371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-18T06:19:36+0000</td>\n",
       "      <td>Completed it mate ‚úÖ</td>\n",
       "      <td>545505737609234_454597556700053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-09-15T19:20:59+0000</td>\n",
       "      <td>Anyone doing Blackmores - marathon/half or 10km?</td>\n",
       "      <td>545505737609234_452894803536995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-08-23T08:49:02+0000</td>\n",
       "      <td>We had the most stunning time in Fiji üá´üáØ\\n\\nHa...</td>\n",
       "      <td>545505737609234_436768241816318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-08-16T02:04:54+0000</td>\n",
       "      <td>If you can‚Äôt beat ‚Äòem, join em üç∏</td>\n",
       "      <td>545505737609234_431993685627107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-08-14T23:22:47+0000</td>\n",
       "      <td>Bingo Loco ü´∂</td>\n",
       "      <td>545505737609234_431270869032722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-08-10T21:13:31+0000</td>\n",
       "      <td>First night out out in awhile ü´∂</td>\n",
       "      <td>545505737609234_428638645962611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-08-01T05:12:00+0000</td>\n",
       "      <td>Christmas in July üéÑ‚ù§Ô∏è</td>\n",
       "      <td>545505737609234_422429943250148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022-06-02T20:18:43+0000</td>\n",
       "      <td>‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è</td>\n",
       "      <td>545505737609234_382682757224867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-05-29T08:12:02+0000</td>\n",
       "      <td>I‚Äôd love to see what you‚Äôd caption this üòÖ\\n\\n‚¨á...</td>\n",
       "      <td>545505737609234_379552780871198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022-02-07T19:29:39+0000</td>\n",
       "      <td>Last of the comp Spam I promise ü§£</td>\n",
       "      <td>545505737609234_307769374716206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022-02-02T19:07:05+0000</td>\n",
       "      <td>Preparing for our first external comp ü•≥</td>\n",
       "      <td>545505737609234_304784511681359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-01-29T10:16:48+0000</td>\n",
       "      <td>Me and Lillie Allen representing the ladies in...</td>\n",
       "      <td>545505737609234_302093805283763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-01-29T10:16:04+0000</td>\n",
       "      <td>First In House Comp of the year üôå</td>\n",
       "      <td>545505737609234_302093515283792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-01-27T20:33:16+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>545505737609234_301203155372828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022-01-03T06:40:22+0000</td>\n",
       "      <td>Expires Tonight. Invitation for women who are ...</td>\n",
       "      <td>545505737609234_286353056857838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-12-19T19:15:22+0000</td>\n",
       "      <td>Run up to Christmas in Sydney üôè</td>\n",
       "      <td>545505737609234_277533934406417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2021-11-16T07:47:31+0000</td>\n",
       "      <td>Right who here Squats???\\n\\nI'm going to be ho...</td>\n",
       "      <td>545505737609234_257020856457725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021-11-09T06:31:04+0000</td>\n",
       "      <td>Hey All,\\n\\n- Are you clueless about resistanc...</td>\n",
       "      <td>545505737609234_252527030240441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2021-11-03T19:17:52+0000</td>\n",
       "      <td>Any goals you'd like to achieve by the end of ...</td>\n",
       "      <td>545505737609234_249141407245670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2021-11-02T23:38:46+0000</td>\n",
       "      <td>Anyone out there still use a written diary or ...</td>\n",
       "      <td>545505737609234_248645757295235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-11-01T18:41:07+0000</td>\n",
       "      <td>What have you done recently that made you real...</td>\n",
       "      <td>545505737609234_247962924030185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-06T23:21:49+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>545505737609234_230971169062694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-09-29T05:20:58+0000</td>\n",
       "      <td>Last Chance people, if you're interested in my...</td>\n",
       "      <td>545505737609234_226206536205824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-09-20T20:20:52+0000</td>\n",
       "      <td>If anyone is interested in getting some help a...</td>\n",
       "      <td>545505737609234_220738340085977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-09-09T06:51:17+0000</td>\n",
       "      <td>ùôÑùô¢ùôñùôúùôûùô£ùôö ùôâùôäùôè ùôõùôöùôñùôßùôûùô£ùôú ùôñ ùôóùôûùô†ùôûùô£ùôû üëô ‚úñ\\n.\\n.\\nùêàùê¶ùêöùê†ùê¢ùêß...</td>\n",
       "      <td>545505737609234_213187764174368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-09-02T06:56:40+0000</td>\n",
       "      <td>One Space left for Monday 6th. If you're inter...</td>\n",
       "      <td>545505737609234_208650987961379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-08-31T05:28:24+0000</td>\n",
       "      <td>28 Day Kickstarter \\n\\n‚ÄºÔ∏èI'm looking for 5 lad...</td>\n",
       "      <td>545505737609234_207336971426114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-08-30T05:55:19+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>545505737609234_206695284823616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-08-28T10:04:22+0000</td>\n",
       "      <td>Crossfit or body-building? ü§î</td>\n",
       "      <td>545505737609234_205475808278897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-08-18T04:21:26+0000</td>\n",
       "      <td>I did Front Foot Elevated Split Squats today f...</td>\n",
       "      <td>545505737609234_198907928935685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-08-16T09:27:13+0000</td>\n",
       "      <td>If you're not already involved, get into this ...</td>\n",
       "      <td>545505737609234_197756439050834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-08-13T01:30:43+0000</td>\n",
       "      <td>Anyone interested in why you don't achieve res...</td>\n",
       "      <td>545505737609234_195604925932652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021-08-10T23:06:29+0000</td>\n",
       "      <td>Favourite high protein breakfast? Minus eggs.....</td>\n",
       "      <td>545505737609234_194233939403084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021-08-01T07:53:38+0000</td>\n",
       "      <td>‚ÄºÔ∏èI'm looking for 5 ladies who want to become ...</td>\n",
       "      <td>545505737609234_187816276711517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-07-30T07:57:14+0000</td>\n",
       "      <td>Movie Suggestions? ü§î</td>\n",
       "      <td>545505737609234_186517523508059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021-07-19T04:38:56+0000</td>\n",
       "      <td>https://www.coachmcloone.com/\\n\\nFinally have ...</td>\n",
       "      <td>545505737609234_178816784278133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021-07-16T10:00:08+0000</td>\n",
       "      <td>If you knew Covid was coming.\\n\\nWhat would yo...</td>\n",
       "      <td>545505737609234_176622584497553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2021-07-01T06:13:39+0000</td>\n",
       "      <td>‚ÄºÔ∏èI'm looking for 5 ladies who want to become ...</td>\n",
       "      <td>545505737609234_166417258851419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-06-30T22:53:38+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>545505737609234_166240302202448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-06-26T11:51:06+0000</td>\n",
       "      <td>Anyone interested in gaining some nutrition kn...</td>\n",
       "      <td>545505737609234_163057309187414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2021-06-16T04:22:08+0000</td>\n",
       "      <td>What are people's favourite FB Groups to be ap...</td>\n",
       "      <td>545505737609234_156435016516310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021-06-07T10:48:27+0000</td>\n",
       "      <td>https://www.facebook.com/100000504382136/posts...</td>\n",
       "      <td>545505737609234_150885450404600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2021-05-11T05:30:40+0000</td>\n",
       "      <td>Aoibheann's Experience:\\n.\\n.\\nI started the p...</td>\n",
       "      <td>545505737609234_133207925505686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2021-05-06T04:43:38+0000</td>\n",
       "      <td>If you could eat one food and still achieve yo...</td>\n",
       "      <td>545505737609234_129813195845159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-03T00:12:56+0000</td>\n",
       "      <td>‚ÄºÔ∏èI'm looking for 5 ladies who want to become ...</td>\n",
       "      <td>545505737609234_128188466007632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-27T04:45:39+0000</td>\n",
       "      <td>Snack Options ‚úÖ\\n.\\n.\\nA question I get so fre...</td>\n",
       "      <td>545505737609234_126493706177108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-12T00:33:05+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>545505737609234_109633854529760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-12T00:25:29+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>545505737609234_109630651196747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1991-02-12T08:00:00+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>545505737609234_109626264530519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                created_time  \\\n",
       "0   2023-01-04T15:06:57+0000   \n",
       "1   2022-11-29T18:55:49+0000   \n",
       "2   2022-10-04T22:39:50+0000   \n",
       "3   2022-09-18T20:34:20+0000   \n",
       "4   2022-09-18T06:19:36+0000   \n",
       "5   2022-09-15T19:20:59+0000   \n",
       "6   2022-08-23T08:49:02+0000   \n",
       "7   2022-08-16T02:04:54+0000   \n",
       "8   2022-08-14T23:22:47+0000   \n",
       "9   2022-08-10T21:13:31+0000   \n",
       "10  2022-08-01T05:12:00+0000   \n",
       "11  2022-06-02T20:18:43+0000   \n",
       "12  2022-05-29T08:12:02+0000   \n",
       "13  2022-02-07T19:29:39+0000   \n",
       "14  2022-02-02T19:07:05+0000   \n",
       "15  2022-01-29T10:16:48+0000   \n",
       "16  2022-01-29T10:16:04+0000   \n",
       "17  2022-01-27T20:33:16+0000   \n",
       "18  2022-01-03T06:40:22+0000   \n",
       "19  2021-12-19T19:15:22+0000   \n",
       "20  2021-11-16T07:47:31+0000   \n",
       "21  2021-11-09T06:31:04+0000   \n",
       "22  2021-11-03T19:17:52+0000   \n",
       "23  2021-11-02T23:38:46+0000   \n",
       "0   2021-11-01T18:41:07+0000   \n",
       "1   2021-10-06T23:21:49+0000   \n",
       "2   2021-09-29T05:20:58+0000   \n",
       "3   2021-09-20T20:20:52+0000   \n",
       "4   2021-09-09T06:51:17+0000   \n",
       "5   2021-09-02T06:56:40+0000   \n",
       "6   2021-08-31T05:28:24+0000   \n",
       "7   2021-08-30T05:55:19+0000   \n",
       "8   2021-08-28T10:04:22+0000   \n",
       "9   2021-08-18T04:21:26+0000   \n",
       "10  2021-08-16T09:27:13+0000   \n",
       "11  2021-08-13T01:30:43+0000   \n",
       "12  2021-08-10T23:06:29+0000   \n",
       "13  2021-08-01T07:53:38+0000   \n",
       "14  2021-07-30T07:57:14+0000   \n",
       "15  2021-07-19T04:38:56+0000   \n",
       "16  2021-07-16T10:00:08+0000   \n",
       "17  2021-07-01T06:13:39+0000   \n",
       "18  2021-06-30T22:53:38+0000   \n",
       "19  2021-06-26T11:51:06+0000   \n",
       "20  2021-06-16T04:22:08+0000   \n",
       "21  2021-06-07T10:48:27+0000   \n",
       "22  2021-05-11T05:30:40+0000   \n",
       "23  2021-05-06T04:43:38+0000   \n",
       "0   2021-05-03T00:12:56+0000   \n",
       "1   2021-04-27T04:45:39+0000   \n",
       "2   2021-03-12T00:33:05+0000   \n",
       "3   2021-03-12T00:25:29+0000   \n",
       "4   1991-02-12T08:00:00+0000   \n",
       "\n",
       "                                              message  \\\n",
       "0   Charity event in Roscommon - Saturday 7th Janu...   \n",
       "1   Nutrition and Training Workshop üí™\\n\\nMy ethos ...   \n",
       "2   ‚ÄòWhy your metabolism is not broken‚Äô\\n\\nReally ...   \n",
       "3                            Anyone else on TikTok? üéØ   \n",
       "4                                 Completed it mate ‚úÖ   \n",
       "5    Anyone doing Blackmores - marathon/half or 10km?   \n",
       "6   We had the most stunning time in Fiji üá´üáØ\\n\\nHa...   \n",
       "7                    If you can‚Äôt beat ‚Äòem, join em üç∏   \n",
       "8                                        Bingo Loco ü´∂   \n",
       "9                     First night out out in awhile ü´∂   \n",
       "10                              Christmas in July üéÑ‚ù§Ô∏è   \n",
       "11                                             ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è   \n",
       "12  I‚Äôd love to see what you‚Äôd caption this üòÖ\\n\\n‚¨á...   \n",
       "13                  Last of the comp Spam I promise ü§£   \n",
       "14            Preparing for our first external comp ü•≥   \n",
       "15  Me and Lillie Allen representing the ladies in...   \n",
       "16                  First In House Comp of the year üôå   \n",
       "17                                                NaN   \n",
       "18  Expires Tonight. Invitation for women who are ...   \n",
       "19                    Run up to Christmas in Sydney üôè   \n",
       "20  Right who here Squats???\\n\\nI'm going to be ho...   \n",
       "21  Hey All,\\n\\n- Are you clueless about resistanc...   \n",
       "22  Any goals you'd like to achieve by the end of ...   \n",
       "23  Anyone out there still use a written diary or ...   \n",
       "0   What have you done recently that made you real...   \n",
       "1                                                 NaN   \n",
       "2   Last Chance people, if you're interested in my...   \n",
       "3   If anyone is interested in getting some help a...   \n",
       "4   ùôÑùô¢ùôñùôúùôûùô£ùôö ùôâùôäùôè ùôõùôöùôñùôßùôûùô£ùôú ùôñ ùôóùôûùô†ùôûùô£ùôû üëô ‚úñ\\n.\\n.\\nùêàùê¶ùêöùê†ùê¢ùêß...   \n",
       "5   One Space left for Monday 6th. If you're inter...   \n",
       "6   28 Day Kickstarter \\n\\n‚ÄºÔ∏èI'm looking for 5 lad...   \n",
       "7                                                 NaN   \n",
       "8                        Crossfit or body-building? ü§î   \n",
       "9   I did Front Foot Elevated Split Squats today f...   \n",
       "10  If you're not already involved, get into this ...   \n",
       "11  Anyone interested in why you don't achieve res...   \n",
       "12  Favourite high protein breakfast? Minus eggs.....   \n",
       "13  ‚ÄºÔ∏èI'm looking for 5 ladies who want to become ...   \n",
       "14                               Movie Suggestions? ü§î   \n",
       "15  https://www.coachmcloone.com/\\n\\nFinally have ...   \n",
       "16  If you knew Covid was coming.\\n\\nWhat would yo...   \n",
       "17  ‚ÄºÔ∏èI'm looking for 5 ladies who want to become ...   \n",
       "18                                                NaN   \n",
       "19  Anyone interested in gaining some nutrition kn...   \n",
       "20  What are people's favourite FB Groups to be ap...   \n",
       "21  https://www.facebook.com/100000504382136/posts...   \n",
       "22  Aoibheann's Experience:\\n.\\n.\\nI started the p...   \n",
       "23  If you could eat one food and still achieve yo...   \n",
       "0   ‚ÄºÔ∏èI'm looking for 5 ladies who want to become ...   \n",
       "1   Snack Options ‚úÖ\\n.\\n.\\nA question I get so fre...   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "\n",
       "                                 id  \n",
       "0   545505737609234_537318481761293  \n",
       "1   545505737609234_509802061179602  \n",
       "2   545505737609234_467470882079387  \n",
       "3   545505737609234_455027709990371  \n",
       "4   545505737609234_454597556700053  \n",
       "5   545505737609234_452894803536995  \n",
       "6   545505737609234_436768241816318  \n",
       "7   545505737609234_431993685627107  \n",
       "8   545505737609234_431270869032722  \n",
       "9   545505737609234_428638645962611  \n",
       "10  545505737609234_422429943250148  \n",
       "11  545505737609234_382682757224867  \n",
       "12  545505737609234_379552780871198  \n",
       "13  545505737609234_307769374716206  \n",
       "14  545505737609234_304784511681359  \n",
       "15  545505737609234_302093805283763  \n",
       "16  545505737609234_302093515283792  \n",
       "17  545505737609234_301203155372828  \n",
       "18  545505737609234_286353056857838  \n",
       "19  545505737609234_277533934406417  \n",
       "20  545505737609234_257020856457725  \n",
       "21  545505737609234_252527030240441  \n",
       "22  545505737609234_249141407245670  \n",
       "23  545505737609234_248645757295235  \n",
       "0   545505737609234_247962924030185  \n",
       "1   545505737609234_230971169062694  \n",
       "2   545505737609234_226206536205824  \n",
       "3   545505737609234_220738340085977  \n",
       "4   545505737609234_213187764174368  \n",
       "5   545505737609234_208650987961379  \n",
       "6   545505737609234_207336971426114  \n",
       "7   545505737609234_206695284823616  \n",
       "8   545505737609234_205475808278897  \n",
       "9   545505737609234_198907928935685  \n",
       "10  545505737609234_197756439050834  \n",
       "11  545505737609234_195604925932652  \n",
       "12  545505737609234_194233939403084  \n",
       "13  545505737609234_187816276711517  \n",
       "14  545505737609234_186517523508059  \n",
       "15  545505737609234_178816784278133  \n",
       "16  545505737609234_176622584497553  \n",
       "17  545505737609234_166417258851419  \n",
       "18  545505737609234_166240302202448  \n",
       "19  545505737609234_163057309187414  \n",
       "20  545505737609234_156435016516310  \n",
       "21  545505737609234_150885450404600  \n",
       "22  545505737609234_133207925505686  \n",
       "23  545505737609234_129813195845159  \n",
       "0   545505737609234_128188466007632  \n",
       "1   545505737609234_126493706177108  \n",
       "2   545505737609234_109633854529760  \n",
       "3   545505737609234_109630651196747  \n",
       "4   545505737609234_109626264530519  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_50p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_post(user_id, access_token, pages=5, filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    user_id = str(user_id)\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url = f'{url_root}{user_id}/posts?access_token={access_token}'\n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    for page in range(1,pages+1):\n",
    "        response = requests.get(url)\n",
    "        print('Request URL:', url)\n",
    "        print('Response status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        df_list.append(json_normalize(response_json_dict[page], record_path='data'))\n",
    "        try:\n",
    "            url = response_json_dict[page]['paging']['next']\n",
    "        except: \n",
    "            pass\n",
    "    df = pd.concat(df_list)\n",
    "    print('Number of posts:',len(df))\n",
    "    if filename:\n",
    "        try:\n",
    "            save_csv(df,filename,csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_photos_am, response_json_photos_am = get_user_photos(am_user_id, access_token, pages=50,\n",
    "    filename='AM_fb_photos_50page_2023-01-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_time</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-04T15:06:57+0000</td>\n",
       "      <td>Charity event in Roscommon - Saturday 7th Janu...</td>\n",
       "      <td>537318171761324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-29T18:55:49+0000</td>\n",
       "      <td>Nutrition and Training Workshop üí™\\n\\nMy ethos ...</td>\n",
       "      <td>509802031179605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-04T22:39:50+0000</td>\n",
       "      <td>‚ÄòWhy your metabolism is not broken‚Äô\\n\\nReally ...</td>\n",
       "      <td>467470492079426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-18T06:19:36+0000</td>\n",
       "      <td>Completed it mate ‚úÖ</td>\n",
       "      <td>454597496700059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-08-23T08:49:02+0000</td>\n",
       "      <td>We had the most stunning time in Fiji üá´üáØ\\n\\nHa...</td>\n",
       "      <td>436768148482994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-12T00:33:05+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109633777863101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-12T00:25:31+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109630624530083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-27T04:45:39+0000</td>\n",
       "      <td>Snack Options ‚úÖ\\n.\\n.\\nA question I get so fre...</td>\n",
       "      <td>126493682843777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-12T00:33:05+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109633777863101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-12T00:25:31+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109630624530083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                created_time  \\\n",
       "0   2023-01-04T15:06:57+0000   \n",
       "1   2022-11-29T18:55:49+0000   \n",
       "2   2022-10-04T22:39:50+0000   \n",
       "3   2022-09-18T06:19:36+0000   \n",
       "4   2022-08-23T08:49:02+0000   \n",
       "..                       ...   \n",
       "1   2021-03-12T00:33:05+0000   \n",
       "2   2021-03-12T00:25:31+0000   \n",
       "0   2021-04-27T04:45:39+0000   \n",
       "1   2021-03-12T00:33:05+0000   \n",
       "2   2021-03-12T00:25:31+0000   \n",
       "\n",
       "                                                 name               id  \n",
       "0   Charity event in Roscommon - Saturday 7th Janu...  537318171761324  \n",
       "1   Nutrition and Training Workshop üí™\\n\\nMy ethos ...  509802031179605  \n",
       "2   ‚ÄòWhy your metabolism is not broken‚Äô\\n\\nReally ...  467470492079426  \n",
       "3                                 Completed it mate ‚úÖ  454597496700059  \n",
       "4   We had the most stunning time in Fiji üá´üáØ\\n\\nHa...  436768148482994  \n",
       "..                                                ...              ...  \n",
       "1                                                 NaN  109633777863101  \n",
       "2                                                 NaN  109630624530083  \n",
       "0   Snack Options ‚úÖ\\n.\\n.\\nA question I get so fre...  126493682843777  \n",
       "1                                                 NaN  109633777863101  \n",
       "2                                                 NaN  109630624530083  \n",
       "\n",
       "[194 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_photos_am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'537318171761324'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "# df_photos_am.loc[index,\"id\"]\n",
    "df_photos_am.reset_index(drop=True).loc[0,'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_photo(df_photos, index, access_token=access_token, filename=None,\n",
    "    path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim\\individual_photo_data'):\n",
    "    \"\"\"\n",
    "    Get the url for the photo. This requires that the access token be active for the user of the photo.\n",
    "    Parameters:\n",
    "        - df_photos: DataFrame containing the API response.\n",
    "        - index (int or list): Index in df_photos for which to obtain the url.\n",
    "    Returns: URL for the photo.\n",
    "    \"\"\"\n",
    "    df_photos.reset_index(drop=True, inplace=True)\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    index_list = []\n",
    "    url_list = []\n",
    "    index_list.append(index)\n",
    "    for index in index_list:\n",
    "        request_url = f'{url_root}{str(df_photos.reset_index(drop=True).loc[index,\"id\"])}/picture?access_token='\n",
    "        print('Request URL without access token:', request_url)\n",
    "        request_url += access_token\n",
    "        response = requests.get(request_url)\n",
    "        print('Response status code: ',response.status_code)\n",
    "        url_list.append(response.request.url)\n",
    "    if filename:\n",
    "        try:\n",
    "            savepickle(url_list,filename+'_picture_url_'+str(index),'sav',json_path)\n",
    "            savepickle(url_list,filename+'_api_response_'+str(index),'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "\n",
    "    return url_list, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list, response = get_photo(df_photos, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://scontent.fcxh3-1.fna.fbcdn.net/v/t39.30808-6/325443079_709033990789318_5257377445815459026_n.jpg?stp=cp1_dst-jpg_p960x960&_nc_cat=110&ccb=1-7&_nc_sid=453a68&_nc_ohc=UhhHUM1TfuQAX95CI46&_nc_ht=scontent.fcxh3-1.fna&edm=AIv30VUEAAAA&oh=00_AfAzjFiZEmfvHgXEU29sj6ue-bcNRBP9rxz4JEsFkbYgtw&oe=63CB06AC']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request URL without access token: https://graph.facebook.com/v15.0/537318171761324/picture?access_token=\n",
      "Response status code:  400\n"
     ]
    }
   ],
   "source": [
    "url_list_am0, response_am0 = get_photo(df_photos_am, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_media(user_id, access_token, pages=5, type='videos',filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "\n",
    "    \"\"\"\n",
    "    Parmeters:\n",
    "        - type ('videos' (default) or 'photos')\n",
    "    \"\"\"\n",
    "    user_id = str(user_id)\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url = f'{url_root}{user_id}/{type}?type=uploaded&access_token={access_token}'\n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    for page in range(1,pages+1):\n",
    "        response = requests.get(url)\n",
    "        print('Request URL:', url)\n",
    "        print('Response status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        df_list.append(json_normalize(response_json_dict[page], record_path='data'))\n",
    "        try:\n",
    "            url = response_json_dict[page]['paging']['next']\n",
    "        except: \n",
    "            pass\n",
    "    df = pd.concat(df_list)\n",
    "    print('Number of photos:',len(df))\n",
    "    if filename:\n",
    "        try:\n",
    "            save_csv(df,filename,csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict\n",
    "\n",
    "df_videos_am, response_json_videos_am = get_user_media(am_user_id, access_token, pages=50,\n",
    "    filename='AM_fb_videos_50page_2023-01-16')\n",
    "df_videos_am\n",
    "\n",
    "# SH 2023-01-16 16:01 Error message using Graph API explorer:\n",
    "    # \"(#10) This endpoint requires the 'pages_read_engagement' permission or the 'Page Public Content Access' feature. Refer to https://developers.facebook.com/docs/apps/review/login-permissions#manage-pages and https://developers.facebook.com/docs/apps/review/feature#reference-PAGES_ACCESS for details.\","
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a Long-Lived User Access Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.json\") as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "access_token = credentials['access_token']\n",
    "app_id = credentials['app_id']\n",
    "app_secret = credentials['app_secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status code:  200\n",
      "Updated token expiry: 2023-03-18 21:09\n",
      "New credentials file created: credentials_long_lived.json\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "def prolong_access_token(credentials_json='credentials.json', access_token_key='access_token', \n",
    "    new_credentials_filename='credentials_long_lived.json'):\n",
    "    \n",
    "    \"\"\" SH 2023-01-17 23:04\n",
    "    - Convert an access token to a Long-Lived User Access Token, which should last 60 days instead\n",
    "    of 2 hours. \n",
    "    - Print the date and time of updated access_token expiry.\n",
    "    - Create a new credentials JSON file with the Long-Lived User Access Token.\n",
    "\n",
    "    Parameters:\n",
    "        - credentials.json : JSON file containing the following fields:\n",
    "            - 'access_token' or other key: User access token.\n",
    "            - 'app_id'\n",
    "            - 'app_secret'\n",
    "        - access_token_key (str): Key to the relevant access_token in the \n",
    "            JSON file if different from 'access_token'\n",
    "        - new_credentials_filename (str): Filename for saving the credentials file \n",
    "            with the long-lived user access token.\n",
    "    Returns:\n",
    "        - token_response: JSON object containing API GET response.\n",
    "\n",
    "    Relevant API documentation:\n",
    "    https://developers.facebook.com/docs/facebook-login/guides/access-tokens/get-long-lived\n",
    "    \"\"\"\n",
    "    # Retrieve credentials\n",
    "    with open(\"credentials.json\") as f:\n",
    "        credentials = json.load(f)\n",
    "    access_token = credentials[access_token_key]\n",
    "    app_id = credentials['app_id']\n",
    "    app_secret = credentials['app_secret']\n",
    "\n",
    "    # Make API request: Query the GET oauth/access_token endpoint\n",
    "    url_root = \"https://graph.facebook.com/v15.0/oauth/access_token\"\n",
    "    request_url = f'{url_root}?grant_type=fb_exchange_token&client_id={app_id}&client_secret={app_secret}&fb_exchange_token={access_token}'\n",
    "    response = requests.get(request_url)\n",
    "    print('Response status code: ',response.status_code)\n",
    "    try:\n",
    "        response_json = response.json()\n",
    "        response_json['request_url'] = request_url\n",
    "        try:\n",
    "            new_access_token = response_json['access_token']\n",
    "            credentials[access_token_key] = new_access_token\n",
    "            time_to_expiry = timedelta(seconds=response_json['expires_in'])\n",
    "            now = datetime.now()\n",
    "            credentials['token_expiry'] = (datetime.now() + time_to_expiry).strftime(\"%Y-%m-%d %H:%M\")\n",
    "            print('Updated token expiry:', credentials['token_expiry'])\n",
    "            with open(new_credentials_filename,'w') as json_file:\n",
    "                json.dump(credentials, json_file)\n",
    "                print('New credentials file created:', new_credentials_filename)\n",
    "        except:\n",
    "            print('Unable to save new credentials; check request response')\n",
    "        return response_json\n",
    "    except:\n",
    "        print('Unable to get response JSON; check request response')\n",
    "        return response\n",
    "\n",
    "token_response = prolong_access_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the new credentials file loads properly\n",
    "new_credentials_filename='credentials_long_lived.json'\n",
    "\n",
    "with open(new_credentials_filename) as json_file:\n",
    "    credentials2 = json.load(json_file)\n",
    "credentials2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get group feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_response = prolong_access_token(access_token_key='am_access_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint for Strong and Sassy group\n",
    "group_feed_url = \"https://graph.facebook.com/v15.0/2139238999669147/feed?access_token=\"+access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_feed(credentials_json='credentials_long_lived.json', group_id_key='SSC_group_id', \n",
    "    access_token_key='am_access_token',\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - credentials.json : JSON file containing the required credentials:\n",
    "            - group_id for the Facebook group.\n",
    "            - access_token for the user.\n",
    "        - group_id_key and access_token_key (str): Keys to the relevant group_id and access_token in the \n",
    "            JSON file.\n",
    "    Returns:\n",
    "        - \n",
    "    \"\"\"\n",
    "    # Retrieve credentials\n",
    "    with open(\"credentials.json\") as f:\n",
    "        credentials = json.load(f)\n",
    "    access_token = credentials[access_token_key]\n",
    "    group_id = credentials[group_id_key]\n",
    "\n",
    "    # Make API request:\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url = f'{url_root}{group_id}/feed?access_token={access_token}'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_time</th>\n",
       "      <th>message</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-04T15:06:57+0000</td>\n",
       "      <td>Charity event in Roscommon - Saturday 7th Janu...</td>\n",
       "      <td>545505737609234_537318481761293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-29T18:55:49+0000</td>\n",
       "      <td>Nutrition and Training Workshop üí™\\n\\nMy ethos ...</td>\n",
       "      <td>545505737609234_509802061179602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-04T22:39:50+0000</td>\n",
       "      <td>‚ÄòWhy your metabolism is not broken‚Äô\\n\\nReally ...</td>\n",
       "      <td>545505737609234_467470882079387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-18T20:34:20+0000</td>\n",
       "      <td>Anyone else on TikTok? üéØ</td>\n",
       "      <td>545505737609234_455027709990371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-18T06:19:36+0000</td>\n",
       "      <td>Completed it mate ‚úÖ</td>\n",
       "      <td>545505737609234_454597556700053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               created_time  \\\n",
       "0  2023-01-04T15:06:57+0000   \n",
       "1  2022-11-29T18:55:49+0000   \n",
       "2  2022-10-04T22:39:50+0000   \n",
       "3  2022-09-18T20:34:20+0000   \n",
       "4  2022-09-18T06:19:36+0000   \n",
       "\n",
       "                                             message  \\\n",
       "0  Charity event in Roscommon - Saturday 7th Janu...   \n",
       "1  Nutrition and Training Workshop üí™\\n\\nMy ethos ...   \n",
       "2  ‚ÄòWhy your metabolism is not broken‚Äô\\n\\nReally ...   \n",
       "3                           Anyone else on TikTok? üéØ   \n",
       "4                                Completed it mate ‚úÖ   \n",
       "\n",
       "                                id  \n",
       "0  545505737609234_537318481761293  \n",
       "1  545505737609234_509802061179602  \n",
       "2  545505737609234_467470882079387  \n",
       "3  545505737609234_455027709990371  \n",
       "4  545505737609234_454597556700053  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_50p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_time</th>\n",
       "      <th>message</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2021-06-07T10:48:27+0000</td>\n",
       "      <td>https://www.facebook.com/100000504382136/posts...</td>\n",
       "      <td>545505737609234_150885450404600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2021-05-11T05:30:40+0000</td>\n",
       "      <td>Aoibheann's Experience:\\n.\\n.\\nI started the p...</td>\n",
       "      <td>545505737609234_133207925505686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2021-05-06T04:43:38+0000</td>\n",
       "      <td>If you could eat one food and still achieve yo...</td>\n",
       "      <td>545505737609234_129813195845159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2021-05-03T00:12:56+0000</td>\n",
       "      <td>‚ÄºÔ∏èI'm looking for 5 ladies who want to become ...</td>\n",
       "      <td>545505737609234_128188466007632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2021-04-27T04:45:39+0000</td>\n",
       "      <td>Snack Options ‚úÖ\\n.\\n.\\nA question I get so fre...</td>\n",
       "      <td>545505737609234_126493706177108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                created_time  \\\n",
       "45  2021-06-07T10:48:27+0000   \n",
       "46  2021-05-11T05:30:40+0000   \n",
       "47  2021-05-06T04:43:38+0000   \n",
       "48  2021-05-03T00:12:56+0000   \n",
       "49  2021-04-27T04:45:39+0000   \n",
       "\n",
       "                                              message  \\\n",
       "45  https://www.facebook.com/100000504382136/posts...   \n",
       "46  Aoibheann's Experience:\\n.\\n.\\nI started the p...   \n",
       "47  If you could eat one food and still achieve yo...   \n",
       "48  ‚ÄºÔ∏èI'm looking for 5 ladies who want to become ...   \n",
       "49  Snack Options ‚úÖ\\n.\\n.\\nA question I get so fre...   \n",
       "\n",
       "                                 id  \n",
       "45  545505737609234_150885450404600  \n",
       "46  545505737609234_133207925505686  \n",
       "47  545505737609234_129813195845159  \n",
       "48  545505737609234_128188466007632  \n",
       "49  545505737609234_126493706177108  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_50p.iloc[-8:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45    https://www.facebook.com/100000504382136/posts...\n",
       "46    Aoibheann's Experience:\\n.\\n.\\nI started the p...\n",
       "47    If you could eat one food and still achieve yo...\n",
       "48    ‚ÄºÔ∏èI'm looking for 5 ladies who want to become ...\n",
       "49    Snack Options ‚úÖ\\n.\\n.\\nA question I get so fre...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_50p.iloc[-8:-3]['message']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options for feature transformation:\n",
    "* bag of words / CountVectorizer\n",
    "* vectorization using pre-trained model\n",
    "\n",
    "Given the small amount of data and the lack of labels (i.e. no business metrics for this time period), it doesn't make to:\n",
    "* Perform topic modelling\n",
    "* Perform any modelling\n",
    "\n",
    "For now, just do EDA; look at bag of words because word embeddings cannot be interpreted by humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>time</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-06-07 10:48:27+00:00</td>\n",
       "      <td>http  who want make change like this üëáüëáüëá</td>\n",
       "      <td>2021-06-07</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>10:48:27</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-11 05:30:40+00:00</td>\n",
       "      <td>aoibheann experience started the programme wan...</td>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>05:30:40</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-06 04:43:38+00:00</td>\n",
       "      <td>you could eat one food and still achieve your ...</td>\n",
       "      <td>2021-05-06</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>04:43:38</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-03 00:12:56+00:00</td>\n",
       "      <td>‚ÄºÔ∏èi looking for lady who want become strong an...</td>\n",
       "      <td>2021-05-03</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>00:12:56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-27 04:45:39+00:00</td>\n",
       "      <td>snack option question get frequently and area ...</td>\n",
       "      <td>2021-04-27</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>04:45:39</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp  \\\n",
       "0 2021-06-07 10:48:27+00:00   \n",
       "1 2021-05-11 05:30:40+00:00   \n",
       "2 2021-05-06 04:43:38+00:00   \n",
       "3 2021-05-03 00:12:56+00:00   \n",
       "4 2021-04-27 04:45:39+00:00   \n",
       "\n",
       "                                                text        date  year  month  \\\n",
       "0          http  who want make change like this üëáüëáüëá   2021-06-07  2021      6   \n",
       "1  aoibheann experience started the programme wan...  2021-05-11  2021      5   \n",
       "2  you could eat one food and still achieve your ...  2021-05-06  2021      5   \n",
       "3  ‚ÄºÔ∏èi looking for lady who want become strong an...  2021-05-03  2021      5   \n",
       "4  snack option question get frequently and area ...  2021-04-27  2021      4   \n",
       "\n",
       "   day_of_week      time  hour  \n",
       "0            0  10:48:27    10  \n",
       "1            1  05:30:40     5  \n",
       "2            3  04:43:38     4  \n",
       "3            0  00:12:56     0  \n",
       "4            1  04:45:39     4  "
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def preprocess_post_text(docs):\n",
    "    \"\"\"\n",
    "    Prepare data from text documents for NLP:\n",
    "    - Remove all the special characters\n",
    "    - Remove all single characters\n",
    "    - Substitute multiple spaces with single space\n",
    "    - Convert to lowercase\n",
    "\n",
    "    Parameters:\n",
    "    docs (n x 1 array or string): Documents.\n",
    "\n",
    "    Returns: Array of processed docs.\n",
    "    \"\"\"\n",
    "    clean_docs = []\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            # Split text into single words (also gets rid of extra white spaces)\n",
    "            words = word_tokenize(doc)\n",
    "\n",
    "            # Convert to lower case\n",
    "            words = [word.lower() for word in words]\n",
    "\n",
    "            # Lemmatize words (must be done after conversion to lower case)\n",
    "            words = [wnl.lemmatize(word) for word in words]\n",
    "\n",
    "            # Remove all single characters\n",
    "            words = [word for word in words if len(word) > 2]\n",
    "            \n",
    "            # join words back together as a string\n",
    "            words = ''.join([word+' ' for word in words])\n",
    "\n",
    "            # Remove any URLs \n",
    "            words = re.sub(r'\\w*\\.+\\w*', '', words) # Remove periods in middle of word\n",
    "            words = re.sub(r'\\w*/+\\w*', '', words) # remove forward slash in middle of word\n",
    "            words = re.sub(r'\\w*/+\\w*', '', words)\n",
    "\n",
    "            # remove special characters\n",
    "            words = ''.join([char for char in words if char not in string.punctuation])\n",
    "            clean_docs.append(words)\n",
    "        except: # In case value is nan\n",
    "            clean_docs.append(doc)\n",
    "        \n",
    "    return np.array(clean_docs)\n",
    "\n",
    "from custom_nlp import *\n",
    "def post_eda(df):\n",
    "    df = process_post_df(df)\n",
    "    df['text'] = preprocess_post_text(df['text'])\n",
    "\n",
    "    return df\n",
    "\n",
    "post_eda(df_50p.iloc[-8:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>time</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-04 15:06:57+00:00</td>\n",
       "      <td>charity event roscommon saturday 7th january 1...</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15:06:57</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-29 18:55:49+00:00</td>\n",
       "      <td>nutrition and training workshop ethos help wom...</td>\n",
       "      <td>2022-11-29</td>\n",
       "      <td>2022</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>18:55:49</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-04 22:39:50+00:00</td>\n",
       "      <td>why your metabolism not broken really looking ...</td>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>2022</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>22:39:50</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-18 20:34:20+00:00</td>\n",
       "      <td>anyone else tiktok</td>\n",
       "      <td>2022-09-18</td>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>20:34:20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-18 06:19:36+00:00</td>\n",
       "      <td>completed mate</td>\n",
       "      <td>2022-09-18</td>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>06:19:36</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-09-15 19:20:59+00:00</td>\n",
       "      <td>anyone doing blackmores  10km</td>\n",
       "      <td>2022-09-15</td>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>19:20:59</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-08-23 08:49:02+00:00</td>\n",
       "      <td>had the most stunning time fiji anyone been</td>\n",
       "      <td>2022-08-23</td>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>08:49:02</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-08-16 02:04:54+00:00</td>\n",
       "      <td>you can beat join</td>\n",
       "      <td>2022-08-16</td>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>02:04:54</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-08-14 23:22:47+00:00</td>\n",
       "      <td>bingo loco</td>\n",
       "      <td>2022-08-14</td>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>23:22:47</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-08-10 21:13:31+00:00</td>\n",
       "      <td>first night out out awhile</td>\n",
       "      <td>2022-08-10</td>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>21:13:31</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-08-01 05:12:00+00:00</td>\n",
       "      <td>christmas july üéÑ‚ù§Ô∏è</td>\n",
       "      <td>2022-08-01</td>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>05:12:00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022-06-02 20:18:43+00:00</td>\n",
       "      <td>‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è</td>\n",
       "      <td>2022-06-02</td>\n",
       "      <td>2022</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>20:18:43</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-05-29 08:12:02+00:00</td>\n",
       "      <td>love see what you caption this ‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è</td>\n",
       "      <td>2022-05-29</td>\n",
       "      <td>2022</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>08:12:02</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022-02-07 19:29:39+00:00</td>\n",
       "      <td>last the comp spam promise</td>\n",
       "      <td>2022-02-07</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>19:29:39</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022-02-02 19:07:05+00:00</td>\n",
       "      <td>preparing for our first external comp</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>19:07:05</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-01-29 10:16:48+00:00</td>\n",
       "      <td>and lillie allen representing the lady the div...</td>\n",
       "      <td>2022-01-29</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10:16:48</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-01-29 10:16:04+00:00</td>\n",
       "      <td>first house comp the year</td>\n",
       "      <td>2022-01-29</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10:16:04</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-01-27 20:33:16+00:00</td>\n",
       "      <td>nan</td>\n",
       "      <td>2022-01-27</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>20:33:16</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022-01-03 06:40:22+00:00</td>\n",
       "      <td>expires tonight invitation for woman who are s...</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>06:40:22</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-12-19 19:15:22+00:00</td>\n",
       "      <td>run christmas sydney</td>\n",
       "      <td>2021-12-19</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>19:15:22</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2021-11-16 07:47:31+00:00</td>\n",
       "      <td>right who here squat going honest hate squat c...</td>\n",
       "      <td>2021-11-16</td>\n",
       "      <td>2021</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>07:47:31</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021-11-09 06:31:04+00:00</td>\n",
       "      <td>hey all are you clueless about resistance trai...</td>\n",
       "      <td>2021-11-09</td>\n",
       "      <td>2021</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>06:31:04</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2021-11-03 19:17:52+00:00</td>\n",
       "      <td>any goal you like achieve the end 2021</td>\n",
       "      <td>2021-11-03</td>\n",
       "      <td>2021</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>19:17:52</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2021-11-02 23:38:46+00:00</td>\n",
       "      <td>anyone out there still use written diary lone ...</td>\n",
       "      <td>2021-11-02</td>\n",
       "      <td>2021</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>23:38:46</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2021-11-01 18:41:07+00:00</td>\n",
       "      <td>what have you done recently that made you real...</td>\n",
       "      <td>2021-11-01</td>\n",
       "      <td>2021</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>18:41:07</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2021-10-06 23:21:49+00:00</td>\n",
       "      <td>nan</td>\n",
       "      <td>2021-10-06</td>\n",
       "      <td>2021</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>23:21:49</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2021-09-29 05:20:58+00:00</td>\n",
       "      <td>last chance people you re interested webinar y...</td>\n",
       "      <td>2021-09-29</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>05:20:58</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2021-09-20 20:20:52+00:00</td>\n",
       "      <td>anyone interested getting some help and guidan...</td>\n",
       "      <td>2021-09-20</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>20:20:52</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2021-09-09 06:51:17+00:00</td>\n",
       "      <td>ùôÑùô¢ùôñùôúùôûùô£ùôö ùôâùôäùôè ùôõùôöùôñùôßùôûùô£ùôú ùôóùôûùô†ùôûùô£ùôû ùêàùê¶ùêöùê†ùê¢ùêßùêû ùê≠ùê°ùêû ùêüùê´ùêûùêûùêùùê®ùê¶...</td>\n",
       "      <td>2021-09-09</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>06:51:17</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2021-09-02 06:56:40+00:00</td>\n",
       "      <td>one space left for monday 6th you re intereste...</td>\n",
       "      <td>2021-09-02</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>06:56:40</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2021-08-31 05:28:24+00:00</td>\n",
       "      <td>day kickstarter ‚ÄºÔ∏èi looking for lady who want ...</td>\n",
       "      <td>2021-08-31</td>\n",
       "      <td>2021</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>05:28:24</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2021-08-30 05:55:19+00:00</td>\n",
       "      <td>nan</td>\n",
       "      <td>2021-08-30</td>\n",
       "      <td>2021</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>05:55:19</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2021-08-28 10:04:22+00:00</td>\n",
       "      <td>crossfit bodybuilding</td>\n",
       "      <td>2021-08-28</td>\n",
       "      <td>2021</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>10:04:22</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2021-08-18 04:21:26+00:00</td>\n",
       "      <td>did front foot elevated split squat today for ...</td>\n",
       "      <td>2021-08-18</td>\n",
       "      <td>2021</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>04:21:26</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2021-08-16 09:27:13+00:00</td>\n",
       "      <td>you re not already involved get into this grou...</td>\n",
       "      <td>2021-08-16</td>\n",
       "      <td>2021</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>09:27:13</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2021-08-13 01:30:43+00:00</td>\n",
       "      <td>anyone interested why you nt achieve result fo...</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>2021</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>01:30:43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2021-08-10 23:06:29+00:00</td>\n",
       "      <td>favourite high protein breakfast minus egg</td>\n",
       "      <td>2021-08-10</td>\n",
       "      <td>2021</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>23:06:29</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2021-08-01 07:53:38+00:00</td>\n",
       "      <td>‚ÄºÔ∏èi looking for lady who want become strong an...</td>\n",
       "      <td>2021-08-01</td>\n",
       "      <td>2021</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>07:53:38</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2021-07-30 07:57:14+00:00</td>\n",
       "      <td>movie suggestion</td>\n",
       "      <td>2021-07-30</td>\n",
       "      <td>2021</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>07:57:14</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2021-07-19 04:38:56+00:00</td>\n",
       "      <td>http  finally have website and running take no...</td>\n",
       "      <td>2021-07-19</td>\n",
       "      <td>2021</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>04:38:56</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2021-07-16 10:00:08+00:00</td>\n",
       "      <td>you knew covid coming what would you have done...</td>\n",
       "      <td>2021-07-16</td>\n",
       "      <td>2021</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>10:00:08</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2021-07-01 06:13:39+00:00</td>\n",
       "      <td>‚ÄºÔ∏èi looking for lady who want become strong an...</td>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>2021</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>06:13:39</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2021-06-30 22:53:38+00:00</td>\n",
       "      <td>nan</td>\n",
       "      <td>2021-06-30</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>22:53:38</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2021-06-26 11:51:06+00:00</td>\n",
       "      <td>anyone interested gaining some nutrition knowl...</td>\n",
       "      <td>2021-06-26</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>11:51:06</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2021-06-16 04:22:08+00:00</td>\n",
       "      <td>what are people favourite group apart</td>\n",
       "      <td>2021-06-16</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>04:22:08</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2021-06-07 10:48:27+00:00</td>\n",
       "      <td>http  who want make change like this üëáüëáüëá</td>\n",
       "      <td>2021-06-07</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>10:48:27</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2021-05-11 05:30:40+00:00</td>\n",
       "      <td>aoibheann experience started the programme wan...</td>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>05:30:40</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2021-05-06 04:43:38+00:00</td>\n",
       "      <td>you could eat one food and still achieve your ...</td>\n",
       "      <td>2021-05-06</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>04:43:38</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2021-05-03 00:12:56+00:00</td>\n",
       "      <td>‚ÄºÔ∏èi looking for lady who want become strong an...</td>\n",
       "      <td>2021-05-03</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>00:12:56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2021-04-27 04:45:39+00:00</td>\n",
       "      <td>snack option question get frequently and area ...</td>\n",
       "      <td>2021-04-27</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>04:45:39</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2021-03-12 00:33:05+00:00</td>\n",
       "      <td>nan</td>\n",
       "      <td>2021-03-12</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>00:33:05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2021-03-12 00:25:29+00:00</td>\n",
       "      <td>nan</td>\n",
       "      <td>2021-03-12</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>00:25:29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1991-02-12 08:00:00+00:00</td>\n",
       "      <td>nan</td>\n",
       "      <td>1991-02-12</td>\n",
       "      <td>1991</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>08:00:00</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp  \\\n",
       "0  2023-01-04 15:06:57+00:00   \n",
       "1  2022-11-29 18:55:49+00:00   \n",
       "2  2022-10-04 22:39:50+00:00   \n",
       "3  2022-09-18 20:34:20+00:00   \n",
       "4  2022-09-18 06:19:36+00:00   \n",
       "5  2022-09-15 19:20:59+00:00   \n",
       "6  2022-08-23 08:49:02+00:00   \n",
       "7  2022-08-16 02:04:54+00:00   \n",
       "8  2022-08-14 23:22:47+00:00   \n",
       "9  2022-08-10 21:13:31+00:00   \n",
       "10 2022-08-01 05:12:00+00:00   \n",
       "11 2022-06-02 20:18:43+00:00   \n",
       "12 2022-05-29 08:12:02+00:00   \n",
       "13 2022-02-07 19:29:39+00:00   \n",
       "14 2022-02-02 19:07:05+00:00   \n",
       "15 2022-01-29 10:16:48+00:00   \n",
       "16 2022-01-29 10:16:04+00:00   \n",
       "17 2022-01-27 20:33:16+00:00   \n",
       "18 2022-01-03 06:40:22+00:00   \n",
       "19 2021-12-19 19:15:22+00:00   \n",
       "20 2021-11-16 07:47:31+00:00   \n",
       "21 2021-11-09 06:31:04+00:00   \n",
       "22 2021-11-03 19:17:52+00:00   \n",
       "23 2021-11-02 23:38:46+00:00   \n",
       "24 2021-11-01 18:41:07+00:00   \n",
       "25 2021-10-06 23:21:49+00:00   \n",
       "26 2021-09-29 05:20:58+00:00   \n",
       "27 2021-09-20 20:20:52+00:00   \n",
       "28 2021-09-09 06:51:17+00:00   \n",
       "29 2021-09-02 06:56:40+00:00   \n",
       "30 2021-08-31 05:28:24+00:00   \n",
       "31 2021-08-30 05:55:19+00:00   \n",
       "32 2021-08-28 10:04:22+00:00   \n",
       "33 2021-08-18 04:21:26+00:00   \n",
       "34 2021-08-16 09:27:13+00:00   \n",
       "35 2021-08-13 01:30:43+00:00   \n",
       "36 2021-08-10 23:06:29+00:00   \n",
       "37 2021-08-01 07:53:38+00:00   \n",
       "38 2021-07-30 07:57:14+00:00   \n",
       "39 2021-07-19 04:38:56+00:00   \n",
       "40 2021-07-16 10:00:08+00:00   \n",
       "41 2021-07-01 06:13:39+00:00   \n",
       "42 2021-06-30 22:53:38+00:00   \n",
       "43 2021-06-26 11:51:06+00:00   \n",
       "44 2021-06-16 04:22:08+00:00   \n",
       "45 2021-06-07 10:48:27+00:00   \n",
       "46 2021-05-11 05:30:40+00:00   \n",
       "47 2021-05-06 04:43:38+00:00   \n",
       "48 2021-05-03 00:12:56+00:00   \n",
       "49 2021-04-27 04:45:39+00:00   \n",
       "50 2021-03-12 00:33:05+00:00   \n",
       "51 2021-03-12 00:25:29+00:00   \n",
       "52 1991-02-12 08:00:00+00:00   \n",
       "\n",
       "                                                 text        date  year  \\\n",
       "0   charity event roscommon saturday 7th january 1...  2023-01-04  2023   \n",
       "1   nutrition and training workshop ethos help wom...  2022-11-29  2022   \n",
       "2   why your metabolism not broken really looking ...  2022-10-04  2022   \n",
       "3                                 anyone else tiktok   2022-09-18  2022   \n",
       "4                                     completed mate   2022-09-18  2022   \n",
       "5                      anyone doing blackmores  10km   2022-09-15  2022   \n",
       "6        had the most stunning time fiji anyone been   2022-08-23  2022   \n",
       "7                                  you can beat join   2022-08-16  2022   \n",
       "8                                         bingo loco   2022-08-14  2022   \n",
       "9                         first night out out awhile   2022-08-10  2022   \n",
       "10                                christmas july üéÑ‚ù§Ô∏è   2022-08-01  2022   \n",
       "11                                            ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è   2022-06-02  2022   \n",
       "12             love see what you caption this ‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è   2022-05-29  2022   \n",
       "13                        last the comp spam promise   2022-02-07  2022   \n",
       "14             preparing for our first external comp   2022-02-02  2022   \n",
       "15  and lillie allen representing the lady the div...  2022-01-29  2022   \n",
       "16                         first house comp the year   2022-01-29  2022   \n",
       "17                                                nan  2022-01-27  2022   \n",
       "18  expires tonight invitation for woman who are s...  2022-01-03  2022   \n",
       "19                              run christmas sydney   2021-12-19  2021   \n",
       "20  right who here squat going honest hate squat c...  2021-11-16  2021   \n",
       "21  hey all are you clueless about resistance trai...  2021-11-09  2021   \n",
       "22            any goal you like achieve the end 2021   2021-11-03  2021   \n",
       "23  anyone out there still use written diary lone ...  2021-11-02  2021   \n",
       "24  what have you done recently that made you real...  2021-11-01  2021   \n",
       "25                                                nan  2021-10-06  2021   \n",
       "26  last chance people you re interested webinar y...  2021-09-29  2021   \n",
       "27  anyone interested getting some help and guidan...  2021-09-20  2021   \n",
       "28  ùôÑùô¢ùôñùôúùôûùô£ùôö ùôâùôäùôè ùôõùôöùôñùôßùôûùô£ùôú ùôóùôûùô†ùôûùô£ùôû ùêàùê¶ùêöùê†ùê¢ùêßùêû ùê≠ùê°ùêû ùêüùê´ùêûùêûùêùùê®ùê¶...  2021-09-09  2021   \n",
       "29  one space left for monday 6th you re intereste...  2021-09-02  2021   \n",
       "30  day kickstarter ‚ÄºÔ∏èi looking for lady who want ...  2021-08-31  2021   \n",
       "31                                                nan  2021-08-30  2021   \n",
       "32                             crossfit bodybuilding   2021-08-28  2021   \n",
       "33  did front foot elevated split squat today for ...  2021-08-18  2021   \n",
       "34  you re not already involved get into this grou...  2021-08-16  2021   \n",
       "35  anyone interested why you nt achieve result fo...  2021-08-13  2021   \n",
       "36       favourite high protein breakfast minus egg    2021-08-10  2021   \n",
       "37  ‚ÄºÔ∏èi looking for lady who want become strong an...  2021-08-01  2021   \n",
       "38                                  movie suggestion   2021-07-30  2021   \n",
       "39  http  finally have website and running take no...  2021-07-19  2021   \n",
       "40  you knew covid coming what would you have done...  2021-07-16  2021   \n",
       "41  ‚ÄºÔ∏èi looking for lady who want become strong an...  2021-07-01  2021   \n",
       "42                                                nan  2021-06-30  2021   \n",
       "43  anyone interested gaining some nutrition knowl...  2021-06-26  2021   \n",
       "44             what are people favourite group apart   2021-06-16  2021   \n",
       "45          http  who want make change like this üëáüëáüëá   2021-06-07  2021   \n",
       "46  aoibheann experience started the programme wan...  2021-05-11  2021   \n",
       "47  you could eat one food and still achieve your ...  2021-05-06  2021   \n",
       "48  ‚ÄºÔ∏èi looking for lady who want become strong an...  2021-05-03  2021   \n",
       "49  snack option question get frequently and area ...  2021-04-27  2021   \n",
       "50                                                nan  2021-03-12  2021   \n",
       "51                                                nan  2021-03-12  2021   \n",
       "52                                                nan  1991-02-12  1991   \n",
       "\n",
       "    month  day_of_week      time  hour  \n",
       "0       1            2  15:06:57    15  \n",
       "1      11            1  18:55:49    18  \n",
       "2      10            1  22:39:50    22  \n",
       "3       9            6  20:34:20    20  \n",
       "4       9            6  06:19:36     6  \n",
       "5       9            3  19:20:59    19  \n",
       "6       8            1  08:49:02     8  \n",
       "7       8            1  02:04:54     2  \n",
       "8       8            6  23:22:47    23  \n",
       "9       8            2  21:13:31    21  \n",
       "10      8            0  05:12:00     5  \n",
       "11      6            3  20:18:43    20  \n",
       "12      5            6  08:12:02     8  \n",
       "13      2            0  19:29:39    19  \n",
       "14      2            2  19:07:05    19  \n",
       "15      1            5  10:16:48    10  \n",
       "16      1            5  10:16:04    10  \n",
       "17      1            3  20:33:16    20  \n",
       "18      1            0  06:40:22     6  \n",
       "19     12            6  19:15:22    19  \n",
       "20     11            1  07:47:31     7  \n",
       "21     11            1  06:31:04     6  \n",
       "22     11            2  19:17:52    19  \n",
       "23     11            1  23:38:46    23  \n",
       "24     11            0  18:41:07    18  \n",
       "25     10            2  23:21:49    23  \n",
       "26      9            2  05:20:58     5  \n",
       "27      9            0  20:20:52    20  \n",
       "28      9            3  06:51:17     6  \n",
       "29      9            3  06:56:40     6  \n",
       "30      8            1  05:28:24     5  \n",
       "31      8            0  05:55:19     5  \n",
       "32      8            5  10:04:22    10  \n",
       "33      8            2  04:21:26     4  \n",
       "34      8            0  09:27:13     9  \n",
       "35      8            4  01:30:43     1  \n",
       "36      8            1  23:06:29    23  \n",
       "37      8            6  07:53:38     7  \n",
       "38      7            4  07:57:14     7  \n",
       "39      7            0  04:38:56     4  \n",
       "40      7            4  10:00:08    10  \n",
       "41      7            3  06:13:39     6  \n",
       "42      6            2  22:53:38    22  \n",
       "43      6            5  11:51:06    11  \n",
       "44      6            2  04:22:08     4  \n",
       "45      6            0  10:48:27    10  \n",
       "46      5            1  05:30:40     5  \n",
       "47      5            3  04:43:38     4  \n",
       "48      5            0  00:12:56     0  \n",
       "49      4            1  04:45:39     4  \n",
       "50      3            4  00:33:05     0  \n",
       "51      3            4  00:25:29     0  \n",
       "52      2            1  08:00:00     8  "
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "post_eda(df_50p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86576fc1f72bb8252e2f1578cc878ed2c12b40840637cdef083c8fb979cf67d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
