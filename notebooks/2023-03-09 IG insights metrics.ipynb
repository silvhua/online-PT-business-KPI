{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "[]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import json_normalize  \n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\custom_python\")\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\src\")\n",
    "from silvhua import *\n",
    "from datetime import datetime\n",
    "from plotly.subplots import make_subplots\n",
    "import requests\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"credentials.json\") as f:\n",
    "#     credentials = json.load(f)\n",
    "\n",
    "# ig_user_id = credentials['am_ig_user_id']\n",
    "# access_token = credentials['am_ig_access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.json\") as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "ig_user_id = credentials['ig_user_id']\n",
    "access_token = credentials['access_token']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `update_ig_account_insights`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://graph.facebook.com/v15.0/17841401256046961/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1677657600.0&since=1675238400.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "Requesting page 2...\n",
      "\tResponse status code:  200\n",
      "Number of days of data: 57\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_account_insights.sav\n",
      "Time completed: 2023-03-09 12:49:03.466033\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_account_insights.sav\n",
      "Time completed: 2023-03-09 12:49:03.468023\n"
     ]
    }
   ],
   "source": [
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = loadpickle(filename, csv_path)\n",
    "        previous_since = df.sort_values('timestamp')\n",
    "    except:\n",
    "        pass\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter >= since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = response\n",
    "    if filename:\n",
    "        filename = f'{filename}_account_insights'\n",
    "        try:\n",
    "            savepickle(df, filename, 'sav', csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename = 'silvialiftsweights'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-09 13:57:31.405137\n",
      "Time completed: 2023-03-09 13:57:31.407139\n",
      "previous until: 2023-03-01 08:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    if filename:\n",
    "        filename = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename+'.sav', csv_path)\n",
    "        response_json_dict = loadpickle(filename+'.sav', json_path)\n",
    "        last_json_page = max(response_json_dict.keys())\n",
    "        previous_since = response_json_dict[last_json_page]['data'][0]['values'][0]['end_time']\n",
    "        previous_since = datetime.strptime(previous_since, \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        \n",
    "        previous_until = response_json_dict[1]['data'][0]['values'][-1]['end_time']\n",
    "        previous_until = datetime.strptime(previous_until, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        print('previous until:', previous_until)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if (previous_since.date() > since.date()) | (previous_until.date() < until.date()):\n",
    "        print('Fetching new account insights')\n",
    "    # if since_parameter:\n",
    "    #     url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    # else:\n",
    "    #     url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "    #     since_parameter = since\n",
    "\n",
    "    # url = url_without_token+'&access_token='+access_token\n",
    "    # print(url_without_token)\n",
    "    \n",
    "    # response_json_dict = dict()\n",
    "    # df_list = []\n",
    "    # earliest_end_time = None\n",
    "    # page = 1\n",
    "    # while (since_parameter >= since):\n",
    "    #     response = requests.get(url)\n",
    "    #     print(f'Requesting page {page}...')\n",
    "    #     print('\\tResponse status code: ',response.status_code)\n",
    "    #     response_json_dict[page] = response.json()\n",
    "    #     if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "    #         print(response_json_dict[page]['error'])\n",
    "    #         break\n",
    "    #     try:\n",
    "    #         df_list.append(\n",
    "    #             pd.concat([\n",
    "    #             json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "    #             json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "    #             ], axis=1)\n",
    "    #         )\n",
    "    #     except:\n",
    "    #         print('No data in request response for page', page)\n",
    "    #     earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "    #     since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "\n",
    "    #     try:\n",
    "    #         next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "    #         if next_endpoint+access_token != url:\n",
    "    #             url = next_endpoint\n",
    "    #         else:\n",
    "    #             print('end')\n",
    "    #             break\n",
    "    #     except: \n",
    "    #         break\n",
    "    #     page +=1\n",
    "    # try:\n",
    "    #     df = pd.concat(df_list)\n",
    "    #     df = df.reset_index(drop=True)\n",
    "    #     print('Number of days of data:',len(df))\n",
    "    # except:\n",
    "    #     df = response\n",
    "    # if filename:\n",
    "    #     try:\n",
    "    #         savepickle(df, filename+'_df', 'sav', csv_path)\n",
    "    #         savepickle(response_json_dict,filename,'sav',json_path)\n",
    "    #     except:\n",
    "    #         print('Unable to save outputs')\n",
    "    return df, response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename = 'silvialiftsweights'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename)\n",
    "# response_json_dict\n",
    "\n",
    "# data = update_ig_account_insights(\n",
    "#     ig_user_id, access_token, since=since, until=until, filename=filename)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response_json_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([3, 4])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip([key+len(response_json_dict) for key in response_json_dict.keys()], response_json_dict.values())).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{**response_json_dict, **dict(zip([key+len(response_json_dict) for key in response_json_dict.keys()], response_json_dict.values()))}.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter >= since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = response\n",
    "    if filename:\n",
    "        try:\n",
    "            save_csv(df,filename,csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-09 14:37:34.968390\n",
      "Time completed: 2023-03-09 14:37:34.968390\n",
      "Fetching new account insights from 2023-01-01 to 2023-01-04\n",
      "https://graph.facebook.com/v15.0/17841401256046961/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1672819200.0&since=1672560000.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "Requesting page 2...\n",
      "\tResponse status code:  200\n",
      "Number of days of data: 7\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    if filename:\n",
    "        filename = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename+'.sav', csv_path)\n",
    "        response_json_dict = loadpickle(filename+'.sav', json_path)\n",
    "        last_json_page = max(response_json_dict.keys())\n",
    "        previous_since = response_json_dict[last_json_page]['data'][0]['values'][0]['end_time']\n",
    "        previous_since = datetime.strptime(previous_since, \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        \n",
    "        previous_until = response_json_dict[1]['data'][0]['values'][-1]['end_time']\n",
    "        previous_until = datetime.strptime(previous_until, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    except:\n",
    "        pass\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'Fetching new account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=since, until=previous_since)\n",
    "        df = pd.concat([df, new_insights_df])\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "        \n",
    "    return df, response_json_dict, new_insights_response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename = 'silvialiftsweights'\n",
    "\n",
    "data, response_json_dict, new_insights_response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-01', until=until, filename=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_insights_response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json_dict[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json_dict[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter >= since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = response\n",
    "    if filename:\n",
    "        try:\n",
    "            save_csv(df,filename,csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-14 13:10:43.161301\n",
      "Time completed: 2023-03-14 13:10:43.180351\n",
      "Fetching new account insights from 2023-01-01 to 2023-01-04\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1672819200.0&since=1672560000.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-01 00:00:00\n",
      "Requesting page 2...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2022-12-29 00:00:00\n",
      "Number of days of data: 7\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    if filename:\n",
    "        filename = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename+'.sav', csv_path)\n",
    "        response_json_dict = loadpickle(filename+'.sav', json_path)\n",
    "        last_json_page = max(response_json_dict.keys())\n",
    "        previous_since = response_json_dict[last_json_page]['data'][0]['values'][0]['end_time']\n",
    "        previous_since = datetime.strptime(previous_since, \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        \n",
    "        previous_until = response_json_dict[1]['data'][0]['values'][-1]['end_time']\n",
    "        previous_until = datetime.strptime(previous_until, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    except:\n",
    "        pass\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'Fetching new account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=since, until=previous_since)\n",
    "        df = pd.concat([df, new_insights_df])\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "        \n",
    "    # if (previous_until.date() < until.date()):\n",
    "\n",
    "    # if since_parameter:\n",
    "    #     url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    # else:\n",
    "    #     url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "    #     since_parameter = since\n",
    "\n",
    "    # url = url_without_token+'&access_token='+access_token\n",
    "    # print(url_without_token)\n",
    "    \n",
    "    # response_json_dict = dict()\n",
    "    # df_list = []\n",
    "    # earliest_end_time = None\n",
    "    # page = 1\n",
    "    # while (since_parameter >= since):\n",
    "    #     response = requests.get(url)\n",
    "    #     print(f'Requesting page {page}...')\n",
    "    #     print('\\tResponse status code: ',response.status_code)\n",
    "    #     response_json_dict[page] = response.json()\n",
    "    #     if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "    #         print(response_json_dict[page]['error'])\n",
    "    #         break\n",
    "    #     try:\n",
    "    #         df_list.append(\n",
    "    #             pd.concat([\n",
    "    #             json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "    #             json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "    #             ], axis=1)\n",
    "    #         )\n",
    "    #     except:\n",
    "    #         print('No data in request response for page', page)\n",
    "    #     earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "    #     since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "\n",
    "    #     try:\n",
    "    #         next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "    #         if next_endpoint+access_token != url:\n",
    "    #             url = next_endpoint\n",
    "    #         else:\n",
    "    #             print('end')\n",
    "    #             break\n",
    "    #     except: \n",
    "    #         break\n",
    "    #     page +=1\n",
    "    # try:\n",
    "    #     df = pd.concat(df_list)\n",
    "    #     df = df.reset_index(drop=True)\n",
    "    #     print('Number of days of data:',len(df))\n",
    "    # except:\n",
    "    #     df = response\n",
    "    # if filename:\n",
    "    #     try:\n",
    "    #         savepickle(df, filename+'_df', 'sav', csv_path)\n",
    "    #         savepickle(response_json_dict,filename,'sav',json_path)\n",
    "    #     except:\n",
    "    #         print('Unable to save outputs')\n",
    "    return df, response_json_dict, new_insights_response_json_dict\n",
    "\n",
    "since = '2023-01-01'\n",
    "until = '2023-03-01'\n",
    "filename = 'silvialiftsweights'\n",
    "\n",
    "data, response_json_dict, new_insights_response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-01', until=until, filename=filename)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None, since_parameter=None,\n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    \n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter < since:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    elif since_parameter == None:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    print('since parameter:', since_parameter)\n",
    "    print('since:', since)\n",
    "    while (since_parameter > since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = df_list \n",
    "    if filename:\n",
    "        try:\n",
    "            save_csv(df,filename,csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-14 20:51:26.589541\n",
      "Time completed: 2023-03-14 20:51:26.590545\n",
      "Fetching new account insights from 2023-01-01 to 2023-01-04\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1672819200.0\n",
      "since parameter: 2023-01-30 00:00:00\n",
      "since: 2023-01-01 00:00:00\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-02 00:00:00\n",
      "Requesting page 2...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2022-12-31 00:00:00\n",
      "Number of days of data: 5\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    if filename:\n",
    "        filename = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename+'.sav', csv_path)\n",
    "        response_json_dict = loadpickle(filename+'.sav', json_path)\n",
    "        last_json_page = max(response_json_dict.keys())\n",
    "        previous_since_tz = response_json_dict[last_json_page]['data'][0]['values'][0]['end_time']\n",
    "        # previous_since_tz = datetime.strptime(previous_since_tz, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', previous_since_tz), \"%Y-%m-%d\")\n",
    "        # print('since parameter:', since_parameter)\n",
    "        previous_since = datetime.strptime(previous_since_tz, \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        \n",
    "        previous_until = response_json_dict[1]['data'][0]['values'][-1]['end_time']\n",
    "        previous_until = datetime.strptime(previous_until, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    except:\n",
    "        pass\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'Fetching new account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=since, until=previous_since, since_parameter=since_parameter)\n",
    "        try:\n",
    "            df = pd.concat([df, new_insights_df])\n",
    "        except:\n",
    "            df = new_insights_df\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "        \n",
    "    # if (previous_until.date() < until.date()):\n",
    "\n",
    "    # if since_parameter:\n",
    "    #     url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    # else:\n",
    "    #     url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "    #     since_parameter = since\n",
    "\n",
    "    # url = url_without_token+'&access_token='+access_token\n",
    "    # print(url_without_token)\n",
    "    \n",
    "    # response_json_dict = dict()\n",
    "    # df_list = []\n",
    "    # earliest_end_time = None\n",
    "    # page = 1\n",
    "    # while (since_parameter >= since):\n",
    "    #     response = requests.get(url)\n",
    "    #     print(f'Requesting page {page}...')\n",
    "    #     print('\\tResponse status code: ',response.status_code)\n",
    "    #     response_json_dict[page] = response.json()\n",
    "    #     if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "    #         print(response_json_dict[page]['error'])\n",
    "    #         break\n",
    "    #     try:\n",
    "    #         df_list.append(\n",
    "    #             pd.concat([\n",
    "    #             json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "    #             json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "    #             ], axis=1)\n",
    "    #         )\n",
    "    #     except:\n",
    "    #         print('No data in request response for page', page)\n",
    "    #     earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "    #     since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "\n",
    "    #     try:\n",
    "    #         next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "    #         if next_endpoint+access_token != url:\n",
    "    #             url = next_endpoint\n",
    "    #         else:\n",
    "    #             print('end')\n",
    "    #             break\n",
    "    #     except: \n",
    "    #         break\n",
    "    #     page +=1\n",
    "    # try:\n",
    "    #     df = pd.concat(df_list)\n",
    "    #     df = df.reset_index(drop=True)\n",
    "    #     print('Number of days of data:',len(df))\n",
    "    # except:\n",
    "    #     df = response\n",
    "    # if filename:\n",
    "    #     try:\n",
    "    #         savepickle(df, filename+'_df', 'sav', csv_path)\n",
    "    #         savepickle(response_json_dict,filename,'sav',json_path)\n",
    "    #     except:\n",
    "    #         print('Unable to save outputs')\n",
    "    return df, response_json_dict, new_insights_response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename = 'silvialiftsweights'\n",
    "\n",
    "data, response_json_dict, new_insights_response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-01', until=until, filename=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12382</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>4913</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11649</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "      <td>5221</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13558</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "      <td>5155</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8538</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "      <td>4588</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12797</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "      <td>5669</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-02T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>169</td>\n",
       "      <td>2023-01-03T08:00:00+0000</td>\n",
       "      <td>132</td>\n",
       "      <td>2023-01-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141</td>\n",
       "      <td>2023-01-04T08:00:00+0000</td>\n",
       "      <td>103</td>\n",
       "      <td>2023-01-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>2022-12-31T08:00:00+0000</td>\n",
       "      <td>17</td>\n",
       "      <td>2022-12-31T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>2023-01-01T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    impressions_value      impressions_end_time  reach_value  \\\n",
       "0               12382  2023-02-01T08:00:00+0000         4913   \n",
       "1               11649  2023-02-02T08:00:00+0000         5221   \n",
       "2               13558  2023-02-03T08:00:00+0000         5155   \n",
       "3                8538  2023-02-04T08:00:00+0000         4588   \n",
       "4               12797  2023-02-05T08:00:00+0000         5669   \n",
       "..                ...                       ...          ...   \n",
       "0                   3  2023-01-02T08:00:00+0000            3   \n",
       "1                 169  2023-01-03T08:00:00+0000          132   \n",
       "2                 141  2023-01-04T08:00:00+0000          103   \n",
       "3                  18  2022-12-31T08:00:00+0000           17   \n",
       "4                  18  2023-01-01T08:00:00+0000            4   \n",
       "\n",
       "              reach_end_time  \n",
       "0   2023-02-01T08:00:00+0000  \n",
       "1   2023-02-02T08:00:00+0000  \n",
       "2   2023-02-03T08:00:00+0000  \n",
       "3   2023-02-04T08:00:00+0000  \n",
       "4   2023-02-05T08:00:00+0000  \n",
       "..                       ...  \n",
       "0   2023-01-02T08:00:00+0000  \n",
       "1   2023-01-03T08:00:00+0000  \n",
       "2   2023-01-04T08:00:00+0000  \n",
       "3   2022-12-31T08:00:00+0000  \n",
       "4   2023-01-01T08:00:00+0000  \n",
       "\n",
       "[62 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 6\n",
    "Adds insights from older dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None, \n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since + timedelta(days=1)\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    print('since parameter:', since_parameter)\n",
    "    print('since:', since)\n",
    "    while (since_parameter > since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = df_list \n",
    "    if filename:\n",
    "        try:\n",
    "            save_csv(df,filename,csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1677657600.0&since=1675065600.0\n",
      "since parameter: 2023-01-30 00:00:00\n",
      "since: 2023-01-01 00:00:00\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-30 00:00:00\n",
      "Requesting page 2...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2022-12-31 00:00:00\n",
      "Number of days of data: 61\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "      <td>141</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "      <td>43</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>169</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>38</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    impressions_value      impressions_end_time  reach_value  \\\n",
       "0                 196  2023-01-30T08:00:00+0000          141   \n",
       "1                  45  2023-01-31T08:00:00+0000           43   \n",
       "2                  14  2023-02-01T08:00:00+0000            9   \n",
       "3                   5  2023-02-02T08:00:00+0000            4   \n",
       "4                 169  2023-02-03T08:00:00+0000          109   \n",
       "..                ...                       ...          ...   \n",
       "56                  0  2023-01-25T08:00:00+0000            0   \n",
       "57                  2  2023-01-26T08:00:00+0000            2   \n",
       "58                 38  2023-01-27T08:00:00+0000            1   \n",
       "59                 29  2023-01-28T08:00:00+0000            1   \n",
       "60                  5  2023-01-29T08:00:00+0000            4   \n",
       "\n",
       "              reach_end_time  \n",
       "0   2023-01-30T08:00:00+0000  \n",
       "1   2023-01-31T08:00:00+0000  \n",
       "2   2023-02-01T08:00:00+0000  \n",
       "3   2023-02-02T08:00:00+0000  \n",
       "4   2023-02-03T08:00:00+0000  \n",
       "..                       ...  \n",
       "56  2023-01-25T08:00:00+0000  \n",
       "57  2023-01-26T08:00:00+0000  \n",
       "58  2023-01-27T08:00:00+0000  \n",
       "59  2023-01-28T08:00:00+0000  \n",
       "60  2023-01-29T08:00:00+0000  \n",
       "\n",
       "[61 rows x 4 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "since = '2023-01-01'\n",
    "until = '2023-03-01'\n",
    "filename = 'silvialiftsweights'\n",
    "\n",
    "data2, response_json_dict2 = get_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=None)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>18</td>\n",
       "      <td>2022-12-31T08:00:00+0000</td>\n",
       "      <td>17</td>\n",
       "      <td>2022-12-31T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>18</td>\n",
       "      <td>2023-01-01T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-02T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>169</td>\n",
       "      <td>2023-01-03T08:00:00+0000</td>\n",
       "      <td>132</td>\n",
       "      <td>2023-01-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>141</td>\n",
       "      <td>2023-01-04T08:00:00+0000</td>\n",
       "      <td>103</td>\n",
       "      <td>2023-01-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "      <td>95</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "      <td>28</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    impressions_value      impressions_end_time  reach_value  \\\n",
       "31                 18  2022-12-31T08:00:00+0000           17   \n",
       "32                 18  2023-01-01T08:00:00+0000            4   \n",
       "33                  3  2023-01-02T08:00:00+0000            3   \n",
       "34                169  2023-01-03T08:00:00+0000          132   \n",
       "35                141  2023-01-04T08:00:00+0000          103   \n",
       "..                ...                       ...          ...   \n",
       "26                  1  2023-02-25T08:00:00+0000            1   \n",
       "27                 32  2023-02-26T08:00:00+0000            9   \n",
       "28                109  2023-02-27T08:00:00+0000           95   \n",
       "29                 29  2023-02-28T08:00:00+0000           28   \n",
       "30                289  2023-03-01T08:00:00+0000          170   \n",
       "\n",
       "              reach_end_time  \n",
       "31  2022-12-31T08:00:00+0000  \n",
       "32  2023-01-01T08:00:00+0000  \n",
       "33  2023-01-02T08:00:00+0000  \n",
       "34  2023-01-03T08:00:00+0000  \n",
       "35  2023-01-04T08:00:00+0000  \n",
       "..                       ...  \n",
       "26  2023-02-25T08:00:00+0000  \n",
       "27  2023-02-26T08:00:00+0000  \n",
       "28  2023-02-27T08:00:00+0000  \n",
       "29  2023-02-28T08:00:00+0000  \n",
       "30  2023-03-01T08:00:00+0000  \n",
       "\n",
       "[61 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.sort_values(['impressions_end_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-14 21:40:53.728313\n",
      "Time completed: 2023-03-14 21:40:53.733351\n",
      "Fetching new account insights from 2023-01-01 to 2023-01-04\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1672819200.0&since=1672560000.0\n",
      "since parameter: 2023-01-02 00:00:00\n",
      "since: 2023-01-01 00:00:00\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-01 00:00:00\n",
      "Number of days of data: 4\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    if filename:\n",
    "        filename = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename+'.sav', csv_path)\n",
    "        response_json_dict = loadpickle(filename+'.sav', json_path)\n",
    "        last_json_page = max(response_json_dict.keys())\n",
    "        previous_since_tz = response_json_dict[last_json_page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', previous_since_tz), \"%Y-%m-%d\")\n",
    "        previous_since = datetime.strptime(previous_since_tz, \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        \n",
    "        previous_until = response_json_dict[1]['data'][0]['values'][-1]['end_time']\n",
    "        previous_until = datetime.strptime(previous_until, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    except:\n",
    "        pass\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'Fetching new account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=since, until=previous_since)\n",
    "        try:\n",
    "            df = pd.concat([df, new_insights_df])\n",
    "        except:\n",
    "            df = new_insights_df\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "        \n",
    "    # if filename:\n",
    "    #     try:\n",
    "    #         savepickle(df, filename+'_df', 'sav', csv_path)\n",
    "    #         savepickle(response_json_dict,filename,'sav',json_path)\n",
    "    #     except:\n",
    "    #         print('Unable to save outputs')\n",
    "    return df, response_json_dict, new_insights_response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename = 'silvialiftsweights'\n",
    "\n",
    "data, response_json_dict, new_insights_response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-01', until=until, filename=filename)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 7\n",
    "Pull insights from more recent dates than what was previously saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None, \n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since + timedelta(days=1)\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    print('since parameter:', since_parameter)\n",
    "    print('since:', since)\n",
    "    while (since_parameter > since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = df_list \n",
    "    if filename:\n",
    "        try:\n",
    "            save_csv(df,filename,csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-14 21:53:40.632006\n",
      "Time completed: 2023-03-14 21:53:40.632006\n",
      "Fetching older account insights from 2023-01-01 to 2023-01-04\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1672819200.0&since=1672560000.0\n",
      "since parameter: 2023-01-02 00:00:00\n",
      "since: 2023-01-01 00:00:00\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-01 00:00:00\n",
      "Number of days of data: 4\n",
      "Fetching newer account insights from 2023-03-01 to 2023-03-05\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1678003200.0&since=1677657600.0\n",
      "since parameter: 2023-03-02 00:00:00\n",
      "since: 2023-03-01 00:00:00\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-03-01 00:00:00\n",
      "Number of days of data: 5\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    if filename:\n",
    "        filename = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename+'.sav', csv_path)\n",
    "        response_json_dict = loadpickle(filename+'.sav', json_path)\n",
    "        last_json_page = max(response_json_dict.keys())\n",
    "        previous_since_tz = response_json_dict[last_json_page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', previous_since_tz), \"%Y-%m-%d\")\n",
    "        previous_since = datetime.strptime(previous_since_tz, \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        \n",
    "        previous_until = response_json_dict[1]['data'][0]['values'][-1]['end_time']\n",
    "        previous_until = datetime.strptime(previous_until, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    except:\n",
    "        pass\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'Fetching older account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        older_insights_df, older_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=since, until=previous_since)\n",
    "        try:\n",
    "            df = pd.concat([df, older_insights_df])\n",
    "        except:\n",
    "            df = older_insights_df\n",
    "        older_insights_response_json_dict = dict( # Update the keys of older_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in older_insights_response_json_dict.keys()], older_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **older_insights_response_json_dict}\n",
    "    if (previous_until.date() < until.date()):\n",
    "        print(f'Fetching newer account insights from {datetime.strftime(previous_until, \"%Y-%m-%d\")} to {datetime.strftime(until, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=previous_until, until=until)\n",
    "        try:\n",
    "            df = pd.concat([df, new_insights_df])\n",
    "        except:\n",
    "            df = new_insights_df\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "\n",
    "        \n",
    "    # if filename:\n",
    "    #     try:\n",
    "    #         savepickle(df, filename+'_df', 'sav', csv_path)\n",
    "    #         savepickle(response_json_dict,filename,'sav',json_path)\n",
    "    #     except:\n",
    "    #         print('Unable to save outputs')\n",
    "    return df, response_json_dict, older_insights_response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename = 'silvialiftsweights'\n",
    "\n",
    "data, response_json_dict, new_insights_response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-01', until='2023-03-05', filename=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12382</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>4913</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11649</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "      <td>5221</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13558</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "      <td>5155</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8538</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "      <td>4588</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12797</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "      <td>5669</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   impressions_value      impressions_end_time  reach_value  \\\n",
       "0              12382  2023-02-01T08:00:00+0000         4913   \n",
       "1              11649  2023-02-02T08:00:00+0000         5221   \n",
       "2              13558  2023-02-03T08:00:00+0000         5155   \n",
       "3               8538  2023-02-04T08:00:00+0000         4588   \n",
       "4              12797  2023-02-05T08:00:00+0000         5669   \n",
       "\n",
       "             reach_end_time  \n",
       "0  2023-02-01T08:00:00+0000  \n",
       "1  2023-02-02T08:00:00+0000  \n",
       "2  2023-02-03T08:00:00+0000  \n",
       "3  2023-02-04T08:00:00+0000  \n",
       "4  2023-02-05T08:00:00+0000  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>2023-01-01T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-02T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>169</td>\n",
       "      <td>2023-01-03T08:00:00+0000</td>\n",
       "      <td>132</td>\n",
       "      <td>2023-01-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141</td>\n",
       "      <td>2023-01-04T08:00:00+0000</td>\n",
       "      <td>103</td>\n",
       "      <td>2023-01-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>19138</td>\n",
       "      <td>2023-01-04T08:00:00+0000</td>\n",
       "      <td>7431</td>\n",
       "      <td>2023-01-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4917</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>1650</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "      <td>30</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>2023-03-03T08:00:00+0000</td>\n",
       "      <td>12</td>\n",
       "      <td>2023-03-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>2023-03-04T08:00:00+0000</td>\n",
       "      <td>8</td>\n",
       "      <td>2023-03-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-03-05T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-03-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    impressions_value      impressions_end_time  reach_value  \\\n",
       "0                  18  2023-01-01T08:00:00+0000            4   \n",
       "1                   3  2023-01-02T08:00:00+0000            3   \n",
       "2                 169  2023-01-03T08:00:00+0000          132   \n",
       "3                 141  2023-01-04T08:00:00+0000          103   \n",
       "29              19138  2023-01-04T08:00:00+0000         7431   \n",
       "..                ...                       ...          ...   \n",
       "28               4917  2023-03-01T08:00:00+0000         1650   \n",
       "1                  51  2023-03-02T08:00:00+0000           30   \n",
       "2                  12  2023-03-03T08:00:00+0000           12   \n",
       "3                  12  2023-03-04T08:00:00+0000            8   \n",
       "4                   3  2023-03-05T08:00:00+0000            3   \n",
       "\n",
       "              reach_end_time  \n",
       "0   2023-01-01T08:00:00+0000  \n",
       "1   2023-01-02T08:00:00+0000  \n",
       "2   2023-01-03T08:00:00+0000  \n",
       "3   2023-01-04T08:00:00+0000  \n",
       "29  2023-01-04T08:00:00+0000  \n",
       "..                       ...  \n",
       "28  2023-03-01T08:00:00+0000  \n",
       "1   2023-03-02T08:00:00+0000  \n",
       "2   2023-03-03T08:00:00+0000  \n",
       "3   2023-03-04T08:00:00+0000  \n",
       "4   2023-03-05T08:00:00+0000  \n",
       "\n",
       "[66 rows x 4 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values('impressions_end_time')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None, \n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since + timedelta(days=1)\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    # print('since parameter:', since_parameter)\n",
    "    # print('since:', since)\n",
    "    while (since_parameter > since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        df = df.sort_values[]\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = df_list \n",
    "    if filename:\n",
    "        try:\n",
    "            save_csv(df,filename,csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-14 22:02:13.391454\n",
      "Time completed: 2023-03-14 22:02:13.392455\n",
      "Fetching older account insights from 2023-01-01 to 2023-01-04\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1672819200.0&since=1672560000.0\n",
      "since parameter: 2023-01-02 00:00:00\n",
      "since: 2023-01-01 00:00:00\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-01 00:00:00\n",
      "Number of days of data: 4\n",
      "Fetching newer account insights from 2023-03-01 to 2023-03-05\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1678003200.0&since=1677657600.0\n",
      "since parameter: 2023-03-02 00:00:00\n",
      "since: 2023-03-01 00:00:00\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-03-01 00:00:00\n",
      "Number of days of data: 5\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_account_insights_df.sav\n",
      "Time completed: 2023-03-14 22:02:14.005860\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_account_insights.sav\n",
      "Time completed: 2023-03-14 22:02:14.009862\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    if filename:\n",
    "        filename = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename+'.sav', csv_path)\n",
    "        response_json_dict = loadpickle(filename+'.sav', json_path)\n",
    "        last_json_page = max(response_json_dict.keys())\n",
    "        previous_since_tz = response_json_dict[last_json_page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', previous_since_tz), \"%Y-%m-%d\")\n",
    "        previous_since = datetime.strptime(previous_since_tz, \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        \n",
    "        previous_until = response_json_dict[1]['data'][0]['values'][-1]['end_time']\n",
    "        previous_until = datetime.strptime(previous_until, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    except:\n",
    "        pass\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'Fetching older account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        older_insights_df, older_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=since, until=previous_since)\n",
    "        try:\n",
    "            df = pd.concat([df, older_insights_df])\n",
    "        except:\n",
    "            df = older_insights_df\n",
    "        older_insights_response_json_dict = dict( # Update the keys of older_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in older_insights_response_json_dict.keys()], older_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **older_insights_response_json_dict}\n",
    "    if (previous_until.date() < until.date()):\n",
    "        print(f'Fetching newer account insights from {datetime.strftime(previous_until, \"%Y-%m-%d\")} to {datetime.strftime(until, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=previous_until, until=until)\n",
    "        try:\n",
    "            df = pd.concat([df, new_insights_df])\n",
    "        except:\n",
    "            df = new_insights_df\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "        \n",
    "    if filename:\n",
    "        try:\n",
    "            savepickle(df, filename+'_df', 'sav', csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict, older_insights_response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename = 'silvialiftsweights'\n",
    "\n",
    "data, response_json_dict, new_insights_response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-01', until='2023-03-05', filename=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-14 22:03:17.946507\n",
      "Time completed: 2023-03-14 22:03:17.964566\n",
      "Fetching older account insights from 2023-01-01 to 2023-03-01\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1677657600.0&since=1675065600.0\n",
      "since parameter: 2023-01-30 00:00:00\n",
      "since: 2023-01-01 00:00:00\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-30 00:00:00\n",
      "Requesting page 2...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2022-12-31 00:00:00\n",
      "Number of days of data: 61\n",
      "Fetching newer account insights from 2023-03-01 to 2023-03-05\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1678003200.0&since=1677657600.0\n",
      "since parameter: 2023-03-02 00:00:00\n",
      "since: 2023-03-01 00:00:00\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-03-01 00:00:00\n",
      "Number of days of data: 5\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_account_insights_df.sav\n",
      "Time completed: 2023-03-14 22:03:18.912203\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_account_insights.sav\n",
      "Time completed: 2023-03-14 22:03:18.914156\n"
     ]
    }
   ],
   "source": [
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename = 'silvialiftsweights'\n",
    "\n",
    "data, response_json_dict, new_insights_response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-01', until='2023-03-05', filename=filename)\n",
    "\n",
    "## There should not have been any API calls here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>18</td>\n",
       "      <td>2022-12-31T08:00:00+0000</td>\n",
       "      <td>17</td>\n",
       "      <td>2022-12-31T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>18</td>\n",
       "      <td>2023-01-01T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-02T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>169</td>\n",
       "      <td>2023-01-03T08:00:00+0000</td>\n",
       "      <td>132</td>\n",
       "      <td>2023-01-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>19138</td>\n",
       "      <td>2023-01-04T08:00:00+0000</td>\n",
       "      <td>7431</td>\n",
       "      <td>2023-01-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "      <td>30</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>2023-03-03T08:00:00+0000</td>\n",
       "      <td>12</td>\n",
       "      <td>2023-03-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>2023-03-04T08:00:00+0000</td>\n",
       "      <td>8</td>\n",
       "      <td>2023-03-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-03-05T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-03-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    impressions_value      impressions_end_time  reach_value  \\\n",
       "31                 18  2022-12-31T08:00:00+0000           17   \n",
       "32                 18  2023-01-01T08:00:00+0000            4   \n",
       "33                  3  2023-01-02T08:00:00+0000            3   \n",
       "34                169  2023-01-03T08:00:00+0000          132   \n",
       "29              19138  2023-01-04T08:00:00+0000         7431   \n",
       "..                ...                       ...          ...   \n",
       "0                 289  2023-03-01T08:00:00+0000          170   \n",
       "1                  51  2023-03-02T08:00:00+0000           30   \n",
       "2                  12  2023-03-03T08:00:00+0000           12   \n",
       "3                  12  2023-03-04T08:00:00+0000            8   \n",
       "4                   3  2023-03-05T08:00:00+0000            3   \n",
       "\n",
       "              reach_end_time  \n",
       "31  2022-12-31T08:00:00+0000  \n",
       "32  2023-01-01T08:00:00+0000  \n",
       "33  2023-01-02T08:00:00+0000  \n",
       "34  2023-01-03T08:00:00+0000  \n",
       "29  2023-01-04T08:00:00+0000  \n",
       "..                       ...  \n",
       "0   2023-03-01T08:00:00+0000  \n",
       "1   2023-03-02T08:00:00+0000  \n",
       "2   2023-03-03T08:00:00+0000  \n",
       "3   2023-03-04T08:00:00+0000  \n",
       "4   2023-03-05T08:00:00+0000  \n",
       "\n",
       "[123 rows x 4 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values('impressions_end_time')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None, \n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since + timedelta(days=1)\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    # print('since parameter:', since_parameter)\n",
    "    # print('since:', since)\n",
    "    while (since_parameter > since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = df_list \n",
    "    if filename:\n",
    "        filename += '_account_insights'\n",
    "        try:\n",
    "            savepickle(response_json_dict,filename+'_df','sav',csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1677657600.0&since=1675238400.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-02-01 00:00:00\n",
      "Number of days of data: 29\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-14_account_insights_df.sav\n",
      "Time completed: 2023-03-14 22:42:30.074136\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-14_account_insights.sav\n",
      "Time completed: 2023-03-14 22:42:30.075091\n"
     ]
    }
   ],
   "source": [
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename2 = 'silvialiftsweights_03-14'\n",
    "data, response_json_dict = get_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-14 22:46:15.220698\n",
      "Time completed: 2023-03-14 22:46:15.221700\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-14_account_insights_df.sav\n",
      "Time completed: 2023-03-14 22:46:15.222703\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-14_account_insights.sav\n",
      "Time completed: 2023-03-14 22:46:15.223701\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    if filename:\n",
    "        filename = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename+'_df.sav', csv_path)\n",
    "        response_json_dict = loadpickle(filename+'.sav', json_path)\n",
    "        last_json_page = max(response_json_dict.keys())\n",
    "        previous_since_tz = response_json_dict[last_json_page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', previous_since_tz), \"%Y-%m-%d\")\n",
    "        previous_since = datetime.strptime(previous_since_tz, \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        \n",
    "        previous_until = response_json_dict[1]['data'][0]['values'][-1]['end_time']\n",
    "        previous_until = datetime.strptime(previous_until, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    except:\n",
    "        pass\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'Fetching older account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        older_insights_df, older_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=since, until=previous_since)\n",
    "        try:\n",
    "            df = pd.concat([df, older_insights_df])\n",
    "        except:\n",
    "            df = older_insights_df\n",
    "        # Update the keys of *response_json_dict* before merging with older_insights_response_json_dict. That way, final \n",
    "            # response dictionary always has insights from oldest dates first\n",
    "        response_json_dict = dict( \n",
    "            zip([key+len(older_insights_response_json_dict) for key in response_json_dict.keys()], response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**older_insights_response_json_dict, **response_json_dict}\n",
    "    if (previous_until.date() < until.date()):\n",
    "        print(f'Fetching newer account insights from {datetime.strftime(previous_until, \"%Y-%m-%d\")} to {datetime.strftime(until, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=previous_until, until=until)\n",
    "        try:\n",
    "            df = pd.concat([df, new_insights_df])\n",
    "        except:\n",
    "            df = new_insights_df\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "        \n",
    "    if filename:\n",
    "        try:\n",
    "            savepickle(df, filename+'_df', 'sav', csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename2 = 'silvialiftsweights_03-14'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=since, filename=filename2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-14 22:47:23.185139\n",
      "Time completed: 2023-03-14 22:47:23.187849\n",
      "Fetching older account insights from 2023-01-01 to 2023-02-01\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1675238400.0&since=1672646400.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-02 00:00:00\n",
      "Requesting page 2...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2022-12-03 00:00:00\n",
      "Number of days of data: 61\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-14_account_insights_df.sav\n",
      "Time completed: 2023-03-14 22:47:23.908521\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-14_account_insights.sav\n",
      "Time completed: 2023-03-14 22:47:23.909485\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-01', until=since, filename=filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-03T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6</td>\n",
       "      <td>2022-12-04T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-12-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-12-05T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-12-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-06T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-06T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-12-07T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-12-07T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>196</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "      <td>141</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>45</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "      <td>43</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    impressions_value      impressions_end_time  reach_value  \\\n",
       "31                  0  2022-12-03T08:00:00+0000            0   \n",
       "32                  6  2022-12-04T08:00:00+0000            2   \n",
       "33                  3  2022-12-05T08:00:00+0000            3   \n",
       "34                  0  2022-12-06T08:00:00+0000            0   \n",
       "35                  1  2022-12-07T08:00:00+0000            1   \n",
       "..                ...                       ...          ...   \n",
       "26                 29  2023-01-28T08:00:00+0000            1   \n",
       "27                  5  2023-01-29T08:00:00+0000            4   \n",
       "28                196  2023-01-30T08:00:00+0000          141   \n",
       "29                 45  2023-01-31T08:00:00+0000           43   \n",
       "30                 14  2023-02-01T08:00:00+0000            9   \n",
       "\n",
       "              reach_end_time  \n",
       "31  2022-12-03T08:00:00+0000  \n",
       "32  2022-12-04T08:00:00+0000  \n",
       "33  2022-12-05T08:00:00+0000  \n",
       "34  2022-12-06T08:00:00+0000  \n",
       "35  2022-12-07T08:00:00+0000  \n",
       "..                       ...  \n",
       "26  2023-01-28T08:00:00+0000  \n",
       "27  2023-01-29T08:00:00+0000  \n",
       "28  2023-01-30T08:00:00+0000  \n",
       "29  2023-01-31T08:00:00+0000  \n",
       "30  2023-02-01T08:00:00+0000  \n",
       "\n",
       "[61 rows x 4 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values('impressions_end_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-14 22:49:59.067882\n",
      "Time completed: 2023-03-14 22:49:59.079885\n",
      "Fetching older account insights from 2023-01-01 to 2023-02-01\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1675238400.0&since=1672646400.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-02 00:00:00\n",
      "Requesting page 2...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2022-12-03 00:00:00\n",
      "Number of days of data: 61\n",
      "Fetching newer account insights from 2023-02-01 to 2023-03-01\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1677657600.0&since=1675238400.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-02-01 00:00:00\n",
      "Number of days of data: 29\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-14_account_insights_df.sav\n",
      "Time completed: 2023-03-14 22:50:00.442835\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-14_account_insights.sav\n",
      "Time completed: 2023-03-14 22:50:00.444838\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-01', until=until, filename=filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-03T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-03T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6</td>\n",
       "      <td>2022-12-04T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-12-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6</td>\n",
       "      <td>2022-12-04T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-12-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-12-05T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-12-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "      <td>95</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "      <td>28</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    impressions_value      impressions_end_time  reach_value  \\\n",
       "31                  0  2022-12-03T08:00:00+0000            0   \n",
       "31                  0  2022-12-03T08:00:00+0000            0   \n",
       "32                  6  2022-12-04T08:00:00+0000            2   \n",
       "32                  6  2022-12-04T08:00:00+0000            2   \n",
       "33                  3  2022-12-05T08:00:00+0000            3   \n",
       "..                ...                       ...          ...   \n",
       "24                  1  2023-02-25T08:00:00+0000            1   \n",
       "25                 32  2023-02-26T08:00:00+0000            9   \n",
       "26                109  2023-02-27T08:00:00+0000           95   \n",
       "27                 29  2023-02-28T08:00:00+0000           28   \n",
       "28                289  2023-03-01T08:00:00+0000          170   \n",
       "\n",
       "              reach_end_time  \n",
       "31  2022-12-03T08:00:00+0000  \n",
       "31  2022-12-03T08:00:00+0000  \n",
       "32  2022-12-04T08:00:00+0000  \n",
       "32  2022-12-04T08:00:00+0000  \n",
       "33  2022-12-05T08:00:00+0000  \n",
       "..                       ...  \n",
       "24  2023-02-25T08:00:00+0000  \n",
       "25  2023-02-26T08:00:00+0000  \n",
       "26  2023-02-27T08:00:00+0000  \n",
       "27  2023-02-28T08:00:00+0000  \n",
       "28  2023-03-01T08:00:00+0000  \n",
       "\n",
       "[151 rows x 4 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values('impressions_end_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2023-01-02T08:00:00+0000\n",
       "1     2023-01-03T08:00:00+0000\n",
       "2     2023-01-04T08:00:00+0000\n",
       "3     2023-01-05T08:00:00+0000\n",
       "4     2023-01-06T08:00:00+0000\n",
       "                ...           \n",
       "24    2023-02-25T08:00:00+0000\n",
       "25    2023-02-26T08:00:00+0000\n",
       "26    2023-02-27T08:00:00+0000\n",
       "27    2023-02-28T08:00:00+0000\n",
       "28    2023-03-01T08:00:00+0000\n",
       "Name: impressions_end_time, Length: 151, dtype: object"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp_column_suffix = 'end_time'\n",
    "timestamp_column = data.columns[data.columns.str.contains('_'+timestamp_column_suffix)][0]\n",
    "data[timestamp_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-14 23:39:34.172171\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-03T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-03T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>2022-12-04T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-12-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2022-12-04T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-12-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-12-05T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-12-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "      <td>95</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "      <td>28</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "      <td>28</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     impressions_value      impressions_end_time  reach_value  \\\n",
       "0                    0  2022-12-03T08:00:00+0000            0   \n",
       "1                    0  2022-12-03T08:00:00+0000            0   \n",
       "2                    6  2022-12-04T08:00:00+0000            2   \n",
       "3                    6  2022-12-04T08:00:00+0000            2   \n",
       "4                    3  2022-12-05T08:00:00+0000            3   \n",
       "..                 ...                       ...          ...   \n",
       "177                109  2023-02-27T08:00:00+0000           95   \n",
       "178                 29  2023-02-28T08:00:00+0000           28   \n",
       "179                 29  2023-02-28T08:00:00+0000           28   \n",
       "180                289  2023-03-01T08:00:00+0000          170   \n",
       "181                289  2023-03-01T08:00:00+0000          170   \n",
       "\n",
       "               reach_end_time  \n",
       "0    2022-12-03T08:00:00+0000  \n",
       "1    2022-12-03T08:00:00+0000  \n",
       "2    2022-12-04T08:00:00+0000  \n",
       "3    2022-12-04T08:00:00+0000  \n",
       "4    2022-12-05T08:00:00+0000  \n",
       "..                        ...  \n",
       "177  2023-02-27T08:00:00+0000  \n",
       "178  2023-02-28T08:00:00+0000  \n",
       "179  2023-02-28T08:00:00+0000  \n",
       "180  2023-03-01T08:00:00+0000  \n",
       "181  2023-03-01T08:00:00+0000  \n",
       "\n",
       "[182 rows x 4 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadpickle(\n",
    "    'silvialiftsweights_03-14_account_insights_df.sav', r'C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim'\n",
    "    ).sort_values('impressions_end_time').reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None, \n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since + timedelta(days=1)\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    # print('since parameter:', since_parameter)\n",
    "    # print('since:', since)\n",
    "    while (since_parameter > since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = df_list \n",
    "    if filename:\n",
    "        filename += '_account_insights'\n",
    "        try:\n",
    "            savepickle(response_json_dict,filename+'_df','sav',csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_column_suffix = 'end_time'\n",
    "timestamp_column = data.columns[data.columns.str.contains('_'+timestamp_column_suffix)][0]\n",
    "data[timestamp_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-14 23:15:06.661853\n",
      "Time completed: 2023-03-14 23:15:06.667170\n",
      "2023-01-02T08:00:00+0000\n",
      "\n",
      "Fetching older account insights from 2023-01-01 to 2023-01-02\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1672646400.0&since=1672560000.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-01 00:00:00\n",
      "Number of days of data: 2\n",
      "\n",
      "Fetching newer account insights from 2023-02-01 to 2023-03-01\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1677657600.0&since=1675238400.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-02-01 00:00:00\n",
      "Number of days of data: 29\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-14_account_insights_df.sav\n",
      "Time completed: 2023-03-14 23:15:07.576549\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-14_account_insights.sav\n",
      "Time completed: 2023-03-14 23:15:07.577509\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    timestamp_column_suffix='end_time', filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    if filename:\n",
    "        filename = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename+'_df.sav', csv_path).reset_index(drop=True)\n",
    "        timestamp_column = df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]\n",
    "        df = df.sort_values(timestamp_column)\n",
    "        response_json_dict = loadpickle(filename+'.sav', json_path)\n",
    "        last_json_page = max(response_json_dict.keys())\n",
    "        # previous_since_tz = response_json_dict[last_json_page]['data'][0]['values'][0]['end_time']\n",
    "        # previous_since = datetime.strptime(previous_since_tz, \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        previous_until = response_json_dict[1]['data'][0]['values'][-1]['end_time']\n",
    "        previous_until = datetime.strptime(previous_until, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        previous_since = datetime.strptime(df.loc[0, timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "    except:\n",
    "        pass\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    print(df.loc[0, timestamp_column])\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'\\nFetching older account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        older_insights_df, older_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=since, until=previous_since)\n",
    "        try:\n",
    "            df = pd.concat([df, older_insights_df])\n",
    "        except:\n",
    "            df = older_insights_df\n",
    "        # Update the keys of *response_json_dict* before merging with older_insights_response_json_dict. That way, final \n",
    "            # response dictionary always has insights from oldest dates first\n",
    "        response_json_dict = dict( \n",
    "            zip([key+len(older_insights_response_json_dict) for key in response_json_dict.keys()], response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**older_insights_response_json_dict, **response_json_dict}\n",
    "    if (previous_until.date() < until.date()):\n",
    "        print(f'\\nFetching newer account insights from {datetime.strftime(previous_until, \"%Y-%m-%d\")} to {datetime.strftime(until, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=previous_until, until=until)\n",
    "        try:\n",
    "            df = pd.concat([df, new_insights_df])\n",
    "        except:\n",
    "            df = new_insights_df\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "        \n",
    "    if filename:\n",
    "        try:\n",
    "            savepickle(df, filename+'_df', 'sav', csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename2 = 'silvialiftsweights_03-14'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-01', until=until, filename=filename2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-03T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-03T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>6</td>\n",
       "      <td>2022-12-04T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-12-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6</td>\n",
       "      <td>2022-12-04T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-12-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-12-05T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-12-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "      <td>95</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "      <td>28</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "      <td>28</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     impressions_value      impressions_end_time  reach_value  \\\n",
       "92                   0  2022-12-03T08:00:00+0000            0   \n",
       "31                   0  2022-12-03T08:00:00+0000            0   \n",
       "93                   6  2022-12-04T08:00:00+0000            2   \n",
       "32                   6  2022-12-04T08:00:00+0000            2   \n",
       "33                   3  2022-12-05T08:00:00+0000            3   \n",
       "..                 ...                       ...          ...   \n",
       "26                 109  2023-02-27T08:00:00+0000           95   \n",
       "27                  29  2023-02-28T08:00:00+0000           28   \n",
       "149                 29  2023-02-28T08:00:00+0000           28   \n",
       "150                289  2023-03-01T08:00:00+0000          170   \n",
       "28                 289  2023-03-01T08:00:00+0000          170   \n",
       "\n",
       "               reach_end_time  \n",
       "92   2022-12-03T08:00:00+0000  \n",
       "31   2022-12-03T08:00:00+0000  \n",
       "93   2022-12-04T08:00:00+0000  \n",
       "32   2022-12-04T08:00:00+0000  \n",
       "33   2022-12-05T08:00:00+0000  \n",
       "..                        ...  \n",
       "26   2023-02-27T08:00:00+0000  \n",
       "27   2023-02-28T08:00:00+0000  \n",
       "149  2023-02-28T08:00:00+0000  \n",
       "150  2023-03-01T08:00:00+0000  \n",
       "28   2023-03-01T08:00:00+0000  \n",
       "\n",
       "[182 rows x 4 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values('impressions_end_time')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None, \n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since + timedelta(days=1)\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter > since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = df_list \n",
    "    if filename:\n",
    "        filename += '_account_insights'\n",
    "        try:\n",
    "            savepickle(df,filename+'_df','sav',csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1677657600.0&since=1675238400.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-02-01 00:00:00\n",
      "Number of days of data: 29\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-14_2_account_insights_df.sav\n",
      "Time completed: 2023-03-14 23:40:44.290538\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-14_2_account_insights.sav\n",
      "Time completed: 2023-03-14 23:40:44.291538\n"
     ]
    }
   ],
   "source": [
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename2 = 'silvialiftsweights_03-14_2'\n",
    "data, response_json_dict = get_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>169</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "      <td>51</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>78</td>\n",
       "      <td>2023-02-06T08:00:00+0000</td>\n",
       "      <td>67</td>\n",
       "      <td>2023-02-06T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>266</td>\n",
       "      <td>2023-02-07T08:00:00+0000</td>\n",
       "      <td>208</td>\n",
       "      <td>2023-02-07T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>77</td>\n",
       "      <td>2023-02-08T08:00:00+0000</td>\n",
       "      <td>52</td>\n",
       "      <td>2023-02-08T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-09T08:00:00+0000</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-09T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-10T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-10T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-11T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-11T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>184</td>\n",
       "      <td>2023-02-12T08:00:00+0000</td>\n",
       "      <td>98</td>\n",
       "      <td>2023-02-12T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>48</td>\n",
       "      <td>2023-02-13T08:00:00+0000</td>\n",
       "      <td>39</td>\n",
       "      <td>2023-02-13T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>200</td>\n",
       "      <td>2023-02-14T08:00:00+0000</td>\n",
       "      <td>131</td>\n",
       "      <td>2023-02-14T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>40</td>\n",
       "      <td>2023-02-15T08:00:00+0000</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-15T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>2023-02-16T08:00:00+0000</td>\n",
       "      <td>10</td>\n",
       "      <td>2023-02-16T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-17T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-17T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-18T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-18T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13</td>\n",
       "      <td>2023-02-19T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-19T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-20T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-20T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>111</td>\n",
       "      <td>2023-02-21T08:00:00+0000</td>\n",
       "      <td>96</td>\n",
       "      <td>2023-02-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19</td>\n",
       "      <td>2023-02-22T08:00:00+0000</td>\n",
       "      <td>16</td>\n",
       "      <td>2023-02-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>31</td>\n",
       "      <td>2023-02-23T08:00:00+0000</td>\n",
       "      <td>7</td>\n",
       "      <td>2023-02-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "      <td>95</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "      <td>28</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    impressions_value      impressions_end_time  reach_value  \\\n",
       "0                  14  2023-02-01T08:00:00+0000            9   \n",
       "1                   5  2023-02-02T08:00:00+0000            4   \n",
       "2                 169  2023-02-03T08:00:00+0000          109   \n",
       "3                  68  2023-02-04T08:00:00+0000           51   \n",
       "4                  57  2023-02-05T08:00:00+0000           29   \n",
       "5                  78  2023-02-06T08:00:00+0000           67   \n",
       "6                 266  2023-02-07T08:00:00+0000          208   \n",
       "7                  77  2023-02-08T08:00:00+0000           52   \n",
       "8                  21  2023-02-09T08:00:00+0000           21   \n",
       "9                   3  2023-02-10T08:00:00+0000            3   \n",
       "10                  1  2023-02-11T08:00:00+0000            1   \n",
       "11                184  2023-02-12T08:00:00+0000           98   \n",
       "12                 48  2023-02-13T08:00:00+0000           39   \n",
       "13                200  2023-02-14T08:00:00+0000          131   \n",
       "14                 40  2023-02-15T08:00:00+0000           21   \n",
       "15                 10  2023-02-16T08:00:00+0000           10   \n",
       "16                  2  2023-02-17T08:00:00+0000            2   \n",
       "17                  2  2023-02-18T08:00:00+0000            2   \n",
       "18                 13  2023-02-19T08:00:00+0000            1   \n",
       "19                  4  2023-02-20T08:00:00+0000            2   \n",
       "20                111  2023-02-21T08:00:00+0000           96   \n",
       "21                 19  2023-02-22T08:00:00+0000           16   \n",
       "22                 31  2023-02-23T08:00:00+0000            7   \n",
       "23                  3  2023-02-24T08:00:00+0000            3   \n",
       "24                  1  2023-02-25T08:00:00+0000            1   \n",
       "25                 32  2023-02-26T08:00:00+0000            9   \n",
       "26                109  2023-02-27T08:00:00+0000           95   \n",
       "27                 29  2023-02-28T08:00:00+0000           28   \n",
       "28                289  2023-03-01T08:00:00+0000          170   \n",
       "\n",
       "              reach_end_time  \n",
       "0   2023-02-01T08:00:00+0000  \n",
       "1   2023-02-02T08:00:00+0000  \n",
       "2   2023-02-03T08:00:00+0000  \n",
       "3   2023-02-04T08:00:00+0000  \n",
       "4   2023-02-05T08:00:00+0000  \n",
       "5   2023-02-06T08:00:00+0000  \n",
       "6   2023-02-07T08:00:00+0000  \n",
       "7   2023-02-08T08:00:00+0000  \n",
       "8   2023-02-09T08:00:00+0000  \n",
       "9   2023-02-10T08:00:00+0000  \n",
       "10  2023-02-11T08:00:00+0000  \n",
       "11  2023-02-12T08:00:00+0000  \n",
       "12  2023-02-13T08:00:00+0000  \n",
       "13  2023-02-14T08:00:00+0000  \n",
       "14  2023-02-15T08:00:00+0000  \n",
       "15  2023-02-16T08:00:00+0000  \n",
       "16  2023-02-17T08:00:00+0000  \n",
       "17  2023-02-18T08:00:00+0000  \n",
       "18  2023-02-19T08:00:00+0000  \n",
       "19  2023-02-20T08:00:00+0000  \n",
       "20  2023-02-21T08:00:00+0000  \n",
       "21  2023-02-22T08:00:00+0000  \n",
       "22  2023-02-23T08:00:00+0000  \n",
       "23  2023-02-24T08:00:00+0000  \n",
       "24  2023-02-25T08:00:00+0000  \n",
       "25  2023-02-26T08:00:00+0000  \n",
       "26  2023-02-27T08:00:00+0000  \n",
       "27  2023-02-28T08:00:00+0000  \n",
       "28  2023-03-01T08:00:00+0000  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values('impressions_end_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 00:05:54.880704\n",
      "Time completed: 2023-03-15 00:05:54.882681\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-14_2_account_insights_df.sav\n",
      "Time completed: 2023-03-15 00:05:54.884684\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-14_2_account_insights.sav\n",
      "Time completed: 2023-03-15 00:05:54.885687\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    timestamp_column_suffix='end_time', filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    previous_since, previous_until = None, None\n",
    "    if filename:\n",
    "        filename = f'{filename}_account_insights'\n",
    "        # df = loadpickle(filename+'_df.sav', csv_path)\n",
    "        # df = df.reset_index(drop=True)\n",
    "        # timestamp_column = df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]\n",
    "        # df = df.sort_values(timestamp_column)\n",
    "        # response_json_dict = loadpickle(filename+'.sav', json_path)\n",
    "        # last_json_page = max(response_json_dict.keys())\n",
    "        # previous_since = datetime.strptime(df.loc[0, timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        # previous_until = datetime.strptime(df.iloc[-1][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    try:\n",
    "        df = loadpickle(filename+'_df.sav', csv_path)\n",
    "        df = df.reset_index(drop=True)\n",
    "        timestamp_column = df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]\n",
    "        df = df.sort_values(timestamp_column)\n",
    "        response_json_dict = loadpickle(filename+'.sav', json_path)\n",
    "        last_json_page = max(response_json_dict.keys())\n",
    "        previous_since = datetime.strptime(df.iloc[0][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        previous_until = datetime.strptime(df.iloc[-1][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    except:\n",
    "        pass\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    \n",
    "    if previous_since == None:\n",
    "        previous_since = since + timedelta (days=1)\n",
    "        print('Previous `since` parameter could not be found; default to since + 1.')\n",
    "    if previous_until == None:\n",
    "        previous_until = until - timedelta (days=1)\n",
    "        print('Previous `until` parameter could not be found; default to until - 1.')\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'\\nFetching older account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        older_insights_df, older_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=since, until=previous_since)\n",
    "        try:\n",
    "            df = pd.concat([df, older_insights_df])\n",
    "        except:\n",
    "            df = older_insights_df\n",
    "        # Update the keys of *response_json_dict* before merging with older_insights_response_json_dict. That way, final \n",
    "            # response dictionary always has insights from oldest dates first\n",
    "        response_json_dict = dict( \n",
    "            zip([key+len(older_insights_response_json_dict) for key in response_json_dict.keys()], response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**older_insights_response_json_dict, **response_json_dict}\n",
    "    if (previous_until.date() < until.date()):\n",
    "        print(f'\\nFetching newer account insights from {datetime.strftime(previous_until, \"%Y-%m-%d\")} to {datetime.strftime(until, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=previous_until, until=until)\n",
    "        try:\n",
    "            df = pd.concat([df, new_insights_df])\n",
    "        except:\n",
    "            df = new_insights_df\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "        \n",
    "    if filename:\n",
    "        try:\n",
    "            savepickle(df, filename+'_df', 'sav', csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename2 = 'silvialiftsweights_03-14_2'\n",
    "\n",
    "data2, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 00:06:07.343059\n",
      "Time completed: 2023-03-15 00:06:07.351097\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-14_2_account_insights_df.sav\n",
      "Time completed: 2023-03-15 00:06:07.353149\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-14_2_account_insights.sav\n",
      "Time completed: 2023-03-15 00:06:07.354162\n"
     ]
    }
   ],
   "source": [
    "data2, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until=until, filename=filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 00:07:10.243979\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "      <td>15</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "      <td>15</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>38</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>38</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>196</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "      <td>141</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>196</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "      <td>141</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>45</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "      <td>43</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>45</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "      <td>43</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>169</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>68</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "      <td>51</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>57</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>78</td>\n",
       "      <td>2023-02-06T08:00:00+0000</td>\n",
       "      <td>67</td>\n",
       "      <td>2023-02-06T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>266</td>\n",
       "      <td>2023-02-07T08:00:00+0000</td>\n",
       "      <td>208</td>\n",
       "      <td>2023-02-07T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>77</td>\n",
       "      <td>2023-02-08T08:00:00+0000</td>\n",
       "      <td>52</td>\n",
       "      <td>2023-02-08T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-09T08:00:00+0000</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-09T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-10T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-10T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-11T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-11T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>184</td>\n",
       "      <td>2023-02-12T08:00:00+0000</td>\n",
       "      <td>98</td>\n",
       "      <td>2023-02-12T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>48</td>\n",
       "      <td>2023-02-13T08:00:00+0000</td>\n",
       "      <td>39</td>\n",
       "      <td>2023-02-13T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>200</td>\n",
       "      <td>2023-02-14T08:00:00+0000</td>\n",
       "      <td>131</td>\n",
       "      <td>2023-02-14T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>2023-02-15T08:00:00+0000</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-15T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10</td>\n",
       "      <td>2023-02-16T08:00:00+0000</td>\n",
       "      <td>10</td>\n",
       "      <td>2023-02-16T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-17T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-17T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-18T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-18T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>13</td>\n",
       "      <td>2023-02-19T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-19T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-20T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-20T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>111</td>\n",
       "      <td>2023-02-21T08:00:00+0000</td>\n",
       "      <td>96</td>\n",
       "      <td>2023-02-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>19</td>\n",
       "      <td>2023-02-22T08:00:00+0000</td>\n",
       "      <td>16</td>\n",
       "      <td>2023-02-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>31</td>\n",
       "      <td>2023-02-23T08:00:00+0000</td>\n",
       "      <td>7</td>\n",
       "      <td>2023-02-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>32</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "      <td>95</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "      <td>28</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>51</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "      <td>30</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    impressions_value      impressions_end_time  reach_value  \\\n",
       "0                  23  2023-01-21T08:00:00+0000           15   \n",
       "1                  23  2023-01-21T08:00:00+0000           15   \n",
       "2                   5  2023-01-22T08:00:00+0000            5   \n",
       "3                   5  2023-01-22T08:00:00+0000            5   \n",
       "4                   3  2023-01-23T08:00:00+0000            3   \n",
       "5                   3  2023-01-23T08:00:00+0000            3   \n",
       "6                   3  2023-01-24T08:00:00+0000            3   \n",
       "7                   3  2023-01-24T08:00:00+0000            3   \n",
       "8                   0  2023-01-25T08:00:00+0000            0   \n",
       "9                   0  2023-01-25T08:00:00+0000            0   \n",
       "10                  2  2023-01-26T08:00:00+0000            2   \n",
       "11                  2  2023-01-26T08:00:00+0000            2   \n",
       "13                 38  2023-01-27T08:00:00+0000            1   \n",
       "12                 38  2023-01-27T08:00:00+0000            1   \n",
       "14                 29  2023-01-28T08:00:00+0000            1   \n",
       "15                 29  2023-01-28T08:00:00+0000            1   \n",
       "16                  5  2023-01-29T08:00:00+0000            4   \n",
       "17                  5  2023-01-29T08:00:00+0000            4   \n",
       "18                196  2023-01-30T08:00:00+0000          141   \n",
       "19                196  2023-01-30T08:00:00+0000          141   \n",
       "20                 45  2023-01-31T08:00:00+0000           43   \n",
       "21                 45  2023-01-31T08:00:00+0000           43   \n",
       "22                 14  2023-02-01T08:00:00+0000            9   \n",
       "23                 14  2023-02-01T08:00:00+0000            9   \n",
       "24                 14  2023-02-01T08:00:00+0000            9   \n",
       "25                  5  2023-02-02T08:00:00+0000            4   \n",
       "26                169  2023-02-03T08:00:00+0000          109   \n",
       "27                 68  2023-02-04T08:00:00+0000           51   \n",
       "28                 57  2023-02-05T08:00:00+0000           29   \n",
       "29                 78  2023-02-06T08:00:00+0000           67   \n",
       "30                266  2023-02-07T08:00:00+0000          208   \n",
       "31                 77  2023-02-08T08:00:00+0000           52   \n",
       "32                 21  2023-02-09T08:00:00+0000           21   \n",
       "33                  3  2023-02-10T08:00:00+0000            3   \n",
       "34                  1  2023-02-11T08:00:00+0000            1   \n",
       "35                184  2023-02-12T08:00:00+0000           98   \n",
       "36                 48  2023-02-13T08:00:00+0000           39   \n",
       "37                200  2023-02-14T08:00:00+0000          131   \n",
       "38                 40  2023-02-15T08:00:00+0000           21   \n",
       "39                 10  2023-02-16T08:00:00+0000           10   \n",
       "40                  2  2023-02-17T08:00:00+0000            2   \n",
       "41                  2  2023-02-18T08:00:00+0000            2   \n",
       "42                 13  2023-02-19T08:00:00+0000            1   \n",
       "43                  4  2023-02-20T08:00:00+0000            2   \n",
       "44                111  2023-02-21T08:00:00+0000           96   \n",
       "45                 19  2023-02-22T08:00:00+0000           16   \n",
       "46                 31  2023-02-23T08:00:00+0000            7   \n",
       "47                  3  2023-02-24T08:00:00+0000            3   \n",
       "48                  1  2023-02-25T08:00:00+0000            1   \n",
       "49                 32  2023-02-26T08:00:00+0000            9   \n",
       "50                109  2023-02-27T08:00:00+0000           95   \n",
       "51                 29  2023-02-28T08:00:00+0000           28   \n",
       "52                289  2023-03-01T08:00:00+0000          170   \n",
       "53                289  2023-03-01T08:00:00+0000          170   \n",
       "54                 51  2023-03-02T08:00:00+0000           30   \n",
       "\n",
       "              reach_end_time  \n",
       "0   2023-01-21T08:00:00+0000  \n",
       "1   2023-01-21T08:00:00+0000  \n",
       "2   2023-01-22T08:00:00+0000  \n",
       "3   2023-01-22T08:00:00+0000  \n",
       "4   2023-01-23T08:00:00+0000  \n",
       "5   2023-01-23T08:00:00+0000  \n",
       "6   2023-01-24T08:00:00+0000  \n",
       "7   2023-01-24T08:00:00+0000  \n",
       "8   2023-01-25T08:00:00+0000  \n",
       "9   2023-01-25T08:00:00+0000  \n",
       "10  2023-01-26T08:00:00+0000  \n",
       "11  2023-01-26T08:00:00+0000  \n",
       "13  2023-01-27T08:00:00+0000  \n",
       "12  2023-01-27T08:00:00+0000  \n",
       "14  2023-01-28T08:00:00+0000  \n",
       "15  2023-01-28T08:00:00+0000  \n",
       "16  2023-01-29T08:00:00+0000  \n",
       "17  2023-01-29T08:00:00+0000  \n",
       "18  2023-01-30T08:00:00+0000  \n",
       "19  2023-01-30T08:00:00+0000  \n",
       "20  2023-01-31T08:00:00+0000  \n",
       "21  2023-01-31T08:00:00+0000  \n",
       "22  2023-02-01T08:00:00+0000  \n",
       "23  2023-02-01T08:00:00+0000  \n",
       "24  2023-02-01T08:00:00+0000  \n",
       "25  2023-02-02T08:00:00+0000  \n",
       "26  2023-02-03T08:00:00+0000  \n",
       "27  2023-02-04T08:00:00+0000  \n",
       "28  2023-02-05T08:00:00+0000  \n",
       "29  2023-02-06T08:00:00+0000  \n",
       "30  2023-02-07T08:00:00+0000  \n",
       "31  2023-02-08T08:00:00+0000  \n",
       "32  2023-02-09T08:00:00+0000  \n",
       "33  2023-02-10T08:00:00+0000  \n",
       "34  2023-02-11T08:00:00+0000  \n",
       "35  2023-02-12T08:00:00+0000  \n",
       "36  2023-02-13T08:00:00+0000  \n",
       "37  2023-02-14T08:00:00+0000  \n",
       "38  2023-02-15T08:00:00+0000  \n",
       "39  2023-02-16T08:00:00+0000  \n",
       "40  2023-02-17T08:00:00+0000  \n",
       "41  2023-02-18T08:00:00+0000  \n",
       "42  2023-02-19T08:00:00+0000  \n",
       "43  2023-02-20T08:00:00+0000  \n",
       "44  2023-02-21T08:00:00+0000  \n",
       "45  2023-02-22T08:00:00+0000  \n",
       "46  2023-02-23T08:00:00+0000  \n",
       "47  2023-02-24T08:00:00+0000  \n",
       "48  2023-02-25T08:00:00+0000  \n",
       "49  2023-02-26T08:00:00+0000  \n",
       "50  2023-02-27T08:00:00+0000  \n",
       "51  2023-02-28T08:00:00+0000  \n",
       "52  2023-03-01T08:00:00+0000  \n",
       "53  2023-03-01T08:00:00+0000  \n",
       "54  2023-03-02T08:00:00+0000  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadpickle(\n",
    "    'silvialiftsweights_03-14_2_account_insights_df.sav', \n",
    "    r'C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim'\n",
    "    ).reset_index(drop=True).sort_values('impressions_end_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "      <td>15</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "      <td>15</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>38</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>38</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>196</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "      <td>141</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>196</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "      <td>141</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>45</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "      <td>43</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>45</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "      <td>43</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>169</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>68</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "      <td>51</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>57</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>78</td>\n",
       "      <td>2023-02-06T08:00:00+0000</td>\n",
       "      <td>67</td>\n",
       "      <td>2023-02-06T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>266</td>\n",
       "      <td>2023-02-07T08:00:00+0000</td>\n",
       "      <td>208</td>\n",
       "      <td>2023-02-07T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>77</td>\n",
       "      <td>2023-02-08T08:00:00+0000</td>\n",
       "      <td>52</td>\n",
       "      <td>2023-02-08T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-09T08:00:00+0000</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-09T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-10T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-10T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-11T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-11T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>184</td>\n",
       "      <td>2023-02-12T08:00:00+0000</td>\n",
       "      <td>98</td>\n",
       "      <td>2023-02-12T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>48</td>\n",
       "      <td>2023-02-13T08:00:00+0000</td>\n",
       "      <td>39</td>\n",
       "      <td>2023-02-13T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>200</td>\n",
       "      <td>2023-02-14T08:00:00+0000</td>\n",
       "      <td>131</td>\n",
       "      <td>2023-02-14T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>2023-02-15T08:00:00+0000</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-15T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10</td>\n",
       "      <td>2023-02-16T08:00:00+0000</td>\n",
       "      <td>10</td>\n",
       "      <td>2023-02-16T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-17T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-17T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-18T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-18T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>13</td>\n",
       "      <td>2023-02-19T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-19T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-20T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-20T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>111</td>\n",
       "      <td>2023-02-21T08:00:00+0000</td>\n",
       "      <td>96</td>\n",
       "      <td>2023-02-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>19</td>\n",
       "      <td>2023-02-22T08:00:00+0000</td>\n",
       "      <td>16</td>\n",
       "      <td>2023-02-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>31</td>\n",
       "      <td>2023-02-23T08:00:00+0000</td>\n",
       "      <td>7</td>\n",
       "      <td>2023-02-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>32</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "      <td>95</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "      <td>28</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>51</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "      <td>30</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    impressions_value      impressions_end_time  reach_value  \\\n",
       "0                  23  2023-01-21T08:00:00+0000           15   \n",
       "1                  23  2023-01-21T08:00:00+0000           15   \n",
       "2                   5  2023-01-22T08:00:00+0000            5   \n",
       "3                   5  2023-01-22T08:00:00+0000            5   \n",
       "4                   3  2023-01-23T08:00:00+0000            3   \n",
       "5                   3  2023-01-23T08:00:00+0000            3   \n",
       "6                   3  2023-01-24T08:00:00+0000            3   \n",
       "7                   3  2023-01-24T08:00:00+0000            3   \n",
       "8                   0  2023-01-25T08:00:00+0000            0   \n",
       "9                   0  2023-01-25T08:00:00+0000            0   \n",
       "10                  2  2023-01-26T08:00:00+0000            2   \n",
       "11                  2  2023-01-26T08:00:00+0000            2   \n",
       "12                 38  2023-01-27T08:00:00+0000            1   \n",
       "13                 38  2023-01-27T08:00:00+0000            1   \n",
       "14                 29  2023-01-28T08:00:00+0000            1   \n",
       "15                 29  2023-01-28T08:00:00+0000            1   \n",
       "16                  5  2023-01-29T08:00:00+0000            4   \n",
       "17                  5  2023-01-29T08:00:00+0000            4   \n",
       "18                196  2023-01-30T08:00:00+0000          141   \n",
       "19                196  2023-01-30T08:00:00+0000          141   \n",
       "20                 45  2023-01-31T08:00:00+0000           43   \n",
       "21                 45  2023-01-31T08:00:00+0000           43   \n",
       "22                 14  2023-02-01T08:00:00+0000            9   \n",
       "23                 14  2023-02-01T08:00:00+0000            9   \n",
       "24                 14  2023-02-01T08:00:00+0000            9   \n",
       "25                  5  2023-02-02T08:00:00+0000            4   \n",
       "26                169  2023-02-03T08:00:00+0000          109   \n",
       "27                 68  2023-02-04T08:00:00+0000           51   \n",
       "28                 57  2023-02-05T08:00:00+0000           29   \n",
       "29                 78  2023-02-06T08:00:00+0000           67   \n",
       "30                266  2023-02-07T08:00:00+0000          208   \n",
       "31                 77  2023-02-08T08:00:00+0000           52   \n",
       "32                 21  2023-02-09T08:00:00+0000           21   \n",
       "33                  3  2023-02-10T08:00:00+0000            3   \n",
       "34                  1  2023-02-11T08:00:00+0000            1   \n",
       "35                184  2023-02-12T08:00:00+0000           98   \n",
       "36                 48  2023-02-13T08:00:00+0000           39   \n",
       "37                200  2023-02-14T08:00:00+0000          131   \n",
       "38                 40  2023-02-15T08:00:00+0000           21   \n",
       "39                 10  2023-02-16T08:00:00+0000           10   \n",
       "40                  2  2023-02-17T08:00:00+0000            2   \n",
       "41                  2  2023-02-18T08:00:00+0000            2   \n",
       "42                 13  2023-02-19T08:00:00+0000            1   \n",
       "43                  4  2023-02-20T08:00:00+0000            2   \n",
       "44                111  2023-02-21T08:00:00+0000           96   \n",
       "45                 19  2023-02-22T08:00:00+0000           16   \n",
       "46                 31  2023-02-23T08:00:00+0000            7   \n",
       "47                  3  2023-02-24T08:00:00+0000            3   \n",
       "48                  1  2023-02-25T08:00:00+0000            1   \n",
       "49                 32  2023-02-26T08:00:00+0000            9   \n",
       "50                109  2023-02-27T08:00:00+0000           95   \n",
       "51                 29  2023-02-28T08:00:00+0000           28   \n",
       "52                289  2023-03-01T08:00:00+0000          170   \n",
       "53                289  2023-03-01T08:00:00+0000          170   \n",
       "54                 51  2023-03-02T08:00:00+0000           30   \n",
       "\n",
       "              reach_end_time  \n",
       "0   2023-01-21T08:00:00+0000  \n",
       "1   2023-01-21T08:00:00+0000  \n",
       "2   2023-01-22T08:00:00+0000  \n",
       "3   2023-01-22T08:00:00+0000  \n",
       "4   2023-01-23T08:00:00+0000  \n",
       "5   2023-01-23T08:00:00+0000  \n",
       "6   2023-01-24T08:00:00+0000  \n",
       "7   2023-01-24T08:00:00+0000  \n",
       "8   2023-01-25T08:00:00+0000  \n",
       "9   2023-01-25T08:00:00+0000  \n",
       "10  2023-01-26T08:00:00+0000  \n",
       "11  2023-01-26T08:00:00+0000  \n",
       "12  2023-01-27T08:00:00+0000  \n",
       "13  2023-01-27T08:00:00+0000  \n",
       "14  2023-01-28T08:00:00+0000  \n",
       "15  2023-01-28T08:00:00+0000  \n",
       "16  2023-01-29T08:00:00+0000  \n",
       "17  2023-01-29T08:00:00+0000  \n",
       "18  2023-01-30T08:00:00+0000  \n",
       "19  2023-01-30T08:00:00+0000  \n",
       "20  2023-01-31T08:00:00+0000  \n",
       "21  2023-01-31T08:00:00+0000  \n",
       "22  2023-02-01T08:00:00+0000  \n",
       "23  2023-02-01T08:00:00+0000  \n",
       "24  2023-02-01T08:00:00+0000  \n",
       "25  2023-02-02T08:00:00+0000  \n",
       "26  2023-02-03T08:00:00+0000  \n",
       "27  2023-02-04T08:00:00+0000  \n",
       "28  2023-02-05T08:00:00+0000  \n",
       "29  2023-02-06T08:00:00+0000  \n",
       "30  2023-02-07T08:00:00+0000  \n",
       "31  2023-02-08T08:00:00+0000  \n",
       "32  2023-02-09T08:00:00+0000  \n",
       "33  2023-02-10T08:00:00+0000  \n",
       "34  2023-02-11T08:00:00+0000  \n",
       "35  2023-02-12T08:00:00+0000  \n",
       "36  2023-02-13T08:00:00+0000  \n",
       "37  2023-02-14T08:00:00+0000  \n",
       "38  2023-02-15T08:00:00+0000  \n",
       "39  2023-02-16T08:00:00+0000  \n",
       "40  2023-02-17T08:00:00+0000  \n",
       "41  2023-02-18T08:00:00+0000  \n",
       "42  2023-02-19T08:00:00+0000  \n",
       "43  2023-02-20T08:00:00+0000  \n",
       "44  2023-02-21T08:00:00+0000  \n",
       "45  2023-02-22T08:00:00+0000  \n",
       "46  2023-02-23T08:00:00+0000  \n",
       "47  2023-02-24T08:00:00+0000  \n",
       "48  2023-02-25T08:00:00+0000  \n",
       "49  2023-02-26T08:00:00+0000  \n",
       "50  2023-02-27T08:00:00+0000  \n",
       "51  2023-02-28T08:00:00+0000  \n",
       "52  2023-03-01T08:00:00+0000  \n",
       "53  2023-03-01T08:00:00+0000  \n",
       "54  2023-03-02T08:00:00+0000  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.sort_values('impressions_end_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 00:07:30.997227\n",
      "Time completed: 2023-03-15 00:07:31.006230\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-14_2_account_insights_df.sav\n",
      "Time completed: 2023-03-15 00:07:31.008217\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-14_2_account_insights.sav\n",
      "Time completed: 2023-03-15 00:07:31.009222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "      <td>15</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "      <td>15</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>38</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>38</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>196</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "      <td>141</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>196</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "      <td>141</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>45</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "      <td>43</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>45</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "      <td>43</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>169</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>68</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "      <td>51</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>57</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>78</td>\n",
       "      <td>2023-02-06T08:00:00+0000</td>\n",
       "      <td>67</td>\n",
       "      <td>2023-02-06T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>266</td>\n",
       "      <td>2023-02-07T08:00:00+0000</td>\n",
       "      <td>208</td>\n",
       "      <td>2023-02-07T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>77</td>\n",
       "      <td>2023-02-08T08:00:00+0000</td>\n",
       "      <td>52</td>\n",
       "      <td>2023-02-08T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-09T08:00:00+0000</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-09T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-10T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-10T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-11T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-11T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>184</td>\n",
       "      <td>2023-02-12T08:00:00+0000</td>\n",
       "      <td>98</td>\n",
       "      <td>2023-02-12T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>48</td>\n",
       "      <td>2023-02-13T08:00:00+0000</td>\n",
       "      <td>39</td>\n",
       "      <td>2023-02-13T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>200</td>\n",
       "      <td>2023-02-14T08:00:00+0000</td>\n",
       "      <td>131</td>\n",
       "      <td>2023-02-14T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>2023-02-15T08:00:00+0000</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-15T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10</td>\n",
       "      <td>2023-02-16T08:00:00+0000</td>\n",
       "      <td>10</td>\n",
       "      <td>2023-02-16T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-17T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-17T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-18T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-18T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>13</td>\n",
       "      <td>2023-02-19T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-19T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-20T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-20T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>111</td>\n",
       "      <td>2023-02-21T08:00:00+0000</td>\n",
       "      <td>96</td>\n",
       "      <td>2023-02-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>19</td>\n",
       "      <td>2023-02-22T08:00:00+0000</td>\n",
       "      <td>16</td>\n",
       "      <td>2023-02-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>31</td>\n",
       "      <td>2023-02-23T08:00:00+0000</td>\n",
       "      <td>7</td>\n",
       "      <td>2023-02-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>32</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "      <td>95</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "      <td>28</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>51</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "      <td>30</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    impressions_value      impressions_end_time  reach_value  \\\n",
       "0                  23  2023-01-21T08:00:00+0000           15   \n",
       "1                  23  2023-01-21T08:00:00+0000           15   \n",
       "2                   5  2023-01-22T08:00:00+0000            5   \n",
       "3                   5  2023-01-22T08:00:00+0000            5   \n",
       "4                   3  2023-01-23T08:00:00+0000            3   \n",
       "5                   3  2023-01-23T08:00:00+0000            3   \n",
       "6                   3  2023-01-24T08:00:00+0000            3   \n",
       "7                   3  2023-01-24T08:00:00+0000            3   \n",
       "8                   0  2023-01-25T08:00:00+0000            0   \n",
       "9                   0  2023-01-25T08:00:00+0000            0   \n",
       "10                  2  2023-01-26T08:00:00+0000            2   \n",
       "11                  2  2023-01-26T08:00:00+0000            2   \n",
       "12                 38  2023-01-27T08:00:00+0000            1   \n",
       "13                 38  2023-01-27T08:00:00+0000            1   \n",
       "14                 29  2023-01-28T08:00:00+0000            1   \n",
       "15                 29  2023-01-28T08:00:00+0000            1   \n",
       "16                  5  2023-01-29T08:00:00+0000            4   \n",
       "17                  5  2023-01-29T08:00:00+0000            4   \n",
       "18                196  2023-01-30T08:00:00+0000          141   \n",
       "19                196  2023-01-30T08:00:00+0000          141   \n",
       "20                 45  2023-01-31T08:00:00+0000           43   \n",
       "21                 45  2023-01-31T08:00:00+0000           43   \n",
       "22                 14  2023-02-01T08:00:00+0000            9   \n",
       "23                 14  2023-02-01T08:00:00+0000            9   \n",
       "24                 14  2023-02-01T08:00:00+0000            9   \n",
       "25                  5  2023-02-02T08:00:00+0000            4   \n",
       "26                169  2023-02-03T08:00:00+0000          109   \n",
       "27                 68  2023-02-04T08:00:00+0000           51   \n",
       "28                 57  2023-02-05T08:00:00+0000           29   \n",
       "29                 78  2023-02-06T08:00:00+0000           67   \n",
       "30                266  2023-02-07T08:00:00+0000          208   \n",
       "31                 77  2023-02-08T08:00:00+0000           52   \n",
       "32                 21  2023-02-09T08:00:00+0000           21   \n",
       "33                  3  2023-02-10T08:00:00+0000            3   \n",
       "34                  1  2023-02-11T08:00:00+0000            1   \n",
       "35                184  2023-02-12T08:00:00+0000           98   \n",
       "36                 48  2023-02-13T08:00:00+0000           39   \n",
       "37                200  2023-02-14T08:00:00+0000          131   \n",
       "38                 40  2023-02-15T08:00:00+0000           21   \n",
       "39                 10  2023-02-16T08:00:00+0000           10   \n",
       "40                  2  2023-02-17T08:00:00+0000            2   \n",
       "41                  2  2023-02-18T08:00:00+0000            2   \n",
       "42                 13  2023-02-19T08:00:00+0000            1   \n",
       "43                  4  2023-02-20T08:00:00+0000            2   \n",
       "44                111  2023-02-21T08:00:00+0000           96   \n",
       "45                 19  2023-02-22T08:00:00+0000           16   \n",
       "46                 31  2023-02-23T08:00:00+0000            7   \n",
       "47                  3  2023-02-24T08:00:00+0000            3   \n",
       "48                  1  2023-02-25T08:00:00+0000            1   \n",
       "49                 32  2023-02-26T08:00:00+0000            9   \n",
       "50                109  2023-02-27T08:00:00+0000           95   \n",
       "51                 29  2023-02-28T08:00:00+0000           28   \n",
       "52                289  2023-03-01T08:00:00+0000          170   \n",
       "53                289  2023-03-01T08:00:00+0000          170   \n",
       "54                 51  2023-03-02T08:00:00+0000           30   \n",
       "\n",
       "              reach_end_time  \n",
       "0   2023-01-21T08:00:00+0000  \n",
       "1   2023-01-21T08:00:00+0000  \n",
       "2   2023-01-22T08:00:00+0000  \n",
       "3   2023-01-22T08:00:00+0000  \n",
       "4   2023-01-23T08:00:00+0000  \n",
       "5   2023-01-23T08:00:00+0000  \n",
       "6   2023-01-24T08:00:00+0000  \n",
       "7   2023-01-24T08:00:00+0000  \n",
       "8   2023-01-25T08:00:00+0000  \n",
       "9   2023-01-25T08:00:00+0000  \n",
       "10  2023-01-26T08:00:00+0000  \n",
       "11  2023-01-26T08:00:00+0000  \n",
       "12  2023-01-27T08:00:00+0000  \n",
       "13  2023-01-27T08:00:00+0000  \n",
       "14  2023-01-28T08:00:00+0000  \n",
       "15  2023-01-28T08:00:00+0000  \n",
       "16  2023-01-29T08:00:00+0000  \n",
       "17  2023-01-29T08:00:00+0000  \n",
       "18  2023-01-30T08:00:00+0000  \n",
       "19  2023-01-30T08:00:00+0000  \n",
       "20  2023-01-31T08:00:00+0000  \n",
       "21  2023-01-31T08:00:00+0000  \n",
       "22  2023-02-01T08:00:00+0000  \n",
       "23  2023-02-01T08:00:00+0000  \n",
       "24  2023-02-01T08:00:00+0000  \n",
       "25  2023-02-02T08:00:00+0000  \n",
       "26  2023-02-03T08:00:00+0000  \n",
       "27  2023-02-04T08:00:00+0000  \n",
       "28  2023-02-05T08:00:00+0000  \n",
       "29  2023-02-06T08:00:00+0000  \n",
       "30  2023-02-07T08:00:00+0000  \n",
       "31  2023-02-08T08:00:00+0000  \n",
       "32  2023-02-09T08:00:00+0000  \n",
       "33  2023-02-10T08:00:00+0000  \n",
       "34  2023-02-11T08:00:00+0000  \n",
       "35  2023-02-12T08:00:00+0000  \n",
       "36  2023-02-13T08:00:00+0000  \n",
       "37  2023-02-14T08:00:00+0000  \n",
       "38  2023-02-15T08:00:00+0000  \n",
       "39  2023-02-16T08:00:00+0000  \n",
       "40  2023-02-17T08:00:00+0000  \n",
       "41  2023-02-18T08:00:00+0000  \n",
       "42  2023-02-19T08:00:00+0000  \n",
       "43  2023-02-20T08:00:00+0000  \n",
       "44  2023-02-21T08:00:00+0000  \n",
       "45  2023-02-22T08:00:00+0000  \n",
       "46  2023-02-23T08:00:00+0000  \n",
       "47  2023-02-24T08:00:00+0000  \n",
       "48  2023-02-25T08:00:00+0000  \n",
       "49  2023-02-26T08:00:00+0000  \n",
       "50  2023-02-27T08:00:00+0000  \n",
       "51  2023-02-28T08:00:00+0000  \n",
       "52  2023-03-01T08:00:00+0000  \n",
       "53  2023-03-01T08:00:00+0000  \n",
       "54  2023-03-02T08:00:00+0000  "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until='2023-03-02', filename=filename2)\n",
    "data2.sort_values('impressions_end_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 00:07:57.110088\n",
      "Time completed: 2023-03-15 00:07:57.119088\n",
      "\n",
      "Fetching newer account insights from 2023-03-02 to 2023-03-05\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1678003200.0&since=1677744000.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-03-02 00:00:00\n",
      "Number of days of data: 4\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-14_2_account_insights_df.sav\n",
      "Time completed: 2023-03-15 00:07:57.546833\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-14_2_account_insights.sav\n",
      "Time completed: 2023-03-15 00:07:57.547836\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "      <td>15</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "      <td>15</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>38</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>38</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>196</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "      <td>141</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>196</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "      <td>141</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>45</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "      <td>43</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>45</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "      <td>43</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>169</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>68</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "      <td>51</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>57</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>78</td>\n",
       "      <td>2023-02-06T08:00:00+0000</td>\n",
       "      <td>67</td>\n",
       "      <td>2023-02-06T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>266</td>\n",
       "      <td>2023-02-07T08:00:00+0000</td>\n",
       "      <td>208</td>\n",
       "      <td>2023-02-07T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>77</td>\n",
       "      <td>2023-02-08T08:00:00+0000</td>\n",
       "      <td>52</td>\n",
       "      <td>2023-02-08T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-09T08:00:00+0000</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-09T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-10T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-10T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-11T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-11T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>184</td>\n",
       "      <td>2023-02-12T08:00:00+0000</td>\n",
       "      <td>98</td>\n",
       "      <td>2023-02-12T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>48</td>\n",
       "      <td>2023-02-13T08:00:00+0000</td>\n",
       "      <td>39</td>\n",
       "      <td>2023-02-13T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>200</td>\n",
       "      <td>2023-02-14T08:00:00+0000</td>\n",
       "      <td>131</td>\n",
       "      <td>2023-02-14T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>2023-02-15T08:00:00+0000</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-15T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10</td>\n",
       "      <td>2023-02-16T08:00:00+0000</td>\n",
       "      <td>10</td>\n",
       "      <td>2023-02-16T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-17T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-17T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-18T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-18T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>13</td>\n",
       "      <td>2023-02-19T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-19T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-20T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-20T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>111</td>\n",
       "      <td>2023-02-21T08:00:00+0000</td>\n",
       "      <td>96</td>\n",
       "      <td>2023-02-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>19</td>\n",
       "      <td>2023-02-22T08:00:00+0000</td>\n",
       "      <td>16</td>\n",
       "      <td>2023-02-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>31</td>\n",
       "      <td>2023-02-23T08:00:00+0000</td>\n",
       "      <td>7</td>\n",
       "      <td>2023-02-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>32</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "      <td>95</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "      <td>28</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>51</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "      <td>30</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "      <td>30</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>2023-03-03T08:00:00+0000</td>\n",
       "      <td>12</td>\n",
       "      <td>2023-03-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>2023-03-04T08:00:00+0000</td>\n",
       "      <td>8</td>\n",
       "      <td>2023-03-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-03-05T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-03-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    impressions_value      impressions_end_time  reach_value  \\\n",
       "0                  23  2023-01-21T08:00:00+0000           15   \n",
       "1                  23  2023-01-21T08:00:00+0000           15   \n",
       "2                   5  2023-01-22T08:00:00+0000            5   \n",
       "3                   5  2023-01-22T08:00:00+0000            5   \n",
       "4                   3  2023-01-23T08:00:00+0000            3   \n",
       "5                   3  2023-01-23T08:00:00+0000            3   \n",
       "6                   3  2023-01-24T08:00:00+0000            3   \n",
       "7                   3  2023-01-24T08:00:00+0000            3   \n",
       "8                   0  2023-01-25T08:00:00+0000            0   \n",
       "9                   0  2023-01-25T08:00:00+0000            0   \n",
       "10                  2  2023-01-26T08:00:00+0000            2   \n",
       "11                  2  2023-01-26T08:00:00+0000            2   \n",
       "13                 38  2023-01-27T08:00:00+0000            1   \n",
       "12                 38  2023-01-27T08:00:00+0000            1   \n",
       "15                 29  2023-01-28T08:00:00+0000            1   \n",
       "14                 29  2023-01-28T08:00:00+0000            1   \n",
       "16                  5  2023-01-29T08:00:00+0000            4   \n",
       "17                  5  2023-01-29T08:00:00+0000            4   \n",
       "18                196  2023-01-30T08:00:00+0000          141   \n",
       "19                196  2023-01-30T08:00:00+0000          141   \n",
       "20                 45  2023-01-31T08:00:00+0000           43   \n",
       "21                 45  2023-01-31T08:00:00+0000           43   \n",
       "22                 14  2023-02-01T08:00:00+0000            9   \n",
       "23                 14  2023-02-01T08:00:00+0000            9   \n",
       "24                 14  2023-02-01T08:00:00+0000            9   \n",
       "25                  5  2023-02-02T08:00:00+0000            4   \n",
       "26                169  2023-02-03T08:00:00+0000          109   \n",
       "27                 68  2023-02-04T08:00:00+0000           51   \n",
       "28                 57  2023-02-05T08:00:00+0000           29   \n",
       "29                 78  2023-02-06T08:00:00+0000           67   \n",
       "30                266  2023-02-07T08:00:00+0000          208   \n",
       "31                 77  2023-02-08T08:00:00+0000           52   \n",
       "32                 21  2023-02-09T08:00:00+0000           21   \n",
       "33                  3  2023-02-10T08:00:00+0000            3   \n",
       "34                  1  2023-02-11T08:00:00+0000            1   \n",
       "35                184  2023-02-12T08:00:00+0000           98   \n",
       "36                 48  2023-02-13T08:00:00+0000           39   \n",
       "37                200  2023-02-14T08:00:00+0000          131   \n",
       "38                 40  2023-02-15T08:00:00+0000           21   \n",
       "39                 10  2023-02-16T08:00:00+0000           10   \n",
       "40                  2  2023-02-17T08:00:00+0000            2   \n",
       "41                  2  2023-02-18T08:00:00+0000            2   \n",
       "42                 13  2023-02-19T08:00:00+0000            1   \n",
       "43                  4  2023-02-20T08:00:00+0000            2   \n",
       "44                111  2023-02-21T08:00:00+0000           96   \n",
       "45                 19  2023-02-22T08:00:00+0000           16   \n",
       "46                 31  2023-02-23T08:00:00+0000            7   \n",
       "47                  3  2023-02-24T08:00:00+0000            3   \n",
       "48                  1  2023-02-25T08:00:00+0000            1   \n",
       "49                 32  2023-02-26T08:00:00+0000            9   \n",
       "50                109  2023-02-27T08:00:00+0000           95   \n",
       "51                 29  2023-02-28T08:00:00+0000           28   \n",
       "52                289  2023-03-01T08:00:00+0000          170   \n",
       "53                289  2023-03-01T08:00:00+0000          170   \n",
       "54                 51  2023-03-02T08:00:00+0000           30   \n",
       "0                  51  2023-03-02T08:00:00+0000           30   \n",
       "1                  12  2023-03-03T08:00:00+0000           12   \n",
       "2                  12  2023-03-04T08:00:00+0000            8   \n",
       "3                   3  2023-03-05T08:00:00+0000            3   \n",
       "\n",
       "              reach_end_time  \n",
       "0   2023-01-21T08:00:00+0000  \n",
       "1   2023-01-21T08:00:00+0000  \n",
       "2   2023-01-22T08:00:00+0000  \n",
       "3   2023-01-22T08:00:00+0000  \n",
       "4   2023-01-23T08:00:00+0000  \n",
       "5   2023-01-23T08:00:00+0000  \n",
       "6   2023-01-24T08:00:00+0000  \n",
       "7   2023-01-24T08:00:00+0000  \n",
       "8   2023-01-25T08:00:00+0000  \n",
       "9   2023-01-25T08:00:00+0000  \n",
       "10  2023-01-26T08:00:00+0000  \n",
       "11  2023-01-26T08:00:00+0000  \n",
       "13  2023-01-27T08:00:00+0000  \n",
       "12  2023-01-27T08:00:00+0000  \n",
       "15  2023-01-28T08:00:00+0000  \n",
       "14  2023-01-28T08:00:00+0000  \n",
       "16  2023-01-29T08:00:00+0000  \n",
       "17  2023-01-29T08:00:00+0000  \n",
       "18  2023-01-30T08:00:00+0000  \n",
       "19  2023-01-30T08:00:00+0000  \n",
       "20  2023-01-31T08:00:00+0000  \n",
       "21  2023-01-31T08:00:00+0000  \n",
       "22  2023-02-01T08:00:00+0000  \n",
       "23  2023-02-01T08:00:00+0000  \n",
       "24  2023-02-01T08:00:00+0000  \n",
       "25  2023-02-02T08:00:00+0000  \n",
       "26  2023-02-03T08:00:00+0000  \n",
       "27  2023-02-04T08:00:00+0000  \n",
       "28  2023-02-05T08:00:00+0000  \n",
       "29  2023-02-06T08:00:00+0000  \n",
       "30  2023-02-07T08:00:00+0000  \n",
       "31  2023-02-08T08:00:00+0000  \n",
       "32  2023-02-09T08:00:00+0000  \n",
       "33  2023-02-10T08:00:00+0000  \n",
       "34  2023-02-11T08:00:00+0000  \n",
       "35  2023-02-12T08:00:00+0000  \n",
       "36  2023-02-13T08:00:00+0000  \n",
       "37  2023-02-14T08:00:00+0000  \n",
       "38  2023-02-15T08:00:00+0000  \n",
       "39  2023-02-16T08:00:00+0000  \n",
       "40  2023-02-17T08:00:00+0000  \n",
       "41  2023-02-18T08:00:00+0000  \n",
       "42  2023-02-19T08:00:00+0000  \n",
       "43  2023-02-20T08:00:00+0000  \n",
       "44  2023-02-21T08:00:00+0000  \n",
       "45  2023-02-22T08:00:00+0000  \n",
       "46  2023-02-23T08:00:00+0000  \n",
       "47  2023-02-24T08:00:00+0000  \n",
       "48  2023-02-25T08:00:00+0000  \n",
       "49  2023-02-26T08:00:00+0000  \n",
       "50  2023-02-27T08:00:00+0000  \n",
       "51  2023-02-28T08:00:00+0000  \n",
       "52  2023-03-01T08:00:00+0000  \n",
       "53  2023-03-01T08:00:00+0000  \n",
       "54  2023-03-02T08:00:00+0000  \n",
       "0   2023-03-02T08:00:00+0000  \n",
       "1   2023-03-03T08:00:00+0000  \n",
       "2   2023-03-04T08:00:00+0000  \n",
       "3   2023-03-05T08:00:00+0000  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until='2023-03-05', filename=filename2)\n",
    "data2.sort_values('impressions_end_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 00:09:04.279704\n",
      "Time completed: 2023-03-15 00:09:04.287665\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-14_2_account_insights_df.sav\n",
      "Time completed: 2023-03-15 00:09:04.290678\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-14_2_account_insights.sav\n",
      "Time completed: 2023-03-15 00:09:04.291667\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "      <td>15</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "      <td>15</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>38</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>38</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>196</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "      <td>141</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>196</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "      <td>141</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>45</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "      <td>43</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>45</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "      <td>43</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>169</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>68</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "      <td>51</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>57</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>78</td>\n",
       "      <td>2023-02-06T08:00:00+0000</td>\n",
       "      <td>67</td>\n",
       "      <td>2023-02-06T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>266</td>\n",
       "      <td>2023-02-07T08:00:00+0000</td>\n",
       "      <td>208</td>\n",
       "      <td>2023-02-07T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>77</td>\n",
       "      <td>2023-02-08T08:00:00+0000</td>\n",
       "      <td>52</td>\n",
       "      <td>2023-02-08T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-09T08:00:00+0000</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-09T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-10T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-10T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-11T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-11T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>184</td>\n",
       "      <td>2023-02-12T08:00:00+0000</td>\n",
       "      <td>98</td>\n",
       "      <td>2023-02-12T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>48</td>\n",
       "      <td>2023-02-13T08:00:00+0000</td>\n",
       "      <td>39</td>\n",
       "      <td>2023-02-13T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>200</td>\n",
       "      <td>2023-02-14T08:00:00+0000</td>\n",
       "      <td>131</td>\n",
       "      <td>2023-02-14T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>2023-02-15T08:00:00+0000</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-15T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10</td>\n",
       "      <td>2023-02-16T08:00:00+0000</td>\n",
       "      <td>10</td>\n",
       "      <td>2023-02-16T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-17T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-17T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-18T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-18T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>13</td>\n",
       "      <td>2023-02-19T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-19T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-20T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-20T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>111</td>\n",
       "      <td>2023-02-21T08:00:00+0000</td>\n",
       "      <td>96</td>\n",
       "      <td>2023-02-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>19</td>\n",
       "      <td>2023-02-22T08:00:00+0000</td>\n",
       "      <td>16</td>\n",
       "      <td>2023-02-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>31</td>\n",
       "      <td>2023-02-23T08:00:00+0000</td>\n",
       "      <td>7</td>\n",
       "      <td>2023-02-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>32</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "      <td>95</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "      <td>28</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>51</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "      <td>30</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>51</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "      <td>30</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>12</td>\n",
       "      <td>2023-03-03T08:00:00+0000</td>\n",
       "      <td>12</td>\n",
       "      <td>2023-03-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>12</td>\n",
       "      <td>2023-03-04T08:00:00+0000</td>\n",
       "      <td>8</td>\n",
       "      <td>2023-03-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-03-05T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-03-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    impressions_value      impressions_end_time  reach_value  \\\n",
       "0                  23  2023-01-21T08:00:00+0000           15   \n",
       "1                  23  2023-01-21T08:00:00+0000           15   \n",
       "2                   5  2023-01-22T08:00:00+0000            5   \n",
       "3                   5  2023-01-22T08:00:00+0000            5   \n",
       "4                   3  2023-01-23T08:00:00+0000            3   \n",
       "5                   3  2023-01-23T08:00:00+0000            3   \n",
       "6                   3  2023-01-24T08:00:00+0000            3   \n",
       "7                   3  2023-01-24T08:00:00+0000            3   \n",
       "8                   0  2023-01-25T08:00:00+0000            0   \n",
       "9                   0  2023-01-25T08:00:00+0000            0   \n",
       "10                  2  2023-01-26T08:00:00+0000            2   \n",
       "11                  2  2023-01-26T08:00:00+0000            2   \n",
       "12                 38  2023-01-27T08:00:00+0000            1   \n",
       "13                 38  2023-01-27T08:00:00+0000            1   \n",
       "14                 29  2023-01-28T08:00:00+0000            1   \n",
       "15                 29  2023-01-28T08:00:00+0000            1   \n",
       "16                  5  2023-01-29T08:00:00+0000            4   \n",
       "17                  5  2023-01-29T08:00:00+0000            4   \n",
       "18                196  2023-01-30T08:00:00+0000          141   \n",
       "19                196  2023-01-30T08:00:00+0000          141   \n",
       "20                 45  2023-01-31T08:00:00+0000           43   \n",
       "21                 45  2023-01-31T08:00:00+0000           43   \n",
       "22                 14  2023-02-01T08:00:00+0000            9   \n",
       "23                 14  2023-02-01T08:00:00+0000            9   \n",
       "24                 14  2023-02-01T08:00:00+0000            9   \n",
       "25                  5  2023-02-02T08:00:00+0000            4   \n",
       "26                169  2023-02-03T08:00:00+0000          109   \n",
       "27                 68  2023-02-04T08:00:00+0000           51   \n",
       "28                 57  2023-02-05T08:00:00+0000           29   \n",
       "29                 78  2023-02-06T08:00:00+0000           67   \n",
       "30                266  2023-02-07T08:00:00+0000          208   \n",
       "31                 77  2023-02-08T08:00:00+0000           52   \n",
       "32                 21  2023-02-09T08:00:00+0000           21   \n",
       "33                  3  2023-02-10T08:00:00+0000            3   \n",
       "34                  1  2023-02-11T08:00:00+0000            1   \n",
       "35                184  2023-02-12T08:00:00+0000           98   \n",
       "36                 48  2023-02-13T08:00:00+0000           39   \n",
       "37                200  2023-02-14T08:00:00+0000          131   \n",
       "38                 40  2023-02-15T08:00:00+0000           21   \n",
       "39                 10  2023-02-16T08:00:00+0000           10   \n",
       "40                  2  2023-02-17T08:00:00+0000            2   \n",
       "41                  2  2023-02-18T08:00:00+0000            2   \n",
       "42                 13  2023-02-19T08:00:00+0000            1   \n",
       "43                  4  2023-02-20T08:00:00+0000            2   \n",
       "44                111  2023-02-21T08:00:00+0000           96   \n",
       "45                 19  2023-02-22T08:00:00+0000           16   \n",
       "46                 31  2023-02-23T08:00:00+0000            7   \n",
       "47                  3  2023-02-24T08:00:00+0000            3   \n",
       "48                  1  2023-02-25T08:00:00+0000            1   \n",
       "49                 32  2023-02-26T08:00:00+0000            9   \n",
       "50                109  2023-02-27T08:00:00+0000           95   \n",
       "51                 29  2023-02-28T08:00:00+0000           28   \n",
       "52                289  2023-03-01T08:00:00+0000          170   \n",
       "53                289  2023-03-01T08:00:00+0000          170   \n",
       "54                 51  2023-03-02T08:00:00+0000           30   \n",
       "55                 51  2023-03-02T08:00:00+0000           30   \n",
       "56                 12  2023-03-03T08:00:00+0000           12   \n",
       "57                 12  2023-03-04T08:00:00+0000            8   \n",
       "58                  3  2023-03-05T08:00:00+0000            3   \n",
       "\n",
       "              reach_end_time  \n",
       "0   2023-01-21T08:00:00+0000  \n",
       "1   2023-01-21T08:00:00+0000  \n",
       "2   2023-01-22T08:00:00+0000  \n",
       "3   2023-01-22T08:00:00+0000  \n",
       "4   2023-01-23T08:00:00+0000  \n",
       "5   2023-01-23T08:00:00+0000  \n",
       "6   2023-01-24T08:00:00+0000  \n",
       "7   2023-01-24T08:00:00+0000  \n",
       "8   2023-01-25T08:00:00+0000  \n",
       "9   2023-01-25T08:00:00+0000  \n",
       "10  2023-01-26T08:00:00+0000  \n",
       "11  2023-01-26T08:00:00+0000  \n",
       "12  2023-01-27T08:00:00+0000  \n",
       "13  2023-01-27T08:00:00+0000  \n",
       "14  2023-01-28T08:00:00+0000  \n",
       "15  2023-01-28T08:00:00+0000  \n",
       "16  2023-01-29T08:00:00+0000  \n",
       "17  2023-01-29T08:00:00+0000  \n",
       "18  2023-01-30T08:00:00+0000  \n",
       "19  2023-01-30T08:00:00+0000  \n",
       "20  2023-01-31T08:00:00+0000  \n",
       "21  2023-01-31T08:00:00+0000  \n",
       "22  2023-02-01T08:00:00+0000  \n",
       "23  2023-02-01T08:00:00+0000  \n",
       "24  2023-02-01T08:00:00+0000  \n",
       "25  2023-02-02T08:00:00+0000  \n",
       "26  2023-02-03T08:00:00+0000  \n",
       "27  2023-02-04T08:00:00+0000  \n",
       "28  2023-02-05T08:00:00+0000  \n",
       "29  2023-02-06T08:00:00+0000  \n",
       "30  2023-02-07T08:00:00+0000  \n",
       "31  2023-02-08T08:00:00+0000  \n",
       "32  2023-02-09T08:00:00+0000  \n",
       "33  2023-02-10T08:00:00+0000  \n",
       "34  2023-02-11T08:00:00+0000  \n",
       "35  2023-02-12T08:00:00+0000  \n",
       "36  2023-02-13T08:00:00+0000  \n",
       "37  2023-02-14T08:00:00+0000  \n",
       "38  2023-02-15T08:00:00+0000  \n",
       "39  2023-02-16T08:00:00+0000  \n",
       "40  2023-02-17T08:00:00+0000  \n",
       "41  2023-02-18T08:00:00+0000  \n",
       "42  2023-02-19T08:00:00+0000  \n",
       "43  2023-02-20T08:00:00+0000  \n",
       "44  2023-02-21T08:00:00+0000  \n",
       "45  2023-02-22T08:00:00+0000  \n",
       "46  2023-02-23T08:00:00+0000  \n",
       "47  2023-02-24T08:00:00+0000  \n",
       "48  2023-02-25T08:00:00+0000  \n",
       "49  2023-02-26T08:00:00+0000  \n",
       "50  2023-02-27T08:00:00+0000  \n",
       "51  2023-02-28T08:00:00+0000  \n",
       "52  2023-03-01T08:00:00+0000  \n",
       "53  2023-03-01T08:00:00+0000  \n",
       "54  2023-03-02T08:00:00+0000  \n",
       "55  2023-03-02T08:00:00+0000  \n",
       "56  2023-03-03T08:00:00+0000  \n",
       "57  2023-03-04T08:00:00+0000  \n",
       "58  2023-03-05T08:00:00+0000  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until='2023-03-02', filename=filename2)\n",
    "data2.sort_values('impressions_end_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 00:09:45.417498\n",
      "Time completed: 2023-03-15 00:09:45.435492\n",
      "\n",
      "Fetching older account insights from 2023-01-11 to 2023-01-21\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1674288000.0&since=1673424000.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-11 00:00:00\n",
      "Number of days of data: 11\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-14_2_account_insights_df.sav\n",
      "Time completed: 2023-03-15 00:09:45.770323\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-14_2_account_insights.sav\n",
      "Time completed: 2023-03-15 00:09:45.771327\n"
     ]
    }
   ],
   "source": [
    "data2, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-11', until='2023-03-02', filename=filename2)\n",
    "# data2.sort_values('impressions_end_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>2023-01-11T08:00:00+0000</td>\n",
       "      <td>25</td>\n",
       "      <td>2023-01-11T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>2023-01-12T08:00:00+0000</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-12T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>412</td>\n",
       "      <td>2023-01-13T08:00:00+0000</td>\n",
       "      <td>180</td>\n",
       "      <td>2023-01-13T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>2023-01-14T08:00:00+0000</td>\n",
       "      <td>39</td>\n",
       "      <td>2023-01-14T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>2023-01-15T08:00:00+0000</td>\n",
       "      <td>11</td>\n",
       "      <td>2023-01-15T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>51</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "      <td>30</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>51</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "      <td>30</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>12</td>\n",
       "      <td>2023-03-03T08:00:00+0000</td>\n",
       "      <td>12</td>\n",
       "      <td>2023-03-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>12</td>\n",
       "      <td>2023-03-04T08:00:00+0000</td>\n",
       "      <td>8</td>\n",
       "      <td>2023-03-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-03-05T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-03-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    impressions_value      impressions_end_time  reach_value  \\\n",
       "0                  28  2023-01-11T08:00:00+0000           25   \n",
       "1                   7  2023-01-12T08:00:00+0000            5   \n",
       "2                 412  2023-01-13T08:00:00+0000          180   \n",
       "3                  45  2023-01-14T08:00:00+0000           39   \n",
       "4                  12  2023-01-15T08:00:00+0000           11   \n",
       "..                ...                       ...          ...   \n",
       "55                 51  2023-03-02T08:00:00+0000           30   \n",
       "54                 51  2023-03-02T08:00:00+0000           30   \n",
       "56                 12  2023-03-03T08:00:00+0000           12   \n",
       "57                 12  2023-03-04T08:00:00+0000            8   \n",
       "58                  3  2023-03-05T08:00:00+0000            3   \n",
       "\n",
       "              reach_end_time  \n",
       "0   2023-01-11T08:00:00+0000  \n",
       "1   2023-01-12T08:00:00+0000  \n",
       "2   2023-01-13T08:00:00+0000  \n",
       "3   2023-01-14T08:00:00+0000  \n",
       "4   2023-01-15T08:00:00+0000  \n",
       "..                       ...  \n",
       "55  2023-03-02T08:00:00+0000  \n",
       "54  2023-03-02T08:00:00+0000  \n",
       "56  2023-03-03T08:00:00+0000  \n",
       "57  2023-03-04T08:00:00+0000  \n",
       "58  2023-03-05T08:00:00+0000  \n",
       "\n",
       "[70 rows x 4 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.sort_values('impressions_end_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 00:10:38.321547\n",
      "Time completed: 2023-03-15 00:10:38.331554\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-14_2_account_insights_df.sav\n",
      "Time completed: 2023-03-15 00:10:38.333554\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-14_2_account_insights.sav\n",
      "Time completed: 2023-03-15 00:10:38.334551\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>2023-01-11T08:00:00+0000</td>\n",
       "      <td>25</td>\n",
       "      <td>2023-01-11T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>2023-01-12T08:00:00+0000</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-12T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>412</td>\n",
       "      <td>2023-01-13T08:00:00+0000</td>\n",
       "      <td>180</td>\n",
       "      <td>2023-01-13T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>2023-01-14T08:00:00+0000</td>\n",
       "      <td>39</td>\n",
       "      <td>2023-01-14T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>2023-01-15T08:00:00+0000</td>\n",
       "      <td>11</td>\n",
       "      <td>2023-01-15T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>51</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "      <td>30</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>51</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "      <td>30</td>\n",
       "      <td>2023-03-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>12</td>\n",
       "      <td>2023-03-03T08:00:00+0000</td>\n",
       "      <td>12</td>\n",
       "      <td>2023-03-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>12</td>\n",
       "      <td>2023-03-04T08:00:00+0000</td>\n",
       "      <td>8</td>\n",
       "      <td>2023-03-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-03-05T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-03-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    impressions_value      impressions_end_time  reach_value  \\\n",
       "0                  28  2023-01-11T08:00:00+0000           25   \n",
       "1                   7  2023-01-12T08:00:00+0000            5   \n",
       "2                 412  2023-01-13T08:00:00+0000          180   \n",
       "3                  45  2023-01-14T08:00:00+0000           39   \n",
       "4                  12  2023-01-15T08:00:00+0000           11   \n",
       "..                ...                       ...          ...   \n",
       "65                 51  2023-03-02T08:00:00+0000           30   \n",
       "66                 51  2023-03-02T08:00:00+0000           30   \n",
       "67                 12  2023-03-03T08:00:00+0000           12   \n",
       "68                 12  2023-03-04T08:00:00+0000            8   \n",
       "69                  3  2023-03-05T08:00:00+0000            3   \n",
       "\n",
       "              reach_end_time  \n",
       "0   2023-01-11T08:00:00+0000  \n",
       "1   2023-01-12T08:00:00+0000  \n",
       "2   2023-01-13T08:00:00+0000  \n",
       "3   2023-01-14T08:00:00+0000  \n",
       "4   2023-01-15T08:00:00+0000  \n",
       "..                       ...  \n",
       "65  2023-03-02T08:00:00+0000  \n",
       "66  2023-03-02T08:00:00+0000  \n",
       "67  2023-03-03T08:00:00+0000  \n",
       "68  2023-03-04T08:00:00+0000  \n",
       "69  2023-03-05T08:00:00+0000  \n",
       "\n",
       "[70 rows x 4 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-31', until='2023-03-02', filename=filename2)\n",
    "data2.sort_values('impressions_end_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 00:08:38.361854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2023-01-21T08:00:00+0000'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadpickle(\n",
    "    'silvialiftsweights_03-14_2_account_insights_df.sav', \n",
    "    r'C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim'\n",
    "    ).reset_index(drop=True).sort_values('impressions_end_time').loc[0, 'impressions_end_time']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None, \n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since + timedelta(days=1)\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter > since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = df_list \n",
    "    if filename:\n",
    "        filename += '_account_insights'\n",
    "        try:\n",
    "            savepickle(df,filename+'_df','sav',csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1677657600.0&since=1675238400.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-02-01 00:00:00\n",
      "Number of days of data: 29\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_account_insights_account_insights_df.sav\n",
      "Time completed: 2023-03-15 00:31:41.710214\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_account_insights_account_insights.sav\n",
      "Time completed: 2023-03-15 00:31:41.711216\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    timestamp_column_suffix='end_time', filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    previous_since, previous_until = None, None\n",
    "    if filename:\n",
    "        filename = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename+'_df.sav', csv_path)\n",
    "        df = df.reset_index(drop=True)\n",
    "        timestamp_column = df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]\n",
    "        df = df.sort_values(timestamp_column)\n",
    "        response_json_dict = loadpickle(filename+'.sav', json_path)\n",
    "        last_json_page = max(response_json_dict.keys())\n",
    "        previous_since = datetime.strptime(df.iloc[0][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        previous_until = datetime.strptime(df.iloc[-1][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    except:\n",
    "        pass\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    \n",
    "    if (previous_since == None) & (previous_until == None):\n",
    "        df, response_json_dict = get_ig_account_insights(ig_user_id, access_token, since=since, until=until, filename=filename)\n",
    "        return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "    elif previous_since == None:\n",
    "        previous_since = since + timedelta (days=1)\n",
    "        print('Previous `since` parameter could not be found; default to since + 1.')\n",
    "    elif previous_until == None:\n",
    "        previous_until = until - timedelta (days=1)\n",
    "        print('Previous `until` parameter could not be found; default to until - 1.')\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'\\nFetching older account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        older_insights_df, older_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=since, until=previous_since)\n",
    "        try:\n",
    "            df = pd.concat([df, older_insights_df])\n",
    "        except:\n",
    "            df = older_insights_df\n",
    "        try:\n",
    "            # Update the keys of *response_json_dict* before merging with older_insights_response_json_dict. That way, final \n",
    "                # response dictionary always has insights from oldest dates first\n",
    "            response_json_dict = dict( \n",
    "                zip([key+len(older_insights_response_json_dict) for key in response_json_dict.keys()], response_json_dict.values())\n",
    "                )\n",
    "            response_json_dict = {**older_insights_response_json_dict, **response_json_dict}\n",
    "        except:\n",
    "            response_json_dict = older_insights_response_json_dict\n",
    "    if (previous_until.date() < until.date()):\n",
    "        print(f'\\nFetching newer account insights from {datetime.strftime(previous_until, \"%Y-%m-%d\")} to {datetime.strftime(until, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=previous_until, until=until)\n",
    "        try:\n",
    "            df = pd.concat([df, new_insights_df])\n",
    "        except:\n",
    "            df = new_insights_df\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "        \n",
    "    if filename:\n",
    "        try:\n",
    "            savepickle(df, filename+'_df', 'sav', csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename3 = 'silvialiftsweights_03-15'\n",
    "\n",
    "data2, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>169</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "      <td>51</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>78</td>\n",
       "      <td>2023-02-06T08:00:00+0000</td>\n",
       "      <td>67</td>\n",
       "      <td>2023-02-06T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>266</td>\n",
       "      <td>2023-02-07T08:00:00+0000</td>\n",
       "      <td>208</td>\n",
       "      <td>2023-02-07T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>77</td>\n",
       "      <td>2023-02-08T08:00:00+0000</td>\n",
       "      <td>52</td>\n",
       "      <td>2023-02-08T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-09T08:00:00+0000</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-09T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-10T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-10T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-11T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-11T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>184</td>\n",
       "      <td>2023-02-12T08:00:00+0000</td>\n",
       "      <td>98</td>\n",
       "      <td>2023-02-12T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>48</td>\n",
       "      <td>2023-02-13T08:00:00+0000</td>\n",
       "      <td>39</td>\n",
       "      <td>2023-02-13T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>200</td>\n",
       "      <td>2023-02-14T08:00:00+0000</td>\n",
       "      <td>131</td>\n",
       "      <td>2023-02-14T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>40</td>\n",
       "      <td>2023-02-15T08:00:00+0000</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-15T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>2023-02-16T08:00:00+0000</td>\n",
       "      <td>10</td>\n",
       "      <td>2023-02-16T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-17T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-17T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-18T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-18T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13</td>\n",
       "      <td>2023-02-19T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-19T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-20T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-20T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>111</td>\n",
       "      <td>2023-02-21T08:00:00+0000</td>\n",
       "      <td>96</td>\n",
       "      <td>2023-02-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19</td>\n",
       "      <td>2023-02-22T08:00:00+0000</td>\n",
       "      <td>16</td>\n",
       "      <td>2023-02-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>31</td>\n",
       "      <td>2023-02-23T08:00:00+0000</td>\n",
       "      <td>7</td>\n",
       "      <td>2023-02-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "      <td>95</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "      <td>28</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    impressions_value      impressions_end_time  reach_value  \\\n",
       "0                  14  2023-02-01T08:00:00+0000            9   \n",
       "1                   5  2023-02-02T08:00:00+0000            4   \n",
       "2                 169  2023-02-03T08:00:00+0000          109   \n",
       "3                  68  2023-02-04T08:00:00+0000           51   \n",
       "4                  57  2023-02-05T08:00:00+0000           29   \n",
       "5                  78  2023-02-06T08:00:00+0000           67   \n",
       "6                 266  2023-02-07T08:00:00+0000          208   \n",
       "7                  77  2023-02-08T08:00:00+0000           52   \n",
       "8                  21  2023-02-09T08:00:00+0000           21   \n",
       "9                   3  2023-02-10T08:00:00+0000            3   \n",
       "10                  1  2023-02-11T08:00:00+0000            1   \n",
       "11                184  2023-02-12T08:00:00+0000           98   \n",
       "12                 48  2023-02-13T08:00:00+0000           39   \n",
       "13                200  2023-02-14T08:00:00+0000          131   \n",
       "14                 40  2023-02-15T08:00:00+0000           21   \n",
       "15                 10  2023-02-16T08:00:00+0000           10   \n",
       "16                  2  2023-02-17T08:00:00+0000            2   \n",
       "17                  2  2023-02-18T08:00:00+0000            2   \n",
       "18                 13  2023-02-19T08:00:00+0000            1   \n",
       "19                  4  2023-02-20T08:00:00+0000            2   \n",
       "20                111  2023-02-21T08:00:00+0000           96   \n",
       "21                 19  2023-02-22T08:00:00+0000           16   \n",
       "22                 31  2023-02-23T08:00:00+0000            7   \n",
       "23                  3  2023-02-24T08:00:00+0000            3   \n",
       "24                  1  2023-02-25T08:00:00+0000            1   \n",
       "25                 32  2023-02-26T08:00:00+0000            9   \n",
       "26                109  2023-02-27T08:00:00+0000           95   \n",
       "27                 29  2023-02-28T08:00:00+0000           28   \n",
       "28                289  2023-03-01T08:00:00+0000          170   \n",
       "\n",
       "              reach_end_time  \n",
       "0   2023-02-01T08:00:00+0000  \n",
       "1   2023-02-02T08:00:00+0000  \n",
       "2   2023-02-03T08:00:00+0000  \n",
       "3   2023-02-04T08:00:00+0000  \n",
       "4   2023-02-05T08:00:00+0000  \n",
       "5   2023-02-06T08:00:00+0000  \n",
       "6   2023-02-07T08:00:00+0000  \n",
       "7   2023-02-08T08:00:00+0000  \n",
       "8   2023-02-09T08:00:00+0000  \n",
       "9   2023-02-10T08:00:00+0000  \n",
       "10  2023-02-11T08:00:00+0000  \n",
       "11  2023-02-12T08:00:00+0000  \n",
       "12  2023-02-13T08:00:00+0000  \n",
       "13  2023-02-14T08:00:00+0000  \n",
       "14  2023-02-15T08:00:00+0000  \n",
       "15  2023-02-16T08:00:00+0000  \n",
       "16  2023-02-17T08:00:00+0000  \n",
       "17  2023-02-18T08:00:00+0000  \n",
       "18  2023-02-19T08:00:00+0000  \n",
       "19  2023-02-20T08:00:00+0000  \n",
       "20  2023-02-21T08:00:00+0000  \n",
       "21  2023-02-22T08:00:00+0000  \n",
       "22  2023-02-23T08:00:00+0000  \n",
       "23  2023-02-24T08:00:00+0000  \n",
       "24  2023-02-25T08:00:00+0000  \n",
       "25  2023-02-26T08:00:00+0000  \n",
       "26  2023-02-27T08:00:00+0000  \n",
       "27  2023-02-28T08:00:00+0000  \n",
       "28  2023-03-01T08:00:00+0000  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1677657600.0&since=1675065600.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-30 00:00:00\n",
      "Requesting page 2...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2022-12-31 00:00:00\n",
      "Number of days of data: 61\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_account_insights_account_insights_df.sav\n",
      "Time completed: 2023-03-15 00:31:57.793222\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_account_insights_account_insights.sav\n",
      "Time completed: 2023-03-15 00:31:57.794222\n"
     ]
    }
   ],
   "source": [
    "data2, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-01', until=until, filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>2022-12-31T08:00:00+0000</td>\n",
       "      <td>17</td>\n",
       "      <td>2022-12-31T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>2023-01-01T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-02T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>169</td>\n",
       "      <td>2023-01-03T08:00:00+0000</td>\n",
       "      <td>132</td>\n",
       "      <td>2023-01-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>141</td>\n",
       "      <td>2023-01-04T08:00:00+0000</td>\n",
       "      <td>103</td>\n",
       "      <td>2023-01-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>32</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "      <td>95</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "      <td>28</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    impressions_value      impressions_end_time  reach_value  \\\n",
       "0                  18  2022-12-31T08:00:00+0000           17   \n",
       "1                  18  2023-01-01T08:00:00+0000            4   \n",
       "2                   3  2023-01-02T08:00:00+0000            3   \n",
       "3                 169  2023-01-03T08:00:00+0000          132   \n",
       "4                 141  2023-01-04T08:00:00+0000          103   \n",
       "..                ...                       ...          ...   \n",
       "56                  1  2023-02-25T08:00:00+0000            1   \n",
       "57                 32  2023-02-26T08:00:00+0000            9   \n",
       "58                109  2023-02-27T08:00:00+0000           95   \n",
       "59                 29  2023-02-28T08:00:00+0000           28   \n",
       "60                289  2023-03-01T08:00:00+0000          170   \n",
       "\n",
       "              reach_end_time  \n",
       "0   2022-12-31T08:00:00+0000  \n",
       "1   2023-01-01T08:00:00+0000  \n",
       "2   2023-01-02T08:00:00+0000  \n",
       "3   2023-01-03T08:00:00+0000  \n",
       "4   2023-01-04T08:00:00+0000  \n",
       "..                       ...  \n",
       "56  2023-02-25T08:00:00+0000  \n",
       "57  2023-02-26T08:00:00+0000  \n",
       "58  2023-02-27T08:00:00+0000  \n",
       "59  2023-02-28T08:00:00+0000  \n",
       "60  2023-03-01T08:00:00+0000  \n",
       "\n",
       "[61 rows x 4 columns]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1677657600.0&since=1675065600.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-30 00:00:00\n",
      "Requesting page 2...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2022-12-31 00:00:00\n",
      "Number of days of data: 61\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_account_insights_account_insights_df.sav\n",
      "Time completed: 2023-03-15 00:33:14.475190\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_account_insights_account_insights.sav\n",
      "Time completed: 2023-03-15 00:33:14.476194\n"
     ]
    }
   ],
   "source": [
    "data2, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-01', until=until, filename=filename3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None, \n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since + timedelta(days=1)\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter > since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = df_list \n",
    "    if filename:\n",
    "        filename += '_account_insights'\n",
    "        try:\n",
    "            savepickle(df,filename+'_df','sav',csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to load prior results; making new API calls for entire date range.\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1677657600.0&since=1675238400.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-02-01 00:00:00\n",
      "Number of days of data: 29\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_2_account_insights_account_insights_df.sav\n",
      "Time completed: 2023-03-15 00:37:54.319413\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_2_account_insights_account_insights.sav\n",
      "Time completed: 2023-03-15 00:37:54.319413\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    timestamp_column_suffix='end_time', filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    previous_since, previous_until = None, None\n",
    "    if filename:\n",
    "        filename = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename+'_df.sav', csv_path)\n",
    "        df = df.reset_index(drop=True)\n",
    "        timestamp_column = df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]\n",
    "        df = df.sort_values(timestamp_column)\n",
    "        response_json_dict = loadpickle(filename+'.sav', json_path)\n",
    "        last_json_page = max(response_json_dict.keys())\n",
    "        previous_since = datetime.strptime(df.iloc[0][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        previous_until = datetime.strptime(df.iloc[-1][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    except:\n",
    "        print('Unable to load prior results; making new API calls for entire date range.')\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    \n",
    "    if (previous_since == None) & (previous_until == None):\n",
    "        df, response_json_dict = get_ig_account_insights(ig_user_id, access_token, since=since, until=until, filename=filename)\n",
    "        return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "    elif previous_since == None:\n",
    "        previous_since = since + timedelta (days=1)\n",
    "        print('Previous `since` parameter could not be found; default to since + 1.')\n",
    "    elif previous_until == None:\n",
    "        previous_until = until - timedelta (days=1)\n",
    "        print('Previous `until` parameter could not be found; default to until - 1.')\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'\\nFetching older account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        older_insights_df, older_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=since, until=previous_since)\n",
    "        try:\n",
    "            df = pd.concat([df, older_insights_df])\n",
    "        except:\n",
    "            df = older_insights_df\n",
    "        try:\n",
    "            # Update the keys of *response_json_dict* before merging with older_insights_response_json_dict. That way, final \n",
    "                # response dictionary always has insights from oldest dates first\n",
    "            response_json_dict = dict( \n",
    "                zip([key+len(older_insights_response_json_dict) for key in response_json_dict.keys()], response_json_dict.values())\n",
    "                )\n",
    "            response_json_dict = {**older_insights_response_json_dict, **response_json_dict}\n",
    "        except:\n",
    "            response_json_dict = older_insights_response_json_dict\n",
    "    if (previous_until.date() < until.date()):\n",
    "        print(f'\\nFetching newer account insights from {datetime.strftime(previous_until, \"%Y-%m-%d\")} to {datetime.strftime(until, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=previous_until, until=until)\n",
    "        try:\n",
    "            df = pd.concat([df, new_insights_df])\n",
    "        except:\n",
    "            df = new_insights_df\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "        \n",
    "    if filename:\n",
    "        try:\n",
    "            savepickle(df, filename+'_df', 'sav', csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename3 = 'silvialiftsweights_03-15_2'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to load prior results; making new API calls for entire date range.\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1677657600.0&since=1675065600.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-30 00:00:00\n",
      "Requesting page 2...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2022-12-31 00:00:00\n",
      "Number of days of data: 61\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_2_account_insights_account_insights_df.sav\n",
      "Time completed: 2023-03-15 00:38:44.612196\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_2_account_insights_account_insights.sav\n",
      "Time completed: 2023-03-15 00:38:44.613198\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until=until, filename=filename3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None, \n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since + timedelta(days=1)\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter > since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = df_list \n",
    "    if filename:\n",
    "        filename += '_account_insights'\n",
    "        try:\n",
    "            savepickle(df,filename+'_df','sav',csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 00:49:23.009939\n",
      "Time completed: 2023-03-15 00:49:23.011946\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_2_df.sav\n",
      "Time completed: 2023-03-15 00:49:23.012939\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_2.sav\n",
      "Time completed: 2023-03-15 00:49:23.013939\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    timestamp_column_suffix='end_time', filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    previous_since, previous_until = None, None\n",
    "    if filename:\n",
    "        filename2 = f'{filename}_account_insights'\n",
    "        # df = loadpickle(filename+'_df.sav', csv_path)\n",
    "        # df = df.reset_index(drop=True)\n",
    "        # timestamp_column = df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]\n",
    "        # df = df.sort_values(timestamp_column)\n",
    "        # response_json_dict = loadpickle(filename+'.sav', json_path)\n",
    "        # last_json_page = max(response_json_dict.keys())\n",
    "        # previous_since = datetime.strptime(df.iloc[0][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        # previous_until = datetime.strptime(df.iloc[-1][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    try:\n",
    "        df = loadpickle(filename2+'_df.sav', csv_path)\n",
    "        df = df.reset_index(drop=True)\n",
    "        timestamp_column = df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]\n",
    "        df = df.sort_values(timestamp_column)\n",
    "        response_json_dict = loadpickle(filename2+'.sav', json_path)\n",
    "        last_json_page = max(response_json_dict.keys())\n",
    "        previous_since = datetime.strptime(df.iloc[0][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        previous_until = datetime.strptime(df.iloc[-1][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    except:\n",
    "        print('Unable to load prior results; making new API calls for entire date range.')\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    \n",
    "    if (previous_since == None) & (previous_until == None):\n",
    "        df, response_json_dict = get_ig_account_insights(ig_user_id, access_token, since=since, until=until, filename=filename)\n",
    "        return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "    elif previous_since == None:\n",
    "        previous_since = since + timedelta (days=1)\n",
    "        print('Previous `since` parameter could not be found; default to since + 1.')\n",
    "    elif previous_until == None:\n",
    "        previous_until = until - timedelta (days=1)\n",
    "        print('Previous `until` parameter could not be found; default to until - 1.')\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'\\nFetching older account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        older_insights_df, older_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=since, until=previous_since)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), older_insights_df])\n",
    "        except:\n",
    "            df = older_insights_df\n",
    "        try:\n",
    "            # Update the keys of *response_json_dict* before merging with older_insights_response_json_dict. That way, final \n",
    "                # response dictionary always has insights from oldest dates first\n",
    "            response_json_dict = dict( \n",
    "                zip([key+len(older_insights_response_json_dict) for key in response_json_dict.keys()], response_json_dict.values())\n",
    "                )\n",
    "            response_json_dict = {**older_insights_response_json_dict, **response_json_dict}\n",
    "        except:\n",
    "            response_json_dict = older_insights_response_json_dict\n",
    "    if (previous_until.date() < until.date()):\n",
    "        print(f'\\nFetching newer account insights from {datetime.strftime(previous_until, \"%Y-%m-%d\")} to {datetime.strftime(until, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=previous_until, until=until)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), new_insights_df])\n",
    "        except:\n",
    "            df = new_insights_df\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "        \n",
    "    if filename:\n",
    "        try:\n",
    "            savepickle(df, filename+'_df', 'sav', csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename3 = 'silvialiftsweights_03-15_2'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 00:49:28.663266\n",
      "Time completed: 2023-03-15 00:49:28.666382\n",
      "\n",
      "Fetching older account insights from 2023-01-21 to 2023-02-01\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1675238400.0&since=1674288000.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-21 00:00:00\n",
      "Number of days of data: 12\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_2_df.sav\n",
      "Time completed: 2023-03-15 00:49:28.955256\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_2.sav\n",
      "Time completed: 2023-03-15 00:49:28.955256\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until=until, filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 00:50:38.642267\n",
      "Time completed: 2023-03-15 00:50:38.643274\n",
      "\n",
      "Fetching older account insights from 2023-01-21 to 2023-02-01\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1675238400.0&since=1674288000.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-21 00:00:00\n",
      "Number of days of data: 12\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_2_df.sav\n",
      "Time completed: 2023-03-15 00:50:38.932952\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_2.sav\n",
      "Time completed: 2023-03-15 00:50:38.932952\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until=until, filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "      <td>15</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>196</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "      <td>141</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>45</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "      <td>43</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>169</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>68</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "      <td>51</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>57</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>78</td>\n",
       "      <td>2023-02-06T08:00:00+0000</td>\n",
       "      <td>67</td>\n",
       "      <td>2023-02-06T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>266</td>\n",
       "      <td>2023-02-07T08:00:00+0000</td>\n",
       "      <td>208</td>\n",
       "      <td>2023-02-07T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>77</td>\n",
       "      <td>2023-02-08T08:00:00+0000</td>\n",
       "      <td>52</td>\n",
       "      <td>2023-02-08T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-09T08:00:00+0000</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-09T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-10T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-10T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-11T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-11T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>184</td>\n",
       "      <td>2023-02-12T08:00:00+0000</td>\n",
       "      <td>98</td>\n",
       "      <td>2023-02-12T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>48</td>\n",
       "      <td>2023-02-13T08:00:00+0000</td>\n",
       "      <td>39</td>\n",
       "      <td>2023-02-13T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>200</td>\n",
       "      <td>2023-02-14T08:00:00+0000</td>\n",
       "      <td>131</td>\n",
       "      <td>2023-02-14T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>40</td>\n",
       "      <td>2023-02-15T08:00:00+0000</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-15T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10</td>\n",
       "      <td>2023-02-16T08:00:00+0000</td>\n",
       "      <td>10</td>\n",
       "      <td>2023-02-16T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-17T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-17T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-18T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-18T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>13</td>\n",
       "      <td>2023-02-19T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-19T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-20T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-20T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>111</td>\n",
       "      <td>2023-02-21T08:00:00+0000</td>\n",
       "      <td>96</td>\n",
       "      <td>2023-02-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>19</td>\n",
       "      <td>2023-02-22T08:00:00+0000</td>\n",
       "      <td>16</td>\n",
       "      <td>2023-02-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>31</td>\n",
       "      <td>2023-02-23T08:00:00+0000</td>\n",
       "      <td>7</td>\n",
       "      <td>2023-02-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>32</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "      <td>95</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "      <td>28</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    impressions_value      impressions_end_time  reach_value  \\\n",
       "0                  23  2023-01-21T08:00:00+0000           15   \n",
       "1                   5  2023-01-22T08:00:00+0000            5   \n",
       "2                   3  2023-01-23T08:00:00+0000            3   \n",
       "3                   3  2023-01-24T08:00:00+0000            3   \n",
       "4                   0  2023-01-25T08:00:00+0000            0   \n",
       "5                   2  2023-01-26T08:00:00+0000            2   \n",
       "6                  38  2023-01-27T08:00:00+0000            1   \n",
       "7                  29  2023-01-28T08:00:00+0000            1   \n",
       "8                   5  2023-01-29T08:00:00+0000            4   \n",
       "9                 196  2023-01-30T08:00:00+0000          141   \n",
       "10                 45  2023-01-31T08:00:00+0000           43   \n",
       "11                 14  2023-02-01T08:00:00+0000            9   \n",
       "12                 14  2023-02-01T08:00:00+0000            9   \n",
       "13                  5  2023-02-02T08:00:00+0000            4   \n",
       "14                169  2023-02-03T08:00:00+0000          109   \n",
       "15                 68  2023-02-04T08:00:00+0000           51   \n",
       "16                 57  2023-02-05T08:00:00+0000           29   \n",
       "17                 78  2023-02-06T08:00:00+0000           67   \n",
       "18                266  2023-02-07T08:00:00+0000          208   \n",
       "19                 77  2023-02-08T08:00:00+0000           52   \n",
       "20                 21  2023-02-09T08:00:00+0000           21   \n",
       "21                  3  2023-02-10T08:00:00+0000            3   \n",
       "22                  1  2023-02-11T08:00:00+0000            1   \n",
       "23                184  2023-02-12T08:00:00+0000           98   \n",
       "24                 48  2023-02-13T08:00:00+0000           39   \n",
       "25                200  2023-02-14T08:00:00+0000          131   \n",
       "26                 40  2023-02-15T08:00:00+0000           21   \n",
       "27                 10  2023-02-16T08:00:00+0000           10   \n",
       "28                  2  2023-02-17T08:00:00+0000            2   \n",
       "29                  2  2023-02-18T08:00:00+0000            2   \n",
       "30                 13  2023-02-19T08:00:00+0000            1   \n",
       "31                  4  2023-02-20T08:00:00+0000            2   \n",
       "32                111  2023-02-21T08:00:00+0000           96   \n",
       "33                 19  2023-02-22T08:00:00+0000           16   \n",
       "34                 31  2023-02-23T08:00:00+0000            7   \n",
       "35                  3  2023-02-24T08:00:00+0000            3   \n",
       "36                  1  2023-02-25T08:00:00+0000            1   \n",
       "37                 32  2023-02-26T08:00:00+0000            9   \n",
       "38                109  2023-02-27T08:00:00+0000           95   \n",
       "39                 29  2023-02-28T08:00:00+0000           28   \n",
       "40                289  2023-03-01T08:00:00+0000          170   \n",
       "\n",
       "              reach_end_time  \n",
       "0   2023-01-21T08:00:00+0000  \n",
       "1   2023-01-22T08:00:00+0000  \n",
       "2   2023-01-23T08:00:00+0000  \n",
       "3   2023-01-24T08:00:00+0000  \n",
       "4   2023-01-25T08:00:00+0000  \n",
       "5   2023-01-26T08:00:00+0000  \n",
       "6   2023-01-27T08:00:00+0000  \n",
       "7   2023-01-28T08:00:00+0000  \n",
       "8   2023-01-29T08:00:00+0000  \n",
       "9   2023-01-30T08:00:00+0000  \n",
       "10  2023-01-31T08:00:00+0000  \n",
       "11  2023-02-01T08:00:00+0000  \n",
       "12  2023-02-01T08:00:00+0000  \n",
       "13  2023-02-02T08:00:00+0000  \n",
       "14  2023-02-03T08:00:00+0000  \n",
       "15  2023-02-04T08:00:00+0000  \n",
       "16  2023-02-05T08:00:00+0000  \n",
       "17  2023-02-06T08:00:00+0000  \n",
       "18  2023-02-07T08:00:00+0000  \n",
       "19  2023-02-08T08:00:00+0000  \n",
       "20  2023-02-09T08:00:00+0000  \n",
       "21  2023-02-10T08:00:00+0000  \n",
       "22  2023-02-11T08:00:00+0000  \n",
       "23  2023-02-12T08:00:00+0000  \n",
       "24  2023-02-13T08:00:00+0000  \n",
       "25  2023-02-14T08:00:00+0000  \n",
       "26  2023-02-15T08:00:00+0000  \n",
       "27  2023-02-16T08:00:00+0000  \n",
       "28  2023-02-17T08:00:00+0000  \n",
       "29  2023-02-18T08:00:00+0000  \n",
       "30  2023-02-19T08:00:00+0000  \n",
       "31  2023-02-20T08:00:00+0000  \n",
       "32  2023-02-21T08:00:00+0000  \n",
       "33  2023-02-22T08:00:00+0000  \n",
       "34  2023-02-23T08:00:00+0000  \n",
       "35  2023-02-24T08:00:00+0000  \n",
       "36  2023-02-25T08:00:00+0000  \n",
       "37  2023-02-26T08:00:00+0000  \n",
       "38  2023-02-27T08:00:00+0000  \n",
       "39  2023-02-28T08:00:00+0000  \n",
       "40  2023-03-01T08:00:00+0000  "
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 00:52:44.280815\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>23</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "      <td>15</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>38</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>196</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "      <td>141</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>45</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "      <td>43</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>169</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "      <td>51</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>78</td>\n",
       "      <td>2023-02-06T08:00:00+0000</td>\n",
       "      <td>67</td>\n",
       "      <td>2023-02-06T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>266</td>\n",
       "      <td>2023-02-07T08:00:00+0000</td>\n",
       "      <td>208</td>\n",
       "      <td>2023-02-07T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>77</td>\n",
       "      <td>2023-02-08T08:00:00+0000</td>\n",
       "      <td>52</td>\n",
       "      <td>2023-02-08T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-09T08:00:00+0000</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-09T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-10T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-10T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-11T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-11T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>184</td>\n",
       "      <td>2023-02-12T08:00:00+0000</td>\n",
       "      <td>98</td>\n",
       "      <td>2023-02-12T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>48</td>\n",
       "      <td>2023-02-13T08:00:00+0000</td>\n",
       "      <td>39</td>\n",
       "      <td>2023-02-13T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>200</td>\n",
       "      <td>2023-02-14T08:00:00+0000</td>\n",
       "      <td>131</td>\n",
       "      <td>2023-02-14T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>40</td>\n",
       "      <td>2023-02-15T08:00:00+0000</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-15T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>2023-02-16T08:00:00+0000</td>\n",
       "      <td>10</td>\n",
       "      <td>2023-02-16T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-17T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-17T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-18T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-18T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13</td>\n",
       "      <td>2023-02-19T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-19T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-20T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-20T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>111</td>\n",
       "      <td>2023-02-21T08:00:00+0000</td>\n",
       "      <td>96</td>\n",
       "      <td>2023-02-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19</td>\n",
       "      <td>2023-02-22T08:00:00+0000</td>\n",
       "      <td>16</td>\n",
       "      <td>2023-02-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>31</td>\n",
       "      <td>2023-02-23T08:00:00+0000</td>\n",
       "      <td>7</td>\n",
       "      <td>2023-02-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "      <td>95</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "      <td>28</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    impressions_value      impressions_end_time  reach_value  \\\n",
       "29                 23  2023-01-21T08:00:00+0000           15   \n",
       "30                  5  2023-01-22T08:00:00+0000            5   \n",
       "31                  3  2023-01-23T08:00:00+0000            3   \n",
       "32                  3  2023-01-24T08:00:00+0000            3   \n",
       "33                  0  2023-01-25T08:00:00+0000            0   \n",
       "34                  2  2023-01-26T08:00:00+0000            2   \n",
       "35                 38  2023-01-27T08:00:00+0000            1   \n",
       "36                 29  2023-01-28T08:00:00+0000            1   \n",
       "37                  5  2023-01-29T08:00:00+0000            4   \n",
       "38                196  2023-01-30T08:00:00+0000          141   \n",
       "39                 45  2023-01-31T08:00:00+0000           43   \n",
       "0                  14  2023-02-01T08:00:00+0000            9   \n",
       "40                 14  2023-02-01T08:00:00+0000            9   \n",
       "1                   5  2023-02-02T08:00:00+0000            4   \n",
       "2                 169  2023-02-03T08:00:00+0000          109   \n",
       "3                  68  2023-02-04T08:00:00+0000           51   \n",
       "4                  57  2023-02-05T08:00:00+0000           29   \n",
       "5                  78  2023-02-06T08:00:00+0000           67   \n",
       "6                 266  2023-02-07T08:00:00+0000          208   \n",
       "7                  77  2023-02-08T08:00:00+0000           52   \n",
       "8                  21  2023-02-09T08:00:00+0000           21   \n",
       "9                   3  2023-02-10T08:00:00+0000            3   \n",
       "10                  1  2023-02-11T08:00:00+0000            1   \n",
       "11                184  2023-02-12T08:00:00+0000           98   \n",
       "12                 48  2023-02-13T08:00:00+0000           39   \n",
       "13                200  2023-02-14T08:00:00+0000          131   \n",
       "14                 40  2023-02-15T08:00:00+0000           21   \n",
       "15                 10  2023-02-16T08:00:00+0000           10   \n",
       "16                  2  2023-02-17T08:00:00+0000            2   \n",
       "17                  2  2023-02-18T08:00:00+0000            2   \n",
       "18                 13  2023-02-19T08:00:00+0000            1   \n",
       "19                  4  2023-02-20T08:00:00+0000            2   \n",
       "20                111  2023-02-21T08:00:00+0000           96   \n",
       "21                 19  2023-02-22T08:00:00+0000           16   \n",
       "22                 31  2023-02-23T08:00:00+0000            7   \n",
       "23                  3  2023-02-24T08:00:00+0000            3   \n",
       "24                  1  2023-02-25T08:00:00+0000            1   \n",
       "25                 32  2023-02-26T08:00:00+0000            9   \n",
       "26                109  2023-02-27T08:00:00+0000           95   \n",
       "27                 29  2023-02-28T08:00:00+0000           28   \n",
       "28                289  2023-03-01T08:00:00+0000          170   \n",
       "\n",
       "              reach_end_time  \n",
       "29  2023-01-21T08:00:00+0000  \n",
       "30  2023-01-22T08:00:00+0000  \n",
       "31  2023-01-23T08:00:00+0000  \n",
       "32  2023-01-24T08:00:00+0000  \n",
       "33  2023-01-25T08:00:00+0000  \n",
       "34  2023-01-26T08:00:00+0000  \n",
       "35  2023-01-27T08:00:00+0000  \n",
       "36  2023-01-28T08:00:00+0000  \n",
       "37  2023-01-29T08:00:00+0000  \n",
       "38  2023-01-30T08:00:00+0000  \n",
       "39  2023-01-31T08:00:00+0000  \n",
       "0   2023-02-01T08:00:00+0000  \n",
       "40  2023-02-01T08:00:00+0000  \n",
       "1   2023-02-02T08:00:00+0000  \n",
       "2   2023-02-03T08:00:00+0000  \n",
       "3   2023-02-04T08:00:00+0000  \n",
       "4   2023-02-05T08:00:00+0000  \n",
       "5   2023-02-06T08:00:00+0000  \n",
       "6   2023-02-07T08:00:00+0000  \n",
       "7   2023-02-08T08:00:00+0000  \n",
       "8   2023-02-09T08:00:00+0000  \n",
       "9   2023-02-10T08:00:00+0000  \n",
       "10  2023-02-11T08:00:00+0000  \n",
       "11  2023-02-12T08:00:00+0000  \n",
       "12  2023-02-13T08:00:00+0000  \n",
       "13  2023-02-14T08:00:00+0000  \n",
       "14  2023-02-15T08:00:00+0000  \n",
       "15  2023-02-16T08:00:00+0000  \n",
       "16  2023-02-17T08:00:00+0000  \n",
       "17  2023-02-18T08:00:00+0000  \n",
       "18  2023-02-19T08:00:00+0000  \n",
       "19  2023-02-20T08:00:00+0000  \n",
       "20  2023-02-21T08:00:00+0000  \n",
       "21  2023-02-22T08:00:00+0000  \n",
       "22  2023-02-23T08:00:00+0000  \n",
       "23  2023-02-24T08:00:00+0000  \n",
       "24  2023-02-25T08:00:00+0000  \n",
       "25  2023-02-26T08:00:00+0000  \n",
       "26  2023-02-27T08:00:00+0000  \n",
       "27  2023-02-28T08:00:00+0000  \n",
       "28  2023-03-01T08:00:00+0000  "
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadpickle(\n",
    "    filename3+'_df.sav', \n",
    "    r'C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim'\n",
    "    ).reset_index(drop=True).sort_values('impressions_end_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 00:53:18.486406\n",
      "Time completed: 2023-03-15 00:53:18.488441\n",
      "\n",
      "Fetching older account insights from 2023-01-21 to 2023-02-01\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1675238400.0&since=1674288000.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-21 00:00:00\n",
      "Number of days of data: 12\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_2_df.sav\n",
      "Time completed: 2023-03-15 00:53:18.785551\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_2.sav\n",
      "Time completed: 2023-03-15 00:53:18.786553\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until=until, filename=filename3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None, \n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since + timedelta(days=1)\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter > since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = df_list \n",
    "    if filename:\n",
    "        filename += '_account_insights'\n",
    "        try:\n",
    "            savepickle(df,filename+'_df','sav',csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 01:07:22.300829\n",
      "Time completed: 2023-03-15 01:07:22.303828\n",
      "previous since date: 2023-02-01 08:00:00+00:00\n",
      "previous until date: 2023-03-01 08:00:00+00:00\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_3_account_insights_df.sav\n",
      "Time completed: 2023-03-15 01:07:22.306826\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_3_account_insights.sav\n",
      "Time completed: 2023-03-15 01:07:22.307825\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    timestamp_column_suffix='end_time', filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    previous_since, previous_until = None, None\n",
    "    if filename:\n",
    "        filename2 = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename2+'_df.sav', csv_path)\n",
    "        df = df.reset_index(drop=True)\n",
    "        timestamp_column = df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]\n",
    "        df = df.sort_values(timestamp_column)\n",
    "        response_json_dict = loadpickle(filename2+'.sav', json_path)\n",
    "        previous_since = datetime.strptime(df.iloc[0][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        previous_until = datetime.strptime(df.iloc[-1][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        print('previous since date:', previous_since)\n",
    "        print('previous until date:', previous_until)\n",
    "    except:\n",
    "        print('Unable to load prior results; making new API calls for entire date range.')\n",
    "    \n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    \n",
    "    if (previous_since == None) & (previous_until == None):\n",
    "        df, response_json_dict = get_ig_account_insights(ig_user_id, access_token, since=since, until=until, filename=filename)\n",
    "        return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "    elif previous_since == None:\n",
    "        previous_since = since + timedelta (days=1)\n",
    "        print('Previous `since` parameter could not be found; default to since + 1.')\n",
    "    elif previous_until == None:\n",
    "        previous_until = until - timedelta (days=1)\n",
    "        print('Previous `until` parameter could not be found; default to until - 1.')\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'\\nFetching older account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        older_insights_df, older_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=since, until=previous_since)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), older_insights_df])\n",
    "        except:\n",
    "            df = older_insights_df\n",
    "        try:\n",
    "            # Update the keys of *response_json_dict* before merging with older_insights_response_json_dict. That way, final \n",
    "                # response dictionary always has insights from oldest dates first\n",
    "            response_json_dict = dict( \n",
    "                zip([key+len(older_insights_response_json_dict) for key in response_json_dict.keys()], response_json_dict.values())\n",
    "                )\n",
    "            response_json_dict = {**older_insights_response_json_dict, **response_json_dict}\n",
    "        except:\n",
    "            response_json_dict = older_insights_response_json_dict\n",
    "    if (previous_until.date() < until.date()):\n",
    "        print(f'\\nFetching newer account insights from {datetime.strftime(previous_until, \"%Y-%m-%d\")} to {datetime.strftime(until, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=previous_until, until=until)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), new_insights_df])\n",
    "        except:\n",
    "            df = new_insights_df\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "    \n",
    "        \n",
    "    if filename:\n",
    "        try:\n",
    "            savepickle(df, filename2+'_df', 'sav', csv_path)\n",
    "            savepickle(response_json_dict,filename2,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename3 = 'silvialiftsweights_03-15_3'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 01:04:24.280137\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressions_value</th>\n",
       "      <th>impressions_end_time</th>\n",
       "      <th>reach_value</th>\n",
       "      <th>reach_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>23</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "      <td>15</td>\n",
       "      <td>2023-01-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>38</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-29T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>196</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "      <td>141</td>\n",
       "      <td>2023-01-30T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>45</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "      <td>43</td>\n",
       "      <td>2023-01-31T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-02T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>169</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-03T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "      <td>51</td>\n",
       "      <td>2023-02-04T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-05T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>78</td>\n",
       "      <td>2023-02-06T08:00:00+0000</td>\n",
       "      <td>67</td>\n",
       "      <td>2023-02-06T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>266</td>\n",
       "      <td>2023-02-07T08:00:00+0000</td>\n",
       "      <td>208</td>\n",
       "      <td>2023-02-07T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>77</td>\n",
       "      <td>2023-02-08T08:00:00+0000</td>\n",
       "      <td>52</td>\n",
       "      <td>2023-02-08T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-09T08:00:00+0000</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-09T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-10T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-10T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-11T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-11T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>184</td>\n",
       "      <td>2023-02-12T08:00:00+0000</td>\n",
       "      <td>98</td>\n",
       "      <td>2023-02-12T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>48</td>\n",
       "      <td>2023-02-13T08:00:00+0000</td>\n",
       "      <td>39</td>\n",
       "      <td>2023-02-13T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>200</td>\n",
       "      <td>2023-02-14T08:00:00+0000</td>\n",
       "      <td>131</td>\n",
       "      <td>2023-02-14T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>40</td>\n",
       "      <td>2023-02-15T08:00:00+0000</td>\n",
       "      <td>21</td>\n",
       "      <td>2023-02-15T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>2023-02-16T08:00:00+0000</td>\n",
       "      <td>10</td>\n",
       "      <td>2023-02-16T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-17T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-17T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-18T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-18T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13</td>\n",
       "      <td>2023-02-19T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-19T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-20T08:00:00+0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-20T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>111</td>\n",
       "      <td>2023-02-21T08:00:00+0000</td>\n",
       "      <td>96</td>\n",
       "      <td>2023-02-21T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19</td>\n",
       "      <td>2023-02-22T08:00:00+0000</td>\n",
       "      <td>16</td>\n",
       "      <td>2023-02-22T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>31</td>\n",
       "      <td>2023-02-23T08:00:00+0000</td>\n",
       "      <td>7</td>\n",
       "      <td>2023-02-23T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-24T08:00:00+0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-24T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-25T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-02-26T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>109</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "      <td>95</td>\n",
       "      <td>2023-02-27T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "      <td>28</td>\n",
       "      <td>2023-02-28T08:00:00+0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>289</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "      <td>170</td>\n",
       "      <td>2023-03-01T08:00:00+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    impressions_value      impressions_end_time  reach_value  \\\n",
       "29                 23  2023-01-21T08:00:00+0000           15   \n",
       "30                  5  2023-01-22T08:00:00+0000            5   \n",
       "31                  3  2023-01-23T08:00:00+0000            3   \n",
       "32                  3  2023-01-24T08:00:00+0000            3   \n",
       "33                  0  2023-01-25T08:00:00+0000            0   \n",
       "34                  2  2023-01-26T08:00:00+0000            2   \n",
       "35                 38  2023-01-27T08:00:00+0000            1   \n",
       "36                 29  2023-01-28T08:00:00+0000            1   \n",
       "37                  5  2023-01-29T08:00:00+0000            4   \n",
       "38                196  2023-01-30T08:00:00+0000          141   \n",
       "39                 45  2023-01-31T08:00:00+0000           43   \n",
       "0                  14  2023-02-01T08:00:00+0000            9   \n",
       "40                 14  2023-02-01T08:00:00+0000            9   \n",
       "1                   5  2023-02-02T08:00:00+0000            4   \n",
       "2                 169  2023-02-03T08:00:00+0000          109   \n",
       "3                  68  2023-02-04T08:00:00+0000           51   \n",
       "4                  57  2023-02-05T08:00:00+0000           29   \n",
       "5                  78  2023-02-06T08:00:00+0000           67   \n",
       "6                 266  2023-02-07T08:00:00+0000          208   \n",
       "7                  77  2023-02-08T08:00:00+0000           52   \n",
       "8                  21  2023-02-09T08:00:00+0000           21   \n",
       "9                   3  2023-02-10T08:00:00+0000            3   \n",
       "10                  1  2023-02-11T08:00:00+0000            1   \n",
       "11                184  2023-02-12T08:00:00+0000           98   \n",
       "12                 48  2023-02-13T08:00:00+0000           39   \n",
       "13                200  2023-02-14T08:00:00+0000          131   \n",
       "14                 40  2023-02-15T08:00:00+0000           21   \n",
       "15                 10  2023-02-16T08:00:00+0000           10   \n",
       "16                  2  2023-02-17T08:00:00+0000            2   \n",
       "17                  2  2023-02-18T08:00:00+0000            2   \n",
       "18                 13  2023-02-19T08:00:00+0000            1   \n",
       "19                  4  2023-02-20T08:00:00+0000            2   \n",
       "20                111  2023-02-21T08:00:00+0000           96   \n",
       "21                 19  2023-02-22T08:00:00+0000           16   \n",
       "22                 31  2023-02-23T08:00:00+0000            7   \n",
       "23                  3  2023-02-24T08:00:00+0000            3   \n",
       "24                  1  2023-02-25T08:00:00+0000            1   \n",
       "25                 32  2023-02-26T08:00:00+0000            9   \n",
       "26                109  2023-02-27T08:00:00+0000           95   \n",
       "27                 29  2023-02-28T08:00:00+0000           28   \n",
       "28                289  2023-03-01T08:00:00+0000          170   \n",
       "\n",
       "              reach_end_time  \n",
       "29  2023-01-21T08:00:00+0000  \n",
       "30  2023-01-22T08:00:00+0000  \n",
       "31  2023-01-23T08:00:00+0000  \n",
       "32  2023-01-24T08:00:00+0000  \n",
       "33  2023-01-25T08:00:00+0000  \n",
       "34  2023-01-26T08:00:00+0000  \n",
       "35  2023-01-27T08:00:00+0000  \n",
       "36  2023-01-28T08:00:00+0000  \n",
       "37  2023-01-29T08:00:00+0000  \n",
       "38  2023-01-30T08:00:00+0000  \n",
       "39  2023-01-31T08:00:00+0000  \n",
       "0   2023-02-01T08:00:00+0000  \n",
       "40  2023-02-01T08:00:00+0000  \n",
       "1   2023-02-02T08:00:00+0000  \n",
       "2   2023-02-03T08:00:00+0000  \n",
       "3   2023-02-04T08:00:00+0000  \n",
       "4   2023-02-05T08:00:00+0000  \n",
       "5   2023-02-06T08:00:00+0000  \n",
       "6   2023-02-07T08:00:00+0000  \n",
       "7   2023-02-08T08:00:00+0000  \n",
       "8   2023-02-09T08:00:00+0000  \n",
       "9   2023-02-10T08:00:00+0000  \n",
       "10  2023-02-11T08:00:00+0000  \n",
       "11  2023-02-12T08:00:00+0000  \n",
       "12  2023-02-13T08:00:00+0000  \n",
       "13  2023-02-14T08:00:00+0000  \n",
       "14  2023-02-15T08:00:00+0000  \n",
       "15  2023-02-16T08:00:00+0000  \n",
       "16  2023-02-17T08:00:00+0000  \n",
       "17  2023-02-18T08:00:00+0000  \n",
       "18  2023-02-19T08:00:00+0000  \n",
       "19  2023-02-20T08:00:00+0000  \n",
       "20  2023-02-21T08:00:00+0000  \n",
       "21  2023-02-22T08:00:00+0000  \n",
       "22  2023-02-23T08:00:00+0000  \n",
       "23  2023-02-24T08:00:00+0000  \n",
       "24  2023-02-25T08:00:00+0000  \n",
       "25  2023-02-26T08:00:00+0000  \n",
       "26  2023-02-27T08:00:00+0000  \n",
       "27  2023-02-28T08:00:00+0000  \n",
       "28  2023-03-01T08:00:00+0000  "
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadpickle(\n",
    "    filename3+'_df.sav', \n",
    "    r'C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim'\n",
    "    ).reset_index(drop=True).sort_values('impressions_end_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 01:07:49.181412\n",
      "Time completed: 2023-03-15 01:07:49.185402\n",
      "previous since date: 2023-02-01 08:00:00+00:00\n",
      "previous until date: 2023-03-01 08:00:00+00:00\n",
      "\n",
      "Fetching older account insights from 2023-01-21 to 2023-02-01\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1675238400.0&since=1674288000.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-21 00:00:00\n",
      "Number of days of data: 12\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_3_account_insights_df.sav\n",
      "Time completed: 2023-03-15 01:07:49.560935\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_3_account_insights.sav\n",
      "Time completed: 2023-03-15 01:07:49.560935\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until=until, filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 01:08:01.075077\n",
      "Time completed: 2023-03-15 01:08:01.078759\n",
      "previous since date: 2023-01-21 08:00:00+00:00\n",
      "previous until date: 2023-03-01 08:00:00+00:00\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_3_account_insights_df.sav\n",
      "Time completed: 2023-03-15 01:08:01.080785\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_3_account_insights.sav\n",
      "Time completed: 2023-03-15 01:08:01.081790\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until=until, filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 01:10:06.833010\n",
      "Time completed: 2023-03-15 01:10:06.835046\n",
      "previous since date: 2023-01-21 08:00:00+00:00\n",
      "previous until date: 2023-03-03 08:00:00+00:00\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_3_account_insights_df.sav\n",
      "Time completed: 2023-03-15 01:10:06.837010\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_3_account_insights.sav\n",
      "Time completed: 2023-03-15 01:10:06.838015\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until='2023-03-03', filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 01:08:49.410722\n",
      "Time completed: 2023-03-15 01:08:49.428698\n",
      "previous since date: 2023-01-21 08:00:00+00:00\n",
      "previous until date: 2023-03-03 08:00:00+00:00\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_3_account_insights_df.sav\n",
      "Time completed: 2023-03-15 01:08:49.431701\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_3_account_insights.sav\n",
      "Time completed: 2023-03-15 01:08:49.432697\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until='2023-03-03', filename=filename3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None, \n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since + timedelta(days=1)\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter > since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = df_list \n",
    "    if filename:\n",
    "        filename += '_account_insights'\n",
    "        try:\n",
    "            savepickle(df,filename+'_df','sav',csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 01:12:54.934493\n",
      "Time completed: 2023-03-15 01:12:54.942491\n",
      "previous since date: 2023-01-21 08:00:00+00:00\n",
      "previous until date: 2023-03-03 08:00:00+00:00\n",
      "\n",
      "Loading previous saved results; no new API calls required.\n",
      "\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_3_account_insights_df.sav\n",
      "Time completed: 2023-03-15 01:12:54.944500\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_3_account_insights.sav\n",
      "Time completed: 2023-03-15 01:12:54.946496\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    timestamp_column_suffix='end_time', filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-15 1:22\n",
    "    Get the daily impressions and reach a given Instagram account. \n",
    "    Load any results that were previously saved (pull new data if no previously saved results available).\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - timestamp_column_suffix (str): Suffix of the timestamp columns. default is 'end_time'. \n",
    "            Required to parse out the date range of the previously saved outputs.\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    previous_since, previous_until = None, None\n",
    "    if filename:\n",
    "        filename2 = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename2+'_df.sav', csv_path)\n",
    "        df = df.reset_index(drop=True)\n",
    "        timestamp_column = df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]\n",
    "        df = df.sort_values(timestamp_column)\n",
    "        response_json_dict = loadpickle(filename2+'.sav', json_path)\n",
    "        previous_since = datetime.strptime(df.iloc[0][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        previous_until = datetime.strptime(df.iloc[-1][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        print('previous since date:', previous_since)\n",
    "        print('previous until date:', previous_until)\n",
    "    except:\n",
    "        print('Unable to load prior results; making new API calls for entire date range.')\n",
    "    \n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    \n",
    "    if (previous_since == None) & (previous_until == None):\n",
    "        df, response_json_dict = get_ig_account_insights(ig_user_id, access_token, since=since, until=until, filename=filename)\n",
    "        return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "    elif previous_since == None:\n",
    "        previous_since = since + timedelta (days=1)\n",
    "        print('Previous `since` parameter could not be found; default to since + 1.')\n",
    "    elif previous_until == None:\n",
    "        previous_until = until - timedelta (days=1)\n",
    "        print('Previous `until` parameter could not be found; default to until - 1.')\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'\\nFetching older account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        older_insights_df, older_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=since, until=previous_since)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), older_insights_df])\n",
    "        except:\n",
    "            df = older_insights_df\n",
    "        try:\n",
    "            # Update the keys of *response_json_dict* before merging with older_insights_response_json_dict. That way, final \n",
    "                # response dictionary always has insights from oldest dates first\n",
    "            response_json_dict = dict( \n",
    "                zip([key+len(older_insights_response_json_dict) for key in response_json_dict.keys()], response_json_dict.values())\n",
    "                )\n",
    "            response_json_dict = {**older_insights_response_json_dict, **response_json_dict}\n",
    "        except:\n",
    "            response_json_dict = older_insights_response_json_dict\n",
    "    if (previous_until.date() < until.date()):\n",
    "        print(f'\\nFetching newer account insights from {datetime.strftime(previous_until, \"%Y-%m-%d\")} to {datetime.strftime(until, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=previous_until, until=until)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), new_insights_df])\n",
    "        except:\n",
    "            df = new_insights_df\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "    \n",
    "    if (previous_until.date() >= until.date()) & (previous_since.date() <= since.date()):\n",
    "        print('\\nLoading previous saved results; no new API calls required.\\n')\n",
    "        \n",
    "    if filename:\n",
    "        try:\n",
    "            savepickle(df, filename2+'_df', 'sav', csv_path)\n",
    "            savepickle(response_json_dict,filename2,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename3 = 'silvialiftsweights_03-15_3'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 01:13:04.954982\n",
      "Time completed: 2023-03-15 01:13:04.958981\n",
      "previous since date: 2023-01-21 08:00:00+00:00\n",
      "previous until date: 2023-03-03 08:00:00+00:00\n",
      "\n",
      "Loading previous saved results; no new API calls required.\n",
      "\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_3_account_insights_df.sav\n",
      "Time completed: 2023-03-15 01:13:04.961982\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_3_account_insights.sav\n",
      "Time completed: 2023-03-15 01:13:04.962982\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until=until, filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 01:13:07.867489\n",
      "Time completed: 2023-03-15 01:13:07.869484\n",
      "previous since date: 2023-01-21 08:00:00+00:00\n",
      "previous until date: 2023-03-03 08:00:00+00:00\n",
      "\n",
      "Loading previous saved results; no new API calls required.\n",
      "\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_3_account_insights_df.sav\n",
      "Time completed: 2023-03-15 01:13:07.871483\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_3_account_insights.sav\n",
      "Time completed: 2023-03-15 01:13:07.872483\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until=until, filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 01:13:11.001121\n",
      "Time completed: 2023-03-15 01:13:11.004121\n",
      "previous since date: 2023-01-21 08:00:00+00:00\n",
      "previous until date: 2023-03-03 08:00:00+00:00\n",
      "\n",
      "Loading previous saved results; no new API calls required.\n",
      "\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_3_account_insights_df.sav\n",
      "Time completed: 2023-03-15 01:13:11.006128\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_3_account_insights.sav\n",
      "Time completed: 2023-03-15 01:13:11.007130\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until='2023-03-03', filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 01:13:13.134806\n",
      "Time completed: 2023-03-15 01:13:13.136804\n",
      "previous since date: 2023-01-21 08:00:00+00:00\n",
      "previous until date: 2023-03-03 08:00:00+00:00\n",
      "\n",
      "Loading previous saved results; no new API calls required.\n",
      "\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_3_account_insights_df.sav\n",
      "Time completed: 2023-03-15 01:13:13.138798\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_3_account_insights.sav\n",
      "Time completed: 2023-03-15 01:13:13.139798\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until='2023-03-03', filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
