{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "[]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import json_normalize  \n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\custom_python\")\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\src\")\n",
    "from silvhua import *\n",
    "from datetime import datetime\n",
    "from plotly.subplots import make_subplots\n",
    "import requests\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.json\") as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "ig_user_id = credentials['am_ig_user_id']\n",
    "access_token = credentials['am_ig_access_token']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `update_ig_account_insights`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://graph.facebook.com/v15.0/17841401256046961/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1677657600.0&since=1675238400.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "Requesting page 2...\n",
      "\tResponse status code:  200\n",
      "Number of days of data: 57\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_account_insights.sav\n",
      "Time completed: 2023-03-09 12:49:03.466033\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_account_insights.sav\n",
      "Time completed: 2023-03-09 12:49:03.468023\n"
     ]
    }
   ],
   "source": [
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = loadpickle(filename, csv_path)\n",
    "        previous_since = df.sort_values('timestamp')\n",
    "    except:\n",
    "        pass\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter >= since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = response\n",
    "    if filename:\n",
    "        filename = f'{filename}_account_insights'\n",
    "        try:\n",
    "            savepickle(df, filename, 'sav', csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename = 'silvialiftsweights'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-09 13:57:31.405137\n",
      "Time completed: 2023-03-09 13:57:31.407139\n",
      "previous until: 2023-03-01 08:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    if filename:\n",
    "        filename = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename+'.sav', csv_path)\n",
    "        response_json_dict = loadpickle(filename+'.sav', json_path)\n",
    "        last_json_page = max(response_json_dict.keys())\n",
    "        previous_since = response_json_dict[last_json_page]['data'][0]['values'][0]['end_time']\n",
    "        previous_since = datetime.strptime(previous_since, \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        \n",
    "        previous_until = response_json_dict[1]['data'][0]['values'][-1]['end_time']\n",
    "        previous_until = datetime.strptime(previous_until, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        print('previous until:', previous_until)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if (previous_since.date() > since.date()) | (previous_until.date() < until.date()):\n",
    "        print('Fetching new account insights')\n",
    "    # if since_parameter:\n",
    "    #     url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    # else:\n",
    "    #     url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "    #     since_parameter = since\n",
    "\n",
    "    # url = url_without_token+'&access_token='+access_token\n",
    "    # print(url_without_token)\n",
    "    \n",
    "    # response_json_dict = dict()\n",
    "    # df_list = []\n",
    "    # earliest_end_time = None\n",
    "    # page = 1\n",
    "    # while (since_parameter >= since):\n",
    "    #     response = requests.get(url)\n",
    "    #     print(f'Requesting page {page}...')\n",
    "    #     print('\\tResponse status code: ',response.status_code)\n",
    "    #     response_json_dict[page] = response.json()\n",
    "    #     if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "    #         print(response_json_dict[page]['error'])\n",
    "    #         break\n",
    "    #     try:\n",
    "    #         df_list.append(\n",
    "    #             pd.concat([\n",
    "    #             json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "    #             json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "    #             ], axis=1)\n",
    "    #         )\n",
    "    #     except:\n",
    "    #         print('No data in request response for page', page)\n",
    "    #     earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "    #     since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "\n",
    "    #     try:\n",
    "    #         next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "    #         if next_endpoint+access_token != url:\n",
    "    #             url = next_endpoint\n",
    "    #         else:\n",
    "    #             print('end')\n",
    "    #             break\n",
    "    #     except: \n",
    "    #         break\n",
    "    #     page +=1\n",
    "    # try:\n",
    "    #     df = pd.concat(df_list)\n",
    "    #     df = df.reset_index(drop=True)\n",
    "    #     print('Number of days of data:',len(df))\n",
    "    # except:\n",
    "    #     df = response\n",
    "    # if filename:\n",
    "    #     try:\n",
    "    #         savepickle(df, filename+'_df', 'sav', csv_path)\n",
    "    #         savepickle(response_json_dict,filename,'sav',json_path)\n",
    "    #     except:\n",
    "    #         print('Unable to save outputs')\n",
    "    return df, response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename = 'silvialiftsweights'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename)\n",
    "# response_json_dict\n",
    "\n",
    "# data = update_ig_account_insights(\n",
    "#     ig_user_id, access_token, since=since, until=until, filename=filename)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response_json_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([3, 4])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip([key+len(response_json_dict) for key in response_json_dict.keys()], response_json_dict.values())).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{**response_json_dict, **dict(zip([key+len(response_json_dict) for key in response_json_dict.keys()], response_json_dict.values()))}.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter >= since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = response\n",
    "    if filename:\n",
    "        try:\n",
    "            save_csv(df,filename,csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-09 14:37:34.968390\n",
      "Time completed: 2023-03-09 14:37:34.968390\n",
      "Fetching new account insights from 2023-01-01 to 2023-01-04\n",
      "https://graph.facebook.com/v15.0/17841401256046961/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1672819200.0&since=1672560000.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "Requesting page 2...\n",
      "\tResponse status code:  200\n",
      "Number of days of data: 7\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    if filename:\n",
    "        filename = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename+'.sav', csv_path)\n",
    "        response_json_dict = loadpickle(filename+'.sav', json_path)\n",
    "        last_json_page = max(response_json_dict.keys())\n",
    "        previous_since = response_json_dict[last_json_page]['data'][0]['values'][0]['end_time']\n",
    "        previous_since = datetime.strptime(previous_since, \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        \n",
    "        previous_until = response_json_dict[1]['data'][0]['values'][-1]['end_time']\n",
    "        previous_until = datetime.strptime(previous_until, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    except:\n",
    "        pass\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'Fetching new account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=since, until=previous_since)\n",
    "        df = pd.concat([df, new_insights_df])\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "        \n",
    "    # if (previous_until.date() < until.date()):\n",
    "\n",
    "    # if since_parameter:\n",
    "    #     url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    # else:\n",
    "    #     url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "    #     since_parameter = since\n",
    "\n",
    "    # url = url_without_token+'&access_token='+access_token\n",
    "    # print(url_without_token)\n",
    "    \n",
    "    # response_json_dict = dict()\n",
    "    # df_list = []\n",
    "    # earliest_end_time = None\n",
    "    # page = 1\n",
    "    # while (since_parameter >= since):\n",
    "    #     response = requests.get(url)\n",
    "    #     print(f'Requesting page {page}...')\n",
    "    #     print('\\tResponse status code: ',response.status_code)\n",
    "    #     response_json_dict[page] = response.json()\n",
    "    #     if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "    #         print(response_json_dict[page]['error'])\n",
    "    #         break\n",
    "    #     try:\n",
    "    #         df_list.append(\n",
    "    #             pd.concat([\n",
    "    #             json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "    #             json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "    #             ], axis=1)\n",
    "    #         )\n",
    "    #     except:\n",
    "    #         print('No data in request response for page', page)\n",
    "    #     earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "    #     since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "\n",
    "    #     try:\n",
    "    #         next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "    #         if next_endpoint+access_token != url:\n",
    "    #             url = next_endpoint\n",
    "    #         else:\n",
    "    #             print('end')\n",
    "    #             break\n",
    "    #     except: \n",
    "    #         break\n",
    "    #     page +=1\n",
    "    # try:\n",
    "    #     df = pd.concat(df_list)\n",
    "    #     df = df.reset_index(drop=True)\n",
    "    #     print('Number of days of data:',len(df))\n",
    "    # except:\n",
    "    #     df = response\n",
    "    # if filename:\n",
    "    #     try:\n",
    "    #         savepickle(df, filename+'_df', 'sav', csv_path)\n",
    "    #         savepickle(response_json_dict,filename,'sav',json_path)\n",
    "    #     except:\n",
    "    #         print('Unable to save outputs')\n",
    "    return df, response_json_dict, new_insights_response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename = 'silvialiftsweights'\n",
    "\n",
    "data, response_json_dict, new_insights_response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-01', until=until, filename=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: {'data': [{'name': 'impressions',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 13185, 'end_time': '2023-01-01T08:00:00+0000'},\n",
       "     {'value': 12254, 'end_time': '2023-01-02T08:00:00+0000'},\n",
       "     {'value': 30510, 'end_time': '2023-01-03T08:00:00+0000'},\n",
       "     {'value': 19138, 'end_time': '2023-01-04T08:00:00+0000'}],\n",
       "    'title': 'Impressions',\n",
       "    'description': \"The total number of times that the business account's media objects have been viewed.\",\n",
       "    'id': '17841401256046961/insights/impressions/day'},\n",
       "   {'name': 'reach',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 5845, 'end_time': '2023-01-01T08:00:00+0000'},\n",
       "     {'value': 8573, 'end_time': '2023-01-02T08:00:00+0000'},\n",
       "     {'value': 8682, 'end_time': '2023-01-03T08:00:00+0000'},\n",
       "     {'value': 7431, 'end_time': '2023-01-04T08:00:00+0000'}],\n",
       "    'title': 'Reach',\n",
       "    'description': \"The total number of times that the business account's media objects have been uniquely viewed.\",\n",
       "    'id': '17841401256046961/insights/reach/day'}],\n",
       "  'paging': {'previous': 'https://graph.facebook.com/v15.0/17841401256046961/insights?access_token=EAAKEofVXnvEBADnuuiqY6NI5hZAmAmF3YBdopZBeW6SVeByNFcVAxIaiO2YZAwJVm9HjXQuUatQIGcfwTSVrVHSp8ArVOWRrkMf4TU4BgeQ4YJVPTwZAKcqtuSk8v9xR1VAXbiaCQd0w1QZBiP1khITGHgNSrOi0gTPZBqWhyN7GQN10N1KZB0g&since=1672300799&until=1672559999&metric=impressions%2Creach&metric_type=time_series&period=day',\n",
       "   'next': 'https://graph.facebook.com/v15.0/17841401256046961/insights?access_token=EAAKEofVXnvEBADnuuiqY6NI5hZAmAmF3YBdopZBeW6SVeByNFcVAxIaiO2YZAwJVm9HjXQuUatQIGcfwTSVrVHSp8ArVOWRrkMf4TU4BgeQ4YJVPTwZAKcqtuSk8v9xR1VAXbiaCQd0w1QZBiP1khITGHgNSrOi0gTPZBqWhyN7GQN10N1KZB0g&since=1672819201&until=1673078401&metric=impressions%2Creach&metric_type=time_series&period=day'}},\n",
       " 4: {'data': [{'name': 'impressions',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 13340, 'end_time': '2022-12-29T08:00:00+0000'},\n",
       "     {'value': 12182, 'end_time': '2022-12-30T08:00:00+0000'},\n",
       "     {'value': 18559, 'end_time': '2022-12-31T08:00:00+0000'}],\n",
       "    'title': 'Impressions',\n",
       "    'description': \"The total number of times that the business account's media objects have been viewed.\",\n",
       "    'id': '17841401256046961/insights/impressions/day'},\n",
       "   {'name': 'reach',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 5085, 'end_time': '2022-12-29T08:00:00+0000'},\n",
       "     {'value': 5158, 'end_time': '2022-12-30T08:00:00+0000'},\n",
       "     {'value': 5936, 'end_time': '2022-12-31T08:00:00+0000'}],\n",
       "    'title': 'Reach',\n",
       "    'description': \"The total number of times that the business account's media objects have been uniquely viewed.\",\n",
       "    'id': '17841401256046961/insights/reach/day'}],\n",
       "  'paging': {'previous': 'https://graph.facebook.com/v15.0/17841401256046961/insights?access_token=EAAKEofVXnvEBADnuuiqY6NI5hZAmAmF3YBdopZBeW6SVeByNFcVAxIaiO2YZAwJVm9HjXQuUatQIGcfwTSVrVHSp8ArVOWRrkMf4TU4BgeQ4YJVPTwZAKcqtuSk8v9xR1VAXbiaCQd0w1QZBiP1khITGHgNSrOi0gTPZBqWhyN7GQN10N1KZB0g&since=1672041598&until=1672300798&metric=impressions%2Creach&metric_type=time_series&period=day',\n",
       "   'next': 'https://graph.facebook.com/v15.0/17841401256046961/insights?access_token=EAAKEofVXnvEBADnuuiqY6NI5hZAmAmF3YBdopZBeW6SVeByNFcVAxIaiO2YZAwJVm9HjXQuUatQIGcfwTSVrVHSp8ArVOWRrkMf4TU4BgeQ4YJVPTwZAKcqtuSk8v9xR1VAXbiaCQd0w1QZBiP1khITGHgNSrOi0gTPZBqWhyN7GQN10N1KZB0g&since=1672560000&until=1672819200&metric=impressions%2Creach&metric_type=time_series&period=day'}}}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_insights_response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'name': 'impressions',\n",
       "   'period': 'day',\n",
       "   'values': [{'value': 13340, 'end_time': '2022-12-29T08:00:00+0000'},\n",
       "    {'value': 12182, 'end_time': '2022-12-30T08:00:00+0000'},\n",
       "    {'value': 18559, 'end_time': '2022-12-31T08:00:00+0000'}],\n",
       "   'title': 'Impressions',\n",
       "   'description': \"The total number of times that the business account's media objects have been viewed.\",\n",
       "   'id': '17841401256046961/insights/impressions/day'},\n",
       "  {'name': 'reach',\n",
       "   'period': 'day',\n",
       "   'values': [{'value': 5085, 'end_time': '2022-12-29T08:00:00+0000'},\n",
       "    {'value': 5158, 'end_time': '2022-12-30T08:00:00+0000'},\n",
       "    {'value': 5936, 'end_time': '2022-12-31T08:00:00+0000'}],\n",
       "   'title': 'Reach',\n",
       "   'description': \"The total number of times that the business account's media objects have been uniquely viewed.\",\n",
       "   'id': '17841401256046961/insights/reach/day'}],\n",
       " 'paging': {'previous': 'https://graph.facebook.com/v15.0/17841401256046961/insights?access_token=EAAKEofVXnvEBADnuuiqY6NI5hZAmAmF3YBdopZBeW6SVeByNFcVAxIaiO2YZAwJVm9HjXQuUatQIGcfwTSVrVHSp8ArVOWRrkMf4TU4BgeQ4YJVPTwZAKcqtuSk8v9xR1VAXbiaCQd0w1QZBiP1khITGHgNSrOi0gTPZBqWhyN7GQN10N1KZB0g&since=1672041598&until=1672300798&metric=impressions%2Creach&metric_type=time_series&period=day',\n",
       "  'next': 'https://graph.facebook.com/v15.0/17841401256046961/insights?access_token=EAAKEofVXnvEBADnuuiqY6NI5hZAmAmF3YBdopZBeW6SVeByNFcVAxIaiO2YZAwJVm9HjXQuUatQIGcfwTSVrVHSp8ArVOWRrkMf4TU4BgeQ4YJVPTwZAKcqtuSk8v9xR1VAXbiaCQd0w1QZBiP1khITGHgNSrOi0gTPZBqWhyN7GQN10N1KZB0g&since=1672560000&until=1672819200&metric=impressions%2Creach&metric_type=time_series&period=day'}}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_json_dict[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'name': 'impressions',\n",
       "   'period': 'day',\n",
       "   'values': [{'value': 12382, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "    {'value': 11649, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "    {'value': 13558, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "    {'value': 8538, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "    {'value': 12797, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "    {'value': 9225, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "    {'value': 14321, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "    {'value': 6518, 'end_time': '2023-02-08T08:00:00+0000'},\n",
       "    {'value': 8438, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "    {'value': 7437, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "    {'value': 11863, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "    {'value': 7469, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "    {'value': 7706, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "    {'value': 7705, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "    {'value': 10405, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "    {'value': 6788, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "    {'value': 8684, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "    {'value': 4733, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "    {'value': 5789, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "    {'value': 4204, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "    {'value': 4699, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "    {'value': 8618, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "    {'value': 10062, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "    {'value': 3919, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "    {'value': 4780, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "    {'value': 11691, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "    {'value': 6595, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "    {'value': 5795, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "    {'value': 4917, 'end_time': '2023-03-01T08:00:00+0000'}],\n",
       "   'title': 'Impressions',\n",
       "   'description': \"The total number of times that the business account's media objects have been viewed.\",\n",
       "   'id': '17841401256046961/insights/impressions/day'},\n",
       "  {'name': 'reach',\n",
       "   'period': 'day',\n",
       "   'values': [{'value': 4913, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "    {'value': 5221, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "    {'value': 5155, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "    {'value': 4588, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "    {'value': 5669, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "    {'value': 5264, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "    {'value': 4283, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "    {'value': 1078, 'end_time': '2023-02-08T08:00:00+0000'},\n",
       "    {'value': 1322, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "    {'value': 1545, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "    {'value': 1840, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "    {'value': 1428, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "    {'value': 1428, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "    {'value': 1145, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "    {'value': 1374, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "    {'value': 1347, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "    {'value': 1679, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "    {'value': 1194, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "    {'value': 1615, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "    {'value': 1485, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "    {'value': 829, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "    {'value': 1842, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "    {'value': 1470, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "    {'value': 964, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "    {'value': 1284, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "    {'value': 1661, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "    {'value': 2003, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "    {'value': 1639, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "    {'value': 1650, 'end_time': '2023-03-01T08:00:00+0000'}],\n",
       "   'title': 'Reach',\n",
       "   'description': \"The total number of times that the business account's media objects have been uniquely viewed.\",\n",
       "   'id': '17841401256046961/insights/reach/day'}],\n",
       " 'paging': {'previous': 'https://graph.facebook.com/v15.0/17841401256046961/insights?access_token=EAAKEofVXnvEBADnuuiqY6NI5hZAmAmF3YBdopZBeW6SVeByNFcVAxIaiO2YZAwJVm9HjXQuUatQIGcfwTSVrVHSp8ArVOWRrkMf4TU4BgeQ4YJVPTwZAKcqtuSk8v9xR1VAXbiaCQd0w1QZBiP1khITGHgNSrOi0gTPZBqWhyN7GQN10N1KZB0g&since=1672819199&until=1675238399&metric=impressions%2Creach&metric_type=time_series&period=day',\n",
       "  'next': 'https://graph.facebook.com/v15.0/17841401256046961/insights?access_token=EAAKEofVXnvEBADnuuiqY6NI5hZAmAmF3YBdopZBeW6SVeByNFcVAxIaiO2YZAwJVm9HjXQuUatQIGcfwTSVrVHSp8ArVOWRrkMf4TU4BgeQ4YJVPTwZAKcqtuSk8v9xR1VAXbiaCQd0w1QZBiP1khITGHgNSrOi0gTPZBqWhyN7GQN10N1KZB0g&since=1677657601&until=1680076801&metric=impressions%2Creach&metric_type=time_series&period=day'}}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_json_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
