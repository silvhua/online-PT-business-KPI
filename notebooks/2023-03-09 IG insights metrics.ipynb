{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "[]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import json_normalize  \n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\custom_python\")\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\src\")\n",
    "from silvhua import *\n",
    "from datetime import datetime\n",
    "from plotly.subplots import make_subplots\n",
    "import requests\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"credentials.json\") as f:\n",
    "#     credentials = json.load(f)\n",
    "\n",
    "# ig_user_id = credentials['am_ig_user_id']\n",
    "# access_token = credentials['am_ig_access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.json\") as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "ig_user_id = credentials['ig_user_id']\n",
    "access_token = credentials['access_token']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `update_ig_account_insights`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://graph.facebook.com/v15.0/17841401256046961/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1677657600.0&since=1675238400.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "Requesting page 2...\n",
      "\tResponse status code:  200\n",
      "Number of days of data: 57\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_account_insights.sav\n",
      "Time completed: 2023-03-09 12:49:03.466033\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_account_insights.sav\n",
      "Time completed: 2023-03-09 12:49:03.468023\n"
     ]
    }
   ],
   "source": [
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = loadpickle(filename, csv_path)\n",
    "        previous_since = df.sort_values('timestamp')\n",
    "    except:\n",
    "        pass\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter >= since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = response\n",
    "    if filename:\n",
    "        filename = f'{filename}_account_insights'\n",
    "        try:\n",
    "            savepickle(df, filename, 'sav', csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename = 'silvialiftsweights'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-09 13:57:31.405137\n",
      "Time completed: 2023-03-09 13:57:31.407139\n",
      "previous until: 2023-03-01 08:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    if filename:\n",
    "        filename = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename+'.sav', csv_path)\n",
    "        response_json_dict = loadpickle(filename+'.sav', json_path)\n",
    "        last_json_page = max(response_json_dict.keys())\n",
    "        previous_since = response_json_dict[last_json_page]['data'][0]['values'][0]['end_time']\n",
    "        previous_since = datetime.strptime(previous_since, \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        \n",
    "        previous_until = response_json_dict[1]['data'][0]['values'][-1]['end_time']\n",
    "        previous_until = datetime.strptime(previous_until, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        print('previous until:', previous_until)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if (previous_since.date() > since.date()) | (previous_until.date() < until.date()):\n",
    "        print('Fetching new account insights')\n",
    "    # if since_parameter:\n",
    "    #     url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    # else:\n",
    "    #     url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "    #     since_parameter = since\n",
    "\n",
    "    # url = url_without_token+'&access_token='+access_token\n",
    "    # print(url_without_token)\n",
    "    \n",
    "    # response_json_dict = dict()\n",
    "    # df_list = []\n",
    "    # earliest_end_time = None\n",
    "    # page = 1\n",
    "    # while (since_parameter >= since):\n",
    "    #     response = requests.get(url)\n",
    "    #     print(f'Requesting page {page}...')\n",
    "    #     print('\\tResponse status code: ',response.status_code)\n",
    "    #     response_json_dict[page] = response.json()\n",
    "    #     if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "    #         print(response_json_dict[page]['error'])\n",
    "    #         break\n",
    "    #     try:\n",
    "    #         df_list.append(\n",
    "    #             pd.concat([\n",
    "    #             json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "    #             json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "    #             ], axis=1)\n",
    "    #         )\n",
    "    #     except:\n",
    "    #         print('No data in request response for page', page)\n",
    "    #     earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "    #     since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "\n",
    "    #     try:\n",
    "    #         next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "    #         if next_endpoint+access_token != url:\n",
    "    #             url = next_endpoint\n",
    "    #         else:\n",
    "    #             print('end')\n",
    "    #             break\n",
    "    #     except: \n",
    "    #         break\n",
    "    #     page +=1\n",
    "    # try:\n",
    "    #     df = pd.concat(df_list)\n",
    "    #     df = df.reset_index(drop=True)\n",
    "    #     print('Number of days of data:',len(df))\n",
    "    # except:\n",
    "    #     df = response\n",
    "    # if filename:\n",
    "    #     try:\n",
    "    #         savepickle(df, filename+'_df', 'sav', csv_path)\n",
    "    #         savepickle(response_json_dict,filename,'sav',json_path)\n",
    "    #     except:\n",
    "    #         print('Unable to save outputs')\n",
    "    return df, response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename = 'silvialiftsweights'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename)\n",
    "# response_json_dict\n",
    "\n",
    "# data = update_ig_account_insights(\n",
    "#     ig_user_id, access_token, since=since, until=until, filename=filename)\n",
    "# data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None, \n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since + timedelta(days=1)\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter > since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = df_list \n",
    "    if filename:\n",
    "        filename += '_account_insights'\n",
    "        try:\n",
    "            savepickle(df,filename+'_df','sav',csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 01:12:54.934493\n",
      "Time completed: 2023-03-15 01:12:54.942491\n",
      "previous since date: 2023-01-21 08:00:00+00:00\n",
      "previous until date: 2023-03-03 08:00:00+00:00\n",
      "\n",
      "Loading previous saved results; no new API calls required.\n",
      "\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_3_account_insights_df.sav\n",
      "Time completed: 2023-03-15 01:12:54.944500\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_3_account_insights.sav\n",
      "Time completed: 2023-03-15 01:12:54.946496\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    timestamp_column_suffix='end_time', filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-15 1:22\n",
    "    Get the daily impressions and reach a given Instagram account. \n",
    "    Load any results that were previously saved (pull new data if no previously saved results available).\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - timestamp_column_suffix (str): Suffix of the timestamp columns. default is 'end_time'. \n",
    "            Required to parse out the date range of the previously saved outputs.\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    previous_since, previous_until = None, None\n",
    "    if filename:\n",
    "        filename2 = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename2+'_df.sav', csv_path)\n",
    "        df = df.reset_index(drop=True)\n",
    "        timestamp_column = df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]\n",
    "        df = df.sort_values(timestamp_column)\n",
    "        response_json_dict = loadpickle(filename2+'.sav', json_path)\n",
    "        previous_since = datetime.strptime(df.iloc[0][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        previous_until = datetime.strptime(df.iloc[-1][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        print('previous since date:', previous_since)\n",
    "        print('previous until date:', previous_until)\n",
    "    except:\n",
    "        print('Unable to load prior results; making new API calls for entire date range.')\n",
    "    \n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    \n",
    "    if (previous_since == None) & (previous_until == None):\n",
    "        df, response_json_dict = get_ig_account_insights(ig_user_id, access_token, since=since, until=until, filename=filename)\n",
    "        return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "    elif previous_since == None:\n",
    "        previous_since = since + timedelta (days=1)\n",
    "        print('Previous `since` parameter could not be found; default to since + 1.')\n",
    "    elif previous_until == None:\n",
    "        previous_until = until - timedelta (days=1)\n",
    "        print('Previous `until` parameter could not be found; default to until - 1.')\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'\\nFetching older account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        older_insights_df, older_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=since, until=previous_since)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), older_insights_df])\n",
    "        except:\n",
    "            df = older_insights_df\n",
    "        try:\n",
    "            # Update the keys of *response_json_dict* before merging with older_insights_response_json_dict. That way, final \n",
    "                # response dictionary always has insights from oldest dates first\n",
    "            response_json_dict = dict( \n",
    "                zip([key+len(older_insights_response_json_dict) for key in response_json_dict.keys()], response_json_dict.values())\n",
    "                )\n",
    "            response_json_dict = {**older_insights_response_json_dict, **response_json_dict}\n",
    "        except:\n",
    "            response_json_dict = older_insights_response_json_dict\n",
    "    if (previous_until.date() < until.date()):\n",
    "        print(f'\\nFetching newer account insights from {datetime.strftime(previous_until, \"%Y-%m-%d\")} to {datetime.strftime(until, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=previous_until, until=until)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), new_insights_df])\n",
    "        except:\n",
    "            df = new_insights_df\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "    \n",
    "    if (previous_until.date() >= until.date()) & (previous_since.date() <= since.date()):\n",
    "        print('\\nLoading previous saved results; no new API calls required.\\n')\n",
    "        \n",
    "    if filename:\n",
    "        try:\n",
    "            savepickle(df, filename2+'_df', 'sav', csv_path)\n",
    "            savepickle(response_json_dict,filename2,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename3 = 'silvialiftsweights_03-15_3'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 01:13:04.954982\n",
      "Time completed: 2023-03-15 01:13:04.958981\n",
      "previous since date: 2023-01-21 08:00:00+00:00\n",
      "previous until date: 2023-03-03 08:00:00+00:00\n",
      "\n",
      "Loading previous saved results; no new API calls required.\n",
      "\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_3_account_insights_df.sav\n",
      "Time completed: 2023-03-15 01:13:04.961982\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_3_account_insights.sav\n",
      "Time completed: 2023-03-15 01:13:04.962982\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until=until, filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 01:13:07.867489\n",
      "Time completed: 2023-03-15 01:13:07.869484\n",
      "previous since date: 2023-01-21 08:00:00+00:00\n",
      "previous until date: 2023-03-03 08:00:00+00:00\n",
      "\n",
      "Loading previous saved results; no new API calls required.\n",
      "\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_3_account_insights_df.sav\n",
      "Time completed: 2023-03-15 01:13:07.871483\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_3_account_insights.sav\n",
      "Time completed: 2023-03-15 01:13:07.872483\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until=until, filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 01:13:11.001121\n",
      "Time completed: 2023-03-15 01:13:11.004121\n",
      "previous since date: 2023-01-21 08:00:00+00:00\n",
      "previous until date: 2023-03-03 08:00:00+00:00\n",
      "\n",
      "Loading previous saved results; no new API calls required.\n",
      "\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_3_account_insights_df.sav\n",
      "Time completed: 2023-03-15 01:13:11.006128\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_3_account_insights.sav\n",
      "Time completed: 2023-03-15 01:13:11.007130\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until='2023-03-03', filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 01:13:13.134806\n",
      "Time completed: 2023-03-15 01:13:13.136804\n",
      "previous since date: 2023-01-21 08:00:00+00:00\n",
      "previous until date: 2023-03-03 08:00:00+00:00\n",
      "\n",
      "Loading previous saved results; no new API calls required.\n",
      "\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_3_account_insights_df.sav\n",
      "Time completed: 2023-03-15 01:13:13.138798\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_3_account_insights.sav\n",
      "Time completed: 2023-03-15 01:13:13.139798\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until='2023-03-03', filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
