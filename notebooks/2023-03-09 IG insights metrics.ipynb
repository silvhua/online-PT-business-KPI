{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "[]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import json_normalize  \n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\custom_python\")\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\src\")\n",
    "from silvhua import *\n",
    "from datetime import datetime\n",
    "from plotly.subplots import make_subplots\n",
    "import requests\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"credentials.json\") as f:\n",
    "#     credentials = json.load(f)\n",
    "\n",
    "# ig_user_id = credentials['am_ig_user_id']\n",
    "# access_token = credentials['am_ig_access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.json\") as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "ig_user_id = credentials['ig_user_id']\n",
    "access_token = credentials['access_token']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `update_ig_account_insights`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://graph.facebook.com/v15.0/17841401256046961/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1677657600.0&since=1675238400.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "Requesting page 2...\n",
      "\tResponse status code:  200\n",
      "Number of days of data: 57\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_account_insights.sav\n",
      "Time completed: 2023-03-09 12:49:03.466033\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_account_insights.sav\n",
      "Time completed: 2023-03-09 12:49:03.468023\n"
     ]
    }
   ],
   "source": [
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = loadpickle(filename, csv_path)\n",
    "        previous_since = df.sort_values('timestamp')\n",
    "    except:\n",
    "        pass\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter >= since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = response\n",
    "    if filename:\n",
    "        filename = f'{filename}_account_insights'\n",
    "        try:\n",
    "            savepickle(df, filename, 'sav', csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename = 'silvialiftsweights'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-09 13:57:31.405137\n",
      "Time completed: 2023-03-09 13:57:31.407139\n",
      "previous until: 2023-03-01 08:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    SH 2023-03-09 12:37\n",
    "    Get the daily impressions and reach a given Instagram account. Load results that were previously saved.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    if filename:\n",
    "        filename = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename+'.sav', csv_path)\n",
    "        response_json_dict = loadpickle(filename+'.sav', json_path)\n",
    "        last_json_page = max(response_json_dict.keys())\n",
    "        previous_since = response_json_dict[last_json_page]['data'][0]['values'][0]['end_time']\n",
    "        previous_since = datetime.strptime(previous_since, \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        \n",
    "        previous_until = response_json_dict[1]['data'][0]['values'][-1]['end_time']\n",
    "        previous_until = datetime.strptime(previous_until, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        print('previous until:', previous_until)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if (previous_since.date() > since.date()) | (previous_until.date() < until.date()):\n",
    "        print('Fetching new account insights')\n",
    "    # if since_parameter:\n",
    "    #     url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    # else:\n",
    "    #     url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "    #     since_parameter = since\n",
    "\n",
    "    # url = url_without_token+'&access_token='+access_token\n",
    "    # print(url_without_token)\n",
    "    \n",
    "    # response_json_dict = dict()\n",
    "    # df_list = []\n",
    "    # earliest_end_time = None\n",
    "    # page = 1\n",
    "    # while (since_parameter >= since):\n",
    "    #     response = requests.get(url)\n",
    "    #     print(f'Requesting page {page}...')\n",
    "    #     print('\\tResponse status code: ',response.status_code)\n",
    "    #     response_json_dict[page] = response.json()\n",
    "    #     if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "    #         print(response_json_dict[page]['error'])\n",
    "    #         break\n",
    "    #     try:\n",
    "    #         df_list.append(\n",
    "    #             pd.concat([\n",
    "    #             json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "    #             json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "    #             ], axis=1)\n",
    "    #         )\n",
    "    #     except:\n",
    "    #         print('No data in request response for page', page)\n",
    "    #     earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "    #     since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "\n",
    "    #     try:\n",
    "    #         next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "    #         if next_endpoint+access_token != url:\n",
    "    #             url = next_endpoint\n",
    "    #         else:\n",
    "    #             print('end')\n",
    "    #             break\n",
    "    #     except: \n",
    "    #         break\n",
    "    #     page +=1\n",
    "    # try:\n",
    "    #     df = pd.concat(df_list)\n",
    "    #     df = df.reset_index(drop=True)\n",
    "    #     print('Number of days of data:',len(df))\n",
    "    # except:\n",
    "    #     df = response\n",
    "    # if filename:\n",
    "    #     try:\n",
    "    #         savepickle(df, filename+'_df', 'sav', csv_path)\n",
    "    #         savepickle(response_json_dict,filename,'sav',json_path)\n",
    "    #     except:\n",
    "    #         print('Unable to save outputs')\n",
    "    return df, response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename = 'silvialiftsweights'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename)\n",
    "# response_json_dict\n",
    "\n",
    "# data = update_ig_account_insights(\n",
    "#     ig_user_id, access_token, since=since, until=until, filename=filename)\n",
    "# data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None, \n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since + timedelta(days=1)\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter > since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = df_list \n",
    "    if filename:\n",
    "        filename += '_account_insights'\n",
    "        try:\n",
    "            savepickle(df,filename+'_df','sav',csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 01:12:54.934493\n",
      "Time completed: 2023-03-15 01:12:54.942491\n",
      "previous since date: 2023-01-21 08:00:00+00:00\n",
      "previous until date: 2023-03-03 08:00:00+00:00\n",
      "\n",
      "Loading previous saved results; no new API calls required.\n",
      "\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_3_account_insights_df.sav\n",
      "Time completed: 2023-03-15 01:12:54.944500\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_3_account_insights.sav\n",
      "Time completed: 2023-03-15 01:12:54.946496\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    timestamp_column_suffix='end_time', filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-15 1:22\n",
    "    Get the daily impressions and reach a given Instagram account. \n",
    "    Load any results that were previously saved (pull new data if no previously saved results available).\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - timestamp_column_suffix (str): Suffix of the timestamp columns. default is 'end_time'. \n",
    "            Required to parse out the date range of the previously saved outputs.\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    previous_since, previous_until = None, None\n",
    "    if filename:\n",
    "        filename2 = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename2+'_df.sav', csv_path)\n",
    "        df = df.reset_index(drop=True)\n",
    "        timestamp_column = df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]\n",
    "        df = df.sort_values(timestamp_column)\n",
    "        response_json_dict = loadpickle(filename2+'.sav', json_path)\n",
    "        previous_since = datetime.strptime(df.iloc[0][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        previous_until = datetime.strptime(df.iloc[-1][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        print('previous since date:', previous_since)\n",
    "        print('previous until date:', previous_until)\n",
    "    except:\n",
    "        print('Unable to load prior results; making new API calls for entire date range.')\n",
    "    \n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    \n",
    "    if (previous_since == None) & (previous_until == None):\n",
    "        df, response_json_dict = get_ig_account_insights(ig_user_id, access_token, since=since, until=until, filename=filename)\n",
    "        return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "    elif previous_since == None:\n",
    "        previous_since = since + timedelta (days=1)\n",
    "        print('Previous `since` parameter could not be found; default to since + 1.')\n",
    "    elif previous_until == None:\n",
    "        previous_until = until - timedelta (days=1)\n",
    "        print('Previous `until` parameter could not be found; default to until - 1.')\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'\\nFetching older account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        older_insights_df, older_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=since, until=previous_since)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), older_insights_df])\n",
    "        except:\n",
    "            df = older_insights_df\n",
    "        try:\n",
    "            # Update the keys of *response_json_dict* before merging with older_insights_response_json_dict. That way, final \n",
    "                # response dictionary always has insights from oldest dates first\n",
    "            response_json_dict = dict( \n",
    "                zip([key+len(older_insights_response_json_dict) for key in response_json_dict.keys()], response_json_dict.values())\n",
    "                )\n",
    "            response_json_dict = {**older_insights_response_json_dict, **response_json_dict}\n",
    "        except:\n",
    "            response_json_dict = older_insights_response_json_dict\n",
    "    if (previous_until.date() < until.date()):\n",
    "        print(f'\\nFetching newer account insights from {datetime.strftime(previous_until, \"%Y-%m-%d\")} to {datetime.strftime(until, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=previous_until, until=until)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), new_insights_df])\n",
    "        except:\n",
    "            df = new_insights_df\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "    \n",
    "    if (previous_until.date() >= until.date()) & (previous_since.date() <= since.date()):\n",
    "        print('\\nLoading previous saved results; no new API calls required.\\n')\n",
    "        \n",
    "    if filename:\n",
    "        try:\n",
    "            savepickle(df, filename2+'_df', 'sav', csv_path)\n",
    "            savepickle(response_json_dict,filename2,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename3 = 'silvialiftsweights_03-15_3'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 01:13:04.954982\n",
      "Time completed: 2023-03-15 01:13:04.958981\n",
      "previous since date: 2023-01-21 08:00:00+00:00\n",
      "previous until date: 2023-03-03 08:00:00+00:00\n",
      "\n",
      "Loading previous saved results; no new API calls required.\n",
      "\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_3_account_insights_df.sav\n",
      "Time completed: 2023-03-15 01:13:04.961982\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_3_account_insights.sav\n",
      "Time completed: 2023-03-15 01:13:04.962982\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until=until, filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 01:13:07.867489\n",
      "Time completed: 2023-03-15 01:13:07.869484\n",
      "previous since date: 2023-01-21 08:00:00+00:00\n",
      "previous until date: 2023-03-03 08:00:00+00:00\n",
      "\n",
      "Loading previous saved results; no new API calls required.\n",
      "\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_3_account_insights_df.sav\n",
      "Time completed: 2023-03-15 01:13:07.871483\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_3_account_insights.sav\n",
      "Time completed: 2023-03-15 01:13:07.872483\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until=until, filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 01:13:11.001121\n",
      "Time completed: 2023-03-15 01:13:11.004121\n",
      "previous since date: 2023-01-21 08:00:00+00:00\n",
      "previous until date: 2023-03-03 08:00:00+00:00\n",
      "\n",
      "Loading previous saved results; no new API calls required.\n",
      "\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_3_account_insights_df.sav\n",
      "Time completed: 2023-03-15 01:13:11.006128\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_3_account_insights.sav\n",
      "Time completed: 2023-03-15 01:13:11.007130\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until='2023-03-03', filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-15 01:13:13.134806\n",
      "Time completed: 2023-03-15 01:13:13.136804\n",
      "previous since date: 2023-01-21 08:00:00+00:00\n",
      "previous until date: 2023-03-03 08:00:00+00:00\n",
      "\n",
      "Loading previous saved results; no new API calls required.\n",
      "\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-15_3_account_insights_df.sav\n",
      "Time completed: 2023-03-15 01:13:13.138798\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/raw/silvialiftsweights_03-15_3_account_insights.sav\n",
      "Time completed: 2023-03-15 01:13:13.139798\n"
     ]
    }
   ],
   "source": [
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since='2023-01-21', until='2023-03-03', filename=filename3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 17\n",
    "remove API key from JSON response before saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None, \n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since + timedelta(days=1)\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter > since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "    except:\n",
    "        df = df_list \n",
    "    if filename:\n",
    "        filename += '_account_insights'\n",
    "        try:\n",
    "            savepickle(df,filename+'_df','sav',csv_path)\n",
    "            savepickle(response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-27 22:18:02.297712\n",
      "Unable to load prior results; making new API calls for entire date range.\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1677657600.0&since=1675238400.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-02-01 00:00:00\n",
      "Number of days of data: 29\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-27_account_insights_df.sav\n",
      "Time completed: 2023-03-27 22:18:02.835910\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/API_response/silvialiftsweights_03-27_account_insights.sav\n",
      "Time completed: 2023-03-27 22:18:02.837902\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    timestamp_column_suffix='end_time', filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\API_response',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-15 1:22\n",
    "    Get the daily impressions and reach a given Instagram account. \n",
    "    Load any results that were previously saved (pull new data if no previously saved results available).\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - timestamp_column_suffix (str): Suffix of the timestamp columns. default is 'end_time'. \n",
    "            Required to parse out the date range of the previously saved outputs.\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    previous_since, previous_until = None, None\n",
    "    if filename:\n",
    "        filename2 = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename2+'_df.sav', csv_path)\n",
    "        df = df.reset_index(drop=True)\n",
    "        timestamp_column = df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]\n",
    "        df = df.sort_values(timestamp_column)\n",
    "        response_json_dict = loadpickle(filename2+'.sav', json_path)\n",
    "        previous_since = datetime.strptime(df.iloc[0][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        previous_until = datetime.strptime(df.iloc[-1][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        print('previous since date:', previous_since)\n",
    "        print('previous until date:', previous_until)\n",
    "    except:\n",
    "        print('Unable to load prior results; making new API calls for entire date range.')\n",
    "    \n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    \n",
    "    if (previous_since == None) & (previous_until == None):\n",
    "        df, response_json_dict = get_ig_account_insights(\n",
    "            ig_user_id, access_token, since=since, until=until, filename=filename,\n",
    "            json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\API_response')\n",
    "        return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "    elif previous_since == None:\n",
    "        previous_since = since + timedelta (days=1)\n",
    "        print('Previous `since` parameter could not be found; default to since + 1.')\n",
    "    elif previous_until == None:\n",
    "        previous_until = until - timedelta (days=1)\n",
    "        print('Previous `until` parameter could not be found; default to until - 1.')\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'\\nFetching older account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        older_insights_df, older_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=since, until=previous_since)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), older_insights_df])\n",
    "        except:\n",
    "            df = older_insights_df\n",
    "        try:\n",
    "            # Update the keys of *response_json_dict* before merging with older_insights_response_json_dict. That way, final \n",
    "                # response dictionary always has insights from oldest dates first\n",
    "            response_json_dict = dict( \n",
    "                zip([key+len(older_insights_response_json_dict) for key in response_json_dict.keys()], response_json_dict.values())\n",
    "                )\n",
    "            response_json_dict = {**older_insights_response_json_dict, **response_json_dict}\n",
    "        except:\n",
    "            response_json_dict = older_insights_response_json_dict\n",
    "    if (previous_until.date() < until.date()):\n",
    "        print(f'\\nFetching newer account insights from {datetime.strftime(previous_until, \"%Y-%m-%d\")} to {datetime.strftime(until, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=previous_until, until=until)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), new_insights_df])\n",
    "        except:\n",
    "            df = new_insights_df\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "        # remove items containing API key as this will invalidate access tokens if published to Github\n",
    "        response_json_dict = [response_json_dict[i]['data'] for i in response_json_dict] \n",
    "    \n",
    "    if (previous_until.date() >= until.date()) & (previous_since.date() <= since.date()):\n",
    "        print('\\nLoading previous saved results; no new API calls required.\\n')\n",
    "        \n",
    "    if filename:\n",
    "        try:\n",
    "            savepickle(df, filename2+'_df', 'sav', csv_path)\n",
    "            savepickle(response_json_dict,filename2,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename3 = 'silvialiftsweights_03-27'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'impressions',\n",
       "  'period': 'day',\n",
       "  'values': [{'value': 14, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "   {'value': 5, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "   {'value': 169, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "   {'value': 68, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "   {'value': 57, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "   {'value': 78, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "   {'value': 266, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "   {'value': 77, 'end_time': '2023-02-08T08:00:00+0000'},\n",
       "   {'value': 21, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "   {'value': 3, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "   {'value': 1, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "   {'value': 184, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "   {'value': 48, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "   {'value': 200, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "   {'value': 40, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "   {'value': 10, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "   {'value': 2, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "   {'value': 2, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "   {'value': 13, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "   {'value': 4, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "   {'value': 111, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "   {'value': 19, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "   {'value': 31, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "   {'value': 3, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "   {'value': 1, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "   {'value': 32, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "   {'value': 109, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "   {'value': 29, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "   {'value': 289, 'end_time': '2023-03-01T08:00:00+0000'}],\n",
       "  'title': 'Impressions',\n",
       "  'description': \"Total number of times the Business Account's media objects have been viewed\",\n",
       "  'id': '17841403231458630/insights/impressions/day'},\n",
       " {'name': 'reach',\n",
       "  'period': 'day',\n",
       "  'values': [{'value': 9, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "   {'value': 4, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "   {'value': 109, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "   {'value': 51, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "   {'value': 29, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "   {'value': 67, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "   {'value': 208, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "   {'value': 52, 'end_time': '2023-02-08T08:00:00+0000'},\n",
       "   {'value': 21, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "   {'value': 3, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "   {'value': 1, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "   {'value': 98, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "   {'value': 39, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "   {'value': 131, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "   {'value': 21, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "   {'value': 10, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "   {'value': 2, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "   {'value': 2, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "   {'value': 1, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "   {'value': 2, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "   {'value': 96, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "   {'value': 16, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "   {'value': 7, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "   {'value': 3, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "   {'value': 1, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "   {'value': 9, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "   {'value': 95, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "   {'value': 28, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "   {'value': 170, 'end_time': '2023-03-01T08:00:00+0000'}],\n",
       "  'title': 'Reach',\n",
       "  'description': \"Total number of times the Business Account's media objects have been uniquely viewed\",\n",
       "  'id': '17841403231458630/insights/reach/day'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_json_dict[1]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'data': [{'name': 'impressions',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 14, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "     {'value': 169, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "     {'value': 68, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "     {'value': 57, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "     {'value': 78, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "     {'value': 266, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "     {'value': 77, 'end_time': '2023-02-08T08:00:00+0000'},\n",
       "     {'value': 21, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "     {'value': 184, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "     {'value': 48, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "     {'value': 200, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "     {'value': 40, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "     {'value': 10, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "     {'value': 13, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "     {'value': 111, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "     {'value': 19, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "     {'value': 31, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "     {'value': 32, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "     {'value': 109, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "     {'value': 29, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "     {'value': 289, 'end_time': '2023-03-01T08:00:00+0000'}],\n",
       "    'title': 'Impressions',\n",
       "    'description': \"Total number of times the Business Account's media objects have been viewed\",\n",
       "    'id': '17841403231458630/insights/impressions/day'},\n",
       "   {'name': 'reach',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 9, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "     {'value': 109, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "     {'value': 51, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "     {'value': 29, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "     {'value': 67, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "     {'value': 208, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "     {'value': 52, 'end_time': '2023-02-08T08:00:00+0000'},\n",
       "     {'value': 21, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "     {'value': 98, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "     {'value': 39, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "     {'value': 131, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "     {'value': 21, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "     {'value': 10, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "     {'value': 96, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "     {'value': 16, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "     {'value': 7, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "     {'value': 9, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "     {'value': 95, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "     {'value': 28, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "     {'value': 170, 'end_time': '2023-03-01T08:00:00+0000'}],\n",
       "    'title': 'Reach',\n",
       "    'description': \"Total number of times the Business Account's media objects have been uniquely viewed\",\n",
       "    'id': '17841403231458630/insights/reach/day'}]}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_response_json_dict = dict()\n",
    "for page, response in response_json_dict.items():\n",
    "    new_response_json_dict[page] = {'data': response['data']}\n",
    "new_response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadpickle('silvialiftsweights_03-27_account_insights.sav',\n",
    "     r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\API_response')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None, \n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since + timedelta(days=1)\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter > since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "        new_response_json_dict = dict()\n",
    "        for page, response in response_json_dict.items():\n",
    "            new_response_json_dict[page] = {'data': response['data']}\n",
    "    except:\n",
    "        df = df_list \n",
    "    if filename:\n",
    "        filename += '_account_insights'\n",
    "        try:\n",
    "            savepickle(df,filename+'_df','sav',csv_path)\n",
    "            savepickle(new_response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to load prior results; making new API calls for entire date range.\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1677657600.0&since=1675238400.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-02-01 00:00:00\n",
      "Number of days of data: 29\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-27_2230_account_insights_df.sav\n",
      "Time completed: 2023-03-27 22:30:16.381550\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/API_response/silvialiftsweights_03-27_2230_account_insights.sav\n",
      "Time completed: 2023-03-27 22:30:16.383563\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    timestamp_column_suffix='end_time', filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\API_response',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-15 1:22\n",
    "    Get the daily impressions and reach a given Instagram account. \n",
    "    Load any results that were previously saved (pull new data if no previously saved results available).\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - timestamp_column_suffix (str): Suffix of the timestamp columns. default is 'end_time'. \n",
    "            Required to parse out the date range of the previously saved outputs.\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    previous_since, previous_until = None, None\n",
    "    if filename:\n",
    "        filename2 = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename2+'_df.sav', csv_path)\n",
    "        df = df.reset_index(drop=True)\n",
    "        timestamp_column = df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]\n",
    "        df = df.sort_values(timestamp_column)\n",
    "        response_json_dict = loadpickle(filename2+'.sav', json_path)\n",
    "        previous_since = datetime.strptime(df.iloc[0][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        previous_until = datetime.strptime(df.iloc[-1][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        print('previous since date:', previous_since)\n",
    "        print('previous until date:', previous_until)\n",
    "    except:\n",
    "        print('Unable to load prior results; making new API calls for entire date range.')\n",
    "    \n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    \n",
    "    if (previous_since == None) & (previous_until == None):\n",
    "        df, response_json_dict = get_ig_account_insights(\n",
    "            ig_user_id, access_token, since=since, until=until, filename=filename,\n",
    "            json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\API_response')\n",
    "        return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "    elif previous_since == None:\n",
    "        previous_since = since + timedelta (days=1)\n",
    "        print('Previous `since` parameter could not be found; default to since + 1.')\n",
    "    elif previous_until == None:\n",
    "        previous_until = until - timedelta (days=1)\n",
    "        print('Previous `until` parameter could not be found; default to until - 1.')\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'\\nFetching older account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        older_insights_df, older_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=since, until=previous_since)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), older_insights_df])\n",
    "        except:\n",
    "            df = older_insights_df\n",
    "        try:\n",
    "            # Update the keys of *response_json_dict* before merging with older_insights_response_json_dict. That way, final \n",
    "                # response dictionary always has insights from oldest dates first\n",
    "            response_json_dict = dict( \n",
    "                zip([key+len(older_insights_response_json_dict) for key in response_json_dict.keys()], response_json_dict.values())\n",
    "                )\n",
    "            response_json_dict = {**older_insights_response_json_dict, **response_json_dict}\n",
    "        except:\n",
    "            response_json_dict = older_insights_response_json_dict\n",
    "    if (previous_until.date() < until.date()):\n",
    "        print(f'\\nFetching newer account insights from {datetime.strftime(previous_until, \"%Y-%m-%d\")} to {datetime.strftime(until, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=previous_until, until=until)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), new_insights_df])\n",
    "        except:\n",
    "            df = new_insights_df\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "        # remove items containing API key as this will invalidate access tokens if published to Github\n",
    "        response_json_dict = [response_json_dict[i]['data'] for i in response_json_dict] \n",
    "    \n",
    "    if (previous_until.date() >= until.date()) & (previous_since.date() <= since.date()):\n",
    "        print('\\nLoading previous saved results; no new API calls required.\\n')\n",
    "        \n",
    "    if filename:\n",
    "        try:\n",
    "            savepickle(df, filename2+'_df', 'sav', csv_path)\n",
    "            savepickle(response_json_dict,filename2,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "\n",
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename3 = 'silvialiftsweights_03-27_2230'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-27 22:34:07.984695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: {'data': [{'name': 'impressions',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 14, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "     {'value': 169, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "     {'value': 68, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "     {'value': 57, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "     {'value': 78, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "     {'value': 266, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "     {'value': 77, 'end_time': '2023-02-08T08:00:00+0000'},\n",
       "     {'value': 21, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "     {'value': 184, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "     {'value': 48, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "     {'value': 200, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "     {'value': 40, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "     {'value': 10, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "     {'value': 13, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "     {'value': 111, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "     {'value': 19, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "     {'value': 31, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "     {'value': 32, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "     {'value': 109, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "     {'value': 29, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "     {'value': 289, 'end_time': '2023-03-01T08:00:00+0000'}],\n",
       "    'title': 'Impressions',\n",
       "    'description': \"Total number of times the Business Account's media objects have been viewed\",\n",
       "    'id': '17841403231458630/insights/impressions/day'},\n",
       "   {'name': 'reach',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 9, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "     {'value': 109, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "     {'value': 51, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "     {'value': 29, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "     {'value': 67, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "     {'value': 208, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "     {'value': 52, 'end_time': '2023-02-08T08:00:00+0000'},\n",
       "     {'value': 21, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "     {'value': 98, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "     {'value': 39, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "     {'value': 131, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "     {'value': 21, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "     {'value': 10, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "     {'value': 96, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "     {'value': 16, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "     {'value': 7, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "     {'value': 9, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "     {'value': 95, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "     {'value': 28, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "     {'value': 170, 'end_time': '2023-03-01T08:00:00+0000'}],\n",
       "    'title': 'Reach',\n",
       "    'description': \"Total number of times the Business Account's media objects have been uniquely viewed\",\n",
       "    'id': '17841403231458630/insights/reach/day'}]}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename3 = 'silvialiftsweights_03-27_2230_account_insights.sav'\n",
    "json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\API_response'\n",
    "loadpickle(filename3, json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-27 22:45:48.556766\n",
      "Time completed: 2023-03-27 22:45:48.637298\n",
      "previous since date: 2023-02-01 08:00:00+00:00\n",
      "previous until date: 2023-03-01 08:00:00+00:00\n",
      "\n",
      "Loading previous saved results; no new API calls required.\n",
      "\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-27_2230_account_insights_df.sav\n",
      "Time completed: 2023-03-27 22:45:48.680339\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/API_response/silvialiftsweights_03-27_2230_account_insights.sav\n",
      "Time completed: 2023-03-27 22:45:48.682298\n"
     ]
    }
   ],
   "source": [
    "since = '2023-02-01'\n",
    "until = '2023-03-01'\n",
    "filename3 = 'silvialiftsweights_03-27_2230'\n",
    "\n",
    "data2, response_json_dict2 = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-27 22:46:54.955980\n",
      "Time completed: 2023-03-27 22:46:54.958976\n",
      "previous since date: 2023-02-01 08:00:00+00:00\n",
      "previous until date: 2023-03-01 08:00:00+00:00\n",
      "\n",
      "Fetching newer account insights from 2023-03-01 to 2023-03-11\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1678521600.0&since=1677657600.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-03-01 00:00:00\n",
      "Number of days of data: 11\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-27_2230_account_insights_df.sav\n",
      "Time completed: 2023-03-27 22:46:55.601658\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/API_response/silvialiftsweights_03-27_2230_account_insights.sav\n",
      "Time completed: 2023-03-27 22:46:55.602648\n"
     ]
    }
   ],
   "source": [
    "since = '2023-02-01'\n",
    "until = '2023-03-11'\n",
    "filename3 = 'silvialiftsweights_03-27_2230'\n",
    "\n",
    "data3, response_json_dict3 = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iteration 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None, \n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since + timedelta(days=1)\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter > since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "        new_response_json_dict = dict()\n",
    "        for page, response in response_json_dict.items():\n",
    "            new_response_json_dict[page] = {'data': response['data']}\n",
    "    except:\n",
    "        df = df_list \n",
    "    if filename:\n",
    "        filename += '_account_insights'\n",
    "        try:\n",
    "            savepickle(df,filename+'_df','sav',csv_path)\n",
    "            savepickle(new_response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, new_response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to load prior results; making new API calls for entire date range.\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1678521600.0&since=1675929600.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-02-09 00:00:00\n",
      "Requesting page 2...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-10 00:00:00\n",
      "Number of days of data: 61\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-27_2300_account_insights_df.sav\n",
      "Time completed: 2023-03-27 23:01:14.204535\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/API_response/silvialiftsweights_03-27_2300_account_insights.sav\n",
      "Time completed: 2023-03-27 23:01:14.205534\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    timestamp_column_suffix='end_time', filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\API_response',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-15 1:22\n",
    "    Get the daily impressions and reach a given Instagram account. \n",
    "    Load any results that were previously saved (pull new data if no previously saved results available).\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - timestamp_column_suffix (str): Suffix of the timestamp columns. default is 'end_time'. \n",
    "            Required to parse out the date range of the previously saved outputs.\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    previous_since, previous_until = None, None\n",
    "    if filename:\n",
    "        filename2 = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename2+'_df.sav', csv_path)\n",
    "        df = df.reset_index(drop=True)\n",
    "        timestamp_column = df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]\n",
    "        df = df.sort_values(timestamp_column)\n",
    "        response_json_dict = loadpickle(filename2+'.sav', json_path)\n",
    "        previous_since = datetime.strptime(df.iloc[0][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        previous_until = datetime.strptime(df.iloc[-1][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        print('previous since date:', previous_since)\n",
    "        print('previous until date:', previous_until)\n",
    "    except:\n",
    "        print('Unable to load prior results; making new API calls for entire date range.')\n",
    "    \n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    \n",
    "    if (previous_since == None) & (previous_until == None):\n",
    "        df, response_json_dict = get_ig_account_insights(\n",
    "            ig_user_id, access_token, since=since, until=until, filename=filename,\n",
    "            json_path=json_path)\n",
    "        return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "    elif previous_since == None:\n",
    "        previous_since = since + timedelta (days=1)\n",
    "        print('Previous `since` parameter could not be found; default to since + 1.')\n",
    "    elif previous_until == None:\n",
    "        previous_until = until - timedelta (days=1)\n",
    "        print('Previous `until` parameter could not be found; default to until - 1.')\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'\\nFetching older account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        older_insights_df, older_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=since, until=previous_since)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), older_insights_df])\n",
    "        except:\n",
    "            df = older_insights_df\n",
    "        try:\n",
    "            # Update the keys of *response_json_dict* before merging with older_insights_response_json_dict. That way, final \n",
    "                # response dictionary always has insights from oldest dates first\n",
    "            response_json_dict = dict( \n",
    "                zip([key+len(older_insights_response_json_dict) for key in response_json_dict.keys()], response_json_dict.values())\n",
    "                )\n",
    "            response_json_dict = {**older_insights_response_json_dict, **response_json_dict}\n",
    "        except:\n",
    "            response_json_dict = older_insights_response_json_dict\n",
    "    if (previous_until.date() < until.date()):\n",
    "        print(f'\\nFetching newer account insights from {datetime.strftime(previous_until, \"%Y-%m-%d\")} to {datetime.strftime(until, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            since=previous_until, until=until)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), new_insights_df])\n",
    "        except:\n",
    "            df = new_insights_df\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "        # remove items containing API key as this will invalidate access tokens if published to Github\n",
    "        response_json_dict = [response_json_dict[i]['data'] for i in response_json_dict] \n",
    "    \n",
    "    if (previous_until.date() >= until.date()) & (previous_since.date() <= since.date()):\n",
    "        print('\\nLoading previous saved results; no new API calls required.\\n')\n",
    "        \n",
    "    if filename:\n",
    "        try:\n",
    "            savepickle(df, filename2+'_df', 'sav', csv_path)\n",
    "            savepickle(response_json_dict,filename2,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "\n",
    "since = '2023-01-21'\n",
    "until = '2023-03-11'\n",
    "filename3 = 'silvialiftsweights_03-27_2300'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'data': [{'name': 'impressions',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 21, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "     {'value': 184, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "     {'value': 48, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "     {'value': 200, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "     {'value': 40, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "     {'value': 10, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "     {'value': 13, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "     {'value': 111, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "     {'value': 19, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "     {'value': 31, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "     {'value': 32, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "     {'value': 109, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "     {'value': 29, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "     {'value': 289, 'end_time': '2023-03-01T08:00:00+0000'},\n",
       "     {'value': 51, 'end_time': '2023-03-02T08:00:00+0000'},\n",
       "     {'value': 12, 'end_time': '2023-03-03T08:00:00+0000'},\n",
       "     {'value': 12, 'end_time': '2023-03-04T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-03-05T08:00:00+0000'},\n",
       "     {'value': 27, 'end_time': '2023-03-06T08:00:00+0000'},\n",
       "     {'value': 14, 'end_time': '2023-03-07T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-03-08T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-03-09T08:00:00+0000'},\n",
       "     {'value': 6, 'end_time': '2023-03-10T08:00:00+0000'},\n",
       "     {'value': 8, 'end_time': '2023-03-11T08:00:00+0000'}],\n",
       "    'title': 'Impressions',\n",
       "    'description': \"Total number of times the Business Account's media objects have been viewed\",\n",
       "    'id': '17841403231458630/insights/impressions/day'},\n",
       "   {'name': 'reach',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 21, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "     {'value': 98, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "     {'value': 39, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "     {'value': 131, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "     {'value': 21, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "     {'value': 10, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "     {'value': 96, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "     {'value': 16, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "     {'value': 7, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "     {'value': 9, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "     {'value': 95, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "     {'value': 28, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "     {'value': 170, 'end_time': '2023-03-01T08:00:00+0000'},\n",
       "     {'value': 30, 'end_time': '2023-03-02T08:00:00+0000'},\n",
       "     {'value': 12, 'end_time': '2023-03-03T08:00:00+0000'},\n",
       "     {'value': 8, 'end_time': '2023-03-04T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-03-05T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-03-06T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-03-07T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-03-08T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-03-09T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-03-10T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-03-11T08:00:00+0000'}],\n",
       "    'title': 'Reach',\n",
       "    'description': \"Total number of times the Business Account's media objects have been uniquely viewed\",\n",
       "    'id': '17841403231458630/insights/reach/day'}]},\n",
       " 2: {'data': [{'name': 'impressions',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 98, 'end_time': '2023-01-10T08:00:00+0000'},\n",
       "     {'value': 28, 'end_time': '2023-01-11T08:00:00+0000'},\n",
       "     {'value': 7, 'end_time': '2023-01-12T08:00:00+0000'},\n",
       "     {'value': 412, 'end_time': '2023-01-13T08:00:00+0000'},\n",
       "     {'value': 45, 'end_time': '2023-01-14T08:00:00+0000'},\n",
       "     {'value': 12, 'end_time': '2023-01-15T08:00:00+0000'},\n",
       "     {'value': 8, 'end_time': '2023-01-16T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-01-17T08:00:00+0000'},\n",
       "     {'value': 172, 'end_time': '2023-01-18T08:00:00+0000'},\n",
       "     {'value': 18, 'end_time': '2023-01-19T08:00:00+0000'},\n",
       "     {'value': 64, 'end_time': '2023-01-20T08:00:00+0000'},\n",
       "     {'value': 23, 'end_time': '2023-01-21T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-01-22T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-01-23T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-01-24T08:00:00+0000'},\n",
       "     {'value': 0, 'end_time': '2023-01-25T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-01-26T08:00:00+0000'},\n",
       "     {'value': 38, 'end_time': '2023-01-27T08:00:00+0000'},\n",
       "     {'value': 29, 'end_time': '2023-01-28T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-01-29T08:00:00+0000'},\n",
       "     {'value': 196, 'end_time': '2023-01-30T08:00:00+0000'},\n",
       "     {'value': 45, 'end_time': '2023-01-31T08:00:00+0000'},\n",
       "     {'value': 14, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "     {'value': 169, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "     {'value': 68, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "     {'value': 57, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "     {'value': 78, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "     {'value': 266, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "     {'value': 77, 'end_time': '2023-02-08T08:00:00+0000'}],\n",
       "    'title': 'Impressions',\n",
       "    'description': \"Total number of times the Business Account's media objects have been viewed\",\n",
       "    'id': '17841403231458630/insights/impressions/day'},\n",
       "   {'name': 'reach',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 84, 'end_time': '2023-01-10T08:00:00+0000'},\n",
       "     {'value': 25, 'end_time': '2023-01-11T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-01-12T08:00:00+0000'},\n",
       "     {'value': 180, 'end_time': '2023-01-13T08:00:00+0000'},\n",
       "     {'value': 39, 'end_time': '2023-01-14T08:00:00+0000'},\n",
       "     {'value': 11, 'end_time': '2023-01-15T08:00:00+0000'},\n",
       "     {'value': 8, 'end_time': '2023-01-16T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-01-17T08:00:00+0000'},\n",
       "     {'value': 127, 'end_time': '2023-01-18T08:00:00+0000'},\n",
       "     {'value': 15, 'end_time': '2023-01-19T08:00:00+0000'},\n",
       "     {'value': 52, 'end_time': '2023-01-20T08:00:00+0000'},\n",
       "     {'value': 15, 'end_time': '2023-01-21T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-01-22T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-01-23T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-01-24T08:00:00+0000'},\n",
       "     {'value': 0, 'end_time': '2023-01-25T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-01-26T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-01-27T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-01-28T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-01-29T08:00:00+0000'},\n",
       "     {'value': 141, 'end_time': '2023-01-30T08:00:00+0000'},\n",
       "     {'value': 43, 'end_time': '2023-01-31T08:00:00+0000'},\n",
       "     {'value': 9, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "     {'value': 109, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "     {'value': 51, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "     {'value': 29, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "     {'value': 67, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "     {'value': 208, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "     {'value': 52, 'end_time': '2023-02-08T08:00:00+0000'}],\n",
       "    'title': 'Reach',\n",
       "    'description': \"Total number of times the Business Account's media objects have been uniquely viewed\",\n",
       "    'id': '17841403231458630/insights/reach/day'}]}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-27 23:03:11.069771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: {'data': [{'name': 'impressions',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 21, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "     {'value': 184, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "     {'value': 48, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "     {'value': 200, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "     {'value': 40, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "     {'value': 10, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "     {'value': 13, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "     {'value': 111, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "     {'value': 19, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "     {'value': 31, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "     {'value': 32, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "     {'value': 109, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "     {'value': 29, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "     {'value': 289, 'end_time': '2023-03-01T08:00:00+0000'},\n",
       "     {'value': 51, 'end_time': '2023-03-02T08:00:00+0000'},\n",
       "     {'value': 12, 'end_time': '2023-03-03T08:00:00+0000'},\n",
       "     {'value': 12, 'end_time': '2023-03-04T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-03-05T08:00:00+0000'},\n",
       "     {'value': 27, 'end_time': '2023-03-06T08:00:00+0000'},\n",
       "     {'value': 14, 'end_time': '2023-03-07T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-03-08T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-03-09T08:00:00+0000'},\n",
       "     {'value': 6, 'end_time': '2023-03-10T08:00:00+0000'},\n",
       "     {'value': 8, 'end_time': '2023-03-11T08:00:00+0000'}],\n",
       "    'title': 'Impressions',\n",
       "    'description': \"Total number of times the Business Account's media objects have been viewed\",\n",
       "    'id': '17841403231458630/insights/impressions/day'},\n",
       "   {'name': 'reach',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 21, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "     {'value': 98, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "     {'value': 39, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "     {'value': 131, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "     {'value': 21, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "     {'value': 10, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "     {'value': 96, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "     {'value': 16, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "     {'value': 7, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "     {'value': 9, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "     {'value': 95, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "     {'value': 28, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "     {'value': 170, 'end_time': '2023-03-01T08:00:00+0000'},\n",
       "     {'value': 30, 'end_time': '2023-03-02T08:00:00+0000'},\n",
       "     {'value': 12, 'end_time': '2023-03-03T08:00:00+0000'},\n",
       "     {'value': 8, 'end_time': '2023-03-04T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-03-05T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-03-06T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-03-07T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-03-08T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-03-09T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-03-10T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-03-11T08:00:00+0000'}],\n",
       "    'title': 'Reach',\n",
       "    'description': \"Total number of times the Business Account's media objects have been uniquely viewed\",\n",
       "    'id': '17841403231458630/insights/reach/day'}]},\n",
       " 2: {'data': [{'name': 'impressions',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 98, 'end_time': '2023-01-10T08:00:00+0000'},\n",
       "     {'value': 28, 'end_time': '2023-01-11T08:00:00+0000'},\n",
       "     {'value': 7, 'end_time': '2023-01-12T08:00:00+0000'},\n",
       "     {'value': 412, 'end_time': '2023-01-13T08:00:00+0000'},\n",
       "     {'value': 45, 'end_time': '2023-01-14T08:00:00+0000'},\n",
       "     {'value': 12, 'end_time': '2023-01-15T08:00:00+0000'},\n",
       "     {'value': 8, 'end_time': '2023-01-16T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-01-17T08:00:00+0000'},\n",
       "     {'value': 172, 'end_time': '2023-01-18T08:00:00+0000'},\n",
       "     {'value': 18, 'end_time': '2023-01-19T08:00:00+0000'},\n",
       "     {'value': 64, 'end_time': '2023-01-20T08:00:00+0000'},\n",
       "     {'value': 23, 'end_time': '2023-01-21T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-01-22T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-01-23T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-01-24T08:00:00+0000'},\n",
       "     {'value': 0, 'end_time': '2023-01-25T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-01-26T08:00:00+0000'},\n",
       "     {'value': 38, 'end_time': '2023-01-27T08:00:00+0000'},\n",
       "     {'value': 29, 'end_time': '2023-01-28T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-01-29T08:00:00+0000'},\n",
       "     {'value': 196, 'end_time': '2023-01-30T08:00:00+0000'},\n",
       "     {'value': 45, 'end_time': '2023-01-31T08:00:00+0000'},\n",
       "     {'value': 14, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "     {'value': 169, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "     {'value': 68, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "     {'value': 57, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "     {'value': 78, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "     {'value': 266, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "     {'value': 77, 'end_time': '2023-02-08T08:00:00+0000'}],\n",
       "    'title': 'Impressions',\n",
       "    'description': \"Total number of times the Business Account's media objects have been viewed\",\n",
       "    'id': '17841403231458630/insights/impressions/day'},\n",
       "   {'name': 'reach',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 84, 'end_time': '2023-01-10T08:00:00+0000'},\n",
       "     {'value': 25, 'end_time': '2023-01-11T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-01-12T08:00:00+0000'},\n",
       "     {'value': 180, 'end_time': '2023-01-13T08:00:00+0000'},\n",
       "     {'value': 39, 'end_time': '2023-01-14T08:00:00+0000'},\n",
       "     {'value': 11, 'end_time': '2023-01-15T08:00:00+0000'},\n",
       "     {'value': 8, 'end_time': '2023-01-16T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-01-17T08:00:00+0000'},\n",
       "     {'value': 127, 'end_time': '2023-01-18T08:00:00+0000'},\n",
       "     {'value': 15, 'end_time': '2023-01-19T08:00:00+0000'},\n",
       "     {'value': 52, 'end_time': '2023-01-20T08:00:00+0000'},\n",
       "     {'value': 15, 'end_time': '2023-01-21T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-01-22T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-01-23T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-01-24T08:00:00+0000'},\n",
       "     {'value': 0, 'end_time': '2023-01-25T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-01-26T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-01-27T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-01-28T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-01-29T08:00:00+0000'},\n",
       "     {'value': 141, 'end_time': '2023-01-30T08:00:00+0000'},\n",
       "     {'value': 43, 'end_time': '2023-01-31T08:00:00+0000'},\n",
       "     {'value': 9, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "     {'value': 109, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "     {'value': 51, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "     {'value': 29, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "     {'value': 67, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "     {'value': 208, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "     {'value': 52, 'end_time': '2023-02-08T08:00:00+0000'}],\n",
       "    'title': 'Reach',\n",
       "    'description': \"Total number of times the Business Account's media objects have been uniquely viewed\",\n",
       "    'id': '17841403231458630/insights/reach/day'}]}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadpickle('silvialiftsweights_03-27_2300_account_insights.sav',\n",
    "     r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\API_response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-27 23:01:14.305098\n",
      "Time completed: 2023-03-27 23:01:14.308065\n",
      "previous since date: 2023-01-21 08:00:00+00:00\n",
      "previous until date: 2023-03-11 08:00:00+00:00\n",
      "\n",
      "Loading previous saved results; no new API calls required.\n",
      "\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-27_2230_account_insights_df.sav\n",
      "Time completed: 2023-03-27 23:01:14.310061\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/API_response/silvialiftsweights_03-27_2230_account_insights.sav\n",
      "Time completed: 2023-03-27 23:01:14.311065\n"
     ]
    }
   ],
   "source": [
    "since = '2023-01-21'\n",
    "until = '2023-03-11'\n",
    "filename3 = 'silvialiftsweights_03-27_2230'\n",
    "\n",
    "data2, response_json_dict2 = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None, \n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since + timedelta(days=1)\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter > since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "        new_response_json_dict = dict()\n",
    "        for page, response in response_json_dict.items():\n",
    "            new_response_json_dict[page] = {'data': response['data']}\n",
    "    except:\n",
    "        df = df_list \n",
    "    if filename:\n",
    "        filename += '_account_insights'\n",
    "        try:\n",
    "            savepickle(df,filename+'_df','sav',csv_path)\n",
    "            savepickle(new_response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, new_response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to load prior results; making new API calls for entire date range.\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1677657600.0&since=1675152000.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-31 00:00:00\n",
      "Number of days of data: 30\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-27_2310_account_insights_df.sav\n",
      "Time completed: 2023-03-27 23:09:52.328145\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/API_response/silvialiftsweights_03-27_2310_account_insights.sav\n",
      "Time completed: 2023-03-27 23:09:52.329145\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    timestamp_column_suffix='end_time', filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\API_response',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-15 1:22\n",
    "    Get the daily impressions and reach a given Instagram account. \n",
    "    Load any results that were previously saved (pull new data if no previously saved results available).\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - timestamp_column_suffix (str): Suffix of the timestamp columns. default is 'end_time'. \n",
    "            Required to parse out the date range of the previously saved outputs.\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    previous_since, previous_until = None, None\n",
    "    if filename:\n",
    "        filename2 = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename2+'_df.sav', csv_path)\n",
    "        df = df.reset_index(drop=True)\n",
    "        timestamp_column = df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]\n",
    "        df = df.sort_values(timestamp_column)\n",
    "        response_json_dict = loadpickle(filename2+'.sav', json_path)\n",
    "        previous_since = datetime.strptime(df.iloc[0][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        previous_until = datetime.strptime(df.iloc[-1][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        print('previous since date:', previous_since)\n",
    "        print('previous until date:', previous_until)\n",
    "    except:\n",
    "        print('Unable to load prior results; making new API calls for entire date range.')\n",
    "    \n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    \n",
    "    if (previous_since == None) & (previous_until == None):\n",
    "        df, response_json_dict = get_ig_account_insights(\n",
    "            ig_user_id, access_token, since=since, until=until, filename=filename,\n",
    "            json_path=json_path)\n",
    "        return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "    elif previous_since == None:\n",
    "        previous_since = since + timedelta (days=1)\n",
    "        print('Previous `since` parameter could not be found; default to since + 1.')\n",
    "    elif previous_until == None:\n",
    "        previous_until = until - timedelta (days=1)\n",
    "        print('Previous `until` parameter could not be found; default to until - 1.')\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'\\nFetching older account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        older_insights_df, older_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token,\n",
    "            json_path=json_path, since=since, until=previous_since)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), older_insights_df])\n",
    "        except:\n",
    "            df = older_insights_df\n",
    "        try:\n",
    "            # Update the keys of *response_json_dict* before merging with older_insights_response_json_dict. That way, final \n",
    "                # response dictionary always has insights from oldest dates first\n",
    "            response_json_dict = dict( \n",
    "                zip([key+len(older_insights_response_json_dict) for key in response_json_dict.keys()], response_json_dict.values())\n",
    "                )\n",
    "            response_json_dict = {**older_insights_response_json_dict, **response_json_dict}\n",
    "        except:\n",
    "            response_json_dict = older_insights_response_json_dict\n",
    "    if (previous_until.date() < until.date()):\n",
    "        print(f'\\nFetching newer account insights from {datetime.strftime(previous_until, \"%Y-%m-%d\")} to {datetime.strftime(until, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            json_path=json_path, since=previous_until, until=until)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), new_insights_df])\n",
    "        except:\n",
    "            df = new_insights_df\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "        # remove items containing API key as this will invalidate access tokens if published to Github\n",
    "        response_json_dict = [response_json_dict[i]['data'] for i in response_json_dict] \n",
    "    \n",
    "    if (previous_until.date() >= until.date()) & (previous_since.date() <= since.date()):\n",
    "        print('\\nLoading previous saved results; no new API calls required.\\n')\n",
    "        \n",
    "    if filename:\n",
    "        try:\n",
    "            savepickle(df, filename2+'_df', 'sav', csv_path)\n",
    "            savepickle(response_json_dict,filename2,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "\n",
    "since = '2023-01-31'\n",
    "until = '2023-03-01'\n",
    "filename3 = 'silvialiftsweights_03-27_2310'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-27 23:29:40.738098\n",
      "Unable to load prior results; making new API calls for entire date range.\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1677657600.0&since=1675152000.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-31 00:00:00\n",
      "Number of days of data: 30\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-27_2310_account_insights_df.sav\n",
      "Time completed: 2023-03-27 23:29:41.713974\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/API_response/silvialiftsweights_03-27_2310_account_insights.sav\n",
      "Time completed: 2023-03-27 23:29:41.714980\n"
     ]
    }
   ],
   "source": [
    "since = '2023-01-31'\n",
    "until = '2023-03-01'\n",
    "filename3 = 'silvialiftsweights_03-27_2310'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-27 23:30:01.269549\n",
      "Time completed: 2023-03-27 23:30:01.271548\n",
      "previous since date: 2023-01-31 08:00:00+00:00\n",
      "previous until date: 2023-03-01 08:00:00+00:00\n",
      "\n",
      "Loading previous saved results; no new API calls required.\n",
      "\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-27_2310_account_insights_df.sav\n",
      "Time completed: 2023-03-27 23:30:01.275521\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/API_response/silvialiftsweights_03-27_2310_account_insights.sav\n",
      "Time completed: 2023-03-27 23:30:01.276522\n"
     ]
    }
   ],
   "source": [
    "since = '2023-01-31'\n",
    "until = '2023-03-01'\n",
    "filename3 = 'silvialiftsweights_03-27_2310'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'data': [{'name': 'impressions',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 45, 'end_time': '2023-01-31T08:00:00+0000'},\n",
       "     {'value': 14, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "     {'value': 169, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "     {'value': 68, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "     {'value': 57, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "     {'value': 78, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "     {'value': 266, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "     {'value': 77, 'end_time': '2023-02-08T08:00:00+0000'},\n",
       "     {'value': 21, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "     {'value': 184, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "     {'value': 48, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "     {'value': 200, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "     {'value': 40, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "     {'value': 10, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "     {'value': 13, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "     {'value': 111, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "     {'value': 19, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "     {'value': 31, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "     {'value': 32, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "     {'value': 109, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "     {'value': 29, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "     {'value': 289, 'end_time': '2023-03-01T08:00:00+0000'}],\n",
       "    'title': 'Impressions',\n",
       "    'description': \"Total number of times the Business Account's media objects have been viewed\",\n",
       "    'id': '17841403231458630/insights/impressions/day'},\n",
       "   {'name': 'reach',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 43, 'end_time': '2023-01-31T08:00:00+0000'},\n",
       "     {'value': 9, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "     {'value': 109, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "     {'value': 51, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "     {'value': 29, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "     {'value': 67, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "     {'value': 208, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "     {'value': 52, 'end_time': '2023-02-08T08:00:00+0000'},\n",
       "     {'value': 21, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "     {'value': 98, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "     {'value': 39, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "     {'value': 131, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "     {'value': 21, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "     {'value': 10, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "     {'value': 96, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "     {'value': 16, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "     {'value': 7, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "     {'value': 9, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "     {'value': 95, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "     {'value': 28, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "     {'value': 170, 'end_time': '2023-03-01T08:00:00+0000'}],\n",
       "    'title': 'Reach',\n",
       "    'description': \"Total number of times the Business Account's media objects have been uniquely viewed\",\n",
       "    'id': '17841403231458630/insights/reach/day'}]}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-27 23:30:30.124609\n",
      "Time completed: 2023-03-27 23:30:30.128608\n",
      "previous since date: 2023-01-31 08:00:00+00:00\n",
      "previous until date: 2023-03-01 08:00:00+00:00\n",
      "\n",
      "Fetching newer account insights from 2023-03-01 to 2023-03-11\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1678521600.0&since=1677657600.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-03-01 00:00:00\n",
      "Number of days of data: 11\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-27_2310_account_insights_df.sav\n",
      "Time completed: 2023-03-27 23:30:30.495335\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/API_response/silvialiftsweights_03-27_2310_account_insights.sav\n",
      "Time completed: 2023-03-27 23:30:30.496330\n"
     ]
    }
   ],
   "source": [
    "since = '2023-01-31'\n",
    "until = '2023-03-11'\n",
    "filename3 = 'silvialiftsweights_03-27_2310'\n",
    "\n",
    "data, response_json_dict = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'name': 'impressions',\n",
       "   'period': 'day',\n",
       "   'values': [{'value': 45, 'end_time': '2023-01-31T08:00:00+0000'},\n",
       "    {'value': 14, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "    {'value': 5, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "    {'value': 169, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "    {'value': 68, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "    {'value': 57, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "    {'value': 78, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "    {'value': 266, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "    {'value': 77, 'end_time': '2023-02-08T08:00:00+0000'},\n",
       "    {'value': 21, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "    {'value': 3, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "    {'value': 1, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "    {'value': 184, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "    {'value': 48, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "    {'value': 200, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "    {'value': 40, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "    {'value': 10, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "    {'value': 2, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "    {'value': 2, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "    {'value': 13, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "    {'value': 4, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "    {'value': 111, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "    {'value': 19, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "    {'value': 31, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "    {'value': 3, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "    {'value': 1, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "    {'value': 32, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "    {'value': 109, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "    {'value': 29, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "    {'value': 289, 'end_time': '2023-03-01T08:00:00+0000'}],\n",
       "   'title': 'Impressions',\n",
       "   'description': \"Total number of times the Business Account's media objects have been viewed\",\n",
       "   'id': '17841403231458630/insights/impressions/day'},\n",
       "  {'name': 'reach',\n",
       "   'period': 'day',\n",
       "   'values': [{'value': 43, 'end_time': '2023-01-31T08:00:00+0000'},\n",
       "    {'value': 9, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "    {'value': 4, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "    {'value': 109, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "    {'value': 51, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "    {'value': 29, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "    {'value': 67, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "    {'value': 208, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "    {'value': 52, 'end_time': '2023-02-08T08:00:00+0000'},\n",
       "    {'value': 21, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "    {'value': 3, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "    {'value': 1, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "    {'value': 98, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "    {'value': 39, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "    {'value': 131, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "    {'value': 21, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "    {'value': 10, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "    {'value': 2, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "    {'value': 2, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "    {'value': 1, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "    {'value': 2, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "    {'value': 96, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "    {'value': 16, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "    {'value': 7, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "    {'value': 3, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "    {'value': 1, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "    {'value': 9, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "    {'value': 95, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "    {'value': 28, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "    {'value': 170, 'end_time': '2023-03-01T08:00:00+0000'}],\n",
       "   'title': 'Reach',\n",
       "   'description': \"Total number of times the Business Account's media objects have been uniquely viewed\",\n",
       "   'id': '17841403231458630/insights/reach/day'}],\n",
       " [{'name': 'impressions',\n",
       "   'period': 'day',\n",
       "   'values': [{'value': 289, 'end_time': '2023-03-01T08:00:00+0000'},\n",
       "    {'value': 51, 'end_time': '2023-03-02T08:00:00+0000'},\n",
       "    {'value': 12, 'end_time': '2023-03-03T08:00:00+0000'},\n",
       "    {'value': 12, 'end_time': '2023-03-04T08:00:00+0000'},\n",
       "    {'value': 3, 'end_time': '2023-03-05T08:00:00+0000'},\n",
       "    {'value': 27, 'end_time': '2023-03-06T08:00:00+0000'},\n",
       "    {'value': 14, 'end_time': '2023-03-07T08:00:00+0000'},\n",
       "    {'value': 1, 'end_time': '2023-03-08T08:00:00+0000'},\n",
       "    {'value': 5, 'end_time': '2023-03-09T08:00:00+0000'},\n",
       "    {'value': 6, 'end_time': '2023-03-10T08:00:00+0000'},\n",
       "    {'value': 8, 'end_time': '2023-03-11T08:00:00+0000'}],\n",
       "   'title': 'Impressions',\n",
       "   'description': \"Total number of times the Business Account's media objects have been viewed\",\n",
       "   'id': '17841403231458630/insights/impressions/day'},\n",
       "  {'name': 'reach',\n",
       "   'period': 'day',\n",
       "   'values': [{'value': 170, 'end_time': '2023-03-01T08:00:00+0000'},\n",
       "    {'value': 30, 'end_time': '2023-03-02T08:00:00+0000'},\n",
       "    {'value': 12, 'end_time': '2023-03-03T08:00:00+0000'},\n",
       "    {'value': 8, 'end_time': '2023-03-04T08:00:00+0000'},\n",
       "    {'value': 3, 'end_time': '2023-03-05T08:00:00+0000'},\n",
       "    {'value': 5, 'end_time': '2023-03-06T08:00:00+0000'},\n",
       "    {'value': 4, 'end_time': '2023-03-07T08:00:00+0000'},\n",
       "    {'value': 1, 'end_time': '2023-03-08T08:00:00+0000'},\n",
       "    {'value': 5, 'end_time': '2023-03-09T08:00:00+0000'},\n",
       "    {'value': 4, 'end_time': '2023-03-10T08:00:00+0000'},\n",
       "    {'value': 2, 'end_time': '2023-03-11T08:00:00+0000'}],\n",
       "   'title': 'Reach',\n",
       "   'description': \"Total number of times the Business Account's media objects have been uniquely viewed\",\n",
       "   'id': '17841403231458630/insights/reach/day'}]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_json_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None, \n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since + timedelta(days=1)\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter > since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "        new_response_json_dict = dict()\n",
    "        for page, response in response_json_dict.items():\n",
    "            new_response_json_dict[page] = {'data': response['data']}\n",
    "    except:\n",
    "        df = df_list \n",
    "    if filename:\n",
    "        filename += '_account_insights'\n",
    "        try:\n",
    "            savepickle(df,filename+'_df','sav',csv_path)\n",
    "            savepickle(new_response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to load prior results; making new API calls for entire date range.\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1677657600.0&since=1675152000.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-31 00:00:00\n",
      "Number of days of data: 30\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-27_2335_account_insights_df.sav\n",
      "Time completed: 2023-03-27 23:38:04.971161\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/API_response/silvialiftsweights_03-27_2335_account_insights.sav\n",
      "Time completed: 2023-03-27 23:38:04.973180\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    timestamp_column_suffix='end_time', filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\API_response',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-15 1:22\n",
    "    Get the daily impressions and reach a given Instagram account. \n",
    "    Load any results that were previously saved (pull new data if no previously saved results available).\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - timestamp_column_suffix (str): Suffix of the timestamp columns. default is 'end_time'. \n",
    "            Required to parse out the date range of the previously saved outputs.\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    previous_since, previous_until = None, None\n",
    "    if filename:\n",
    "        filename2 = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename2+'_df.sav', csv_path)\n",
    "        df = df.reset_index(drop=True)\n",
    "        timestamp_column = df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]\n",
    "        df = df.sort_values(timestamp_column)\n",
    "        response_json_dict = loadpickle(filename2+'.sav', json_path)\n",
    "        previous_since = datetime.strptime(df.iloc[0][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        previous_until = datetime.strptime(df.iloc[-1][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        print('previous since date:', previous_since)\n",
    "        print('previous until date:', previous_until)\n",
    "    except:\n",
    "        print('Unable to load prior results; making new API calls for entire date range.')\n",
    "    \n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    \n",
    "    if (previous_since == None) & (previous_until == None):\n",
    "        df, response_json_dict = get_ig_account_insights(\n",
    "            ig_user_id, access_token, since=since, until=until, filename=filename,\n",
    "            json_path=json_path)\n",
    "        return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "    elif previous_since == None:\n",
    "        previous_since = since + timedelta (days=1)\n",
    "        print('Previous `since` parameter could not be found; default to since + 1.')\n",
    "    elif previous_until == None:\n",
    "        previous_until = until - timedelta (days=1)\n",
    "        print('Previous `until` parameter could not be found; default to until - 1.')\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'\\nFetching older account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        older_insights_df, older_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token,\n",
    "            json_path=json_path, since=since, until=previous_since)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), older_insights_df])\n",
    "        except:\n",
    "            df = older_insights_df\n",
    "        try:\n",
    "            # Update the keys of *response_json_dict* before merging with older_insights_response_json_dict. That way, final \n",
    "                # response dictionary always has insights from oldest dates first\n",
    "            response_json_dict = dict( \n",
    "                zip([key+len(older_insights_response_json_dict) for key in response_json_dict.keys()], response_json_dict.values())\n",
    "                )\n",
    "            response_json_dict = {**older_insights_response_json_dict, **response_json_dict}\n",
    "        except:\n",
    "            response_json_dict = older_insights_response_json_dict\n",
    "    if (previous_until.date() < until.date()):\n",
    "        print(f'\\nFetching newer account insights from {datetime.strftime(previous_until, \"%Y-%m-%d\")} to {datetime.strftime(until, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            json_path=json_path, since=previous_until, until=until)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), new_insights_df])\n",
    "        except:\n",
    "            df = new_insights_df\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "        # remove items containing API key as this will invalidate access tokens if published to Github\n",
    "        response_json_dict = [response_json_dict[i]['data'] for i in response_json_dict] \n",
    "        new_response_json_dict = dict()\n",
    "        for page, response in response_json_dict.items():\n",
    "            new_response_json_dict[page] = {'data': response['data']}\n",
    "    \n",
    "    if (previous_until.date() >= until.date()) & (previous_since.date() <= since.date()):\n",
    "        print('\\nLoading previous saved results; no new API calls required.\\n')\n",
    "        new_response_json_dict = response_json_dict\n",
    "        \n",
    "    if filename:\n",
    "        try:\n",
    "            savepickle(df, filename2+'_df', 'sav', csv_path)\n",
    "            savepickle(new_response_json_dict,filename2,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "\n",
    "since = '2023-01-31'\n",
    "until = '2023-03-01'\n",
    "filename3 = 'silvialiftsweights_03-27_2335'\n",
    "\n",
    "data21, response_json_dict21 = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-27 23:38:10.620770\n",
      "Time completed: 2023-03-27 23:38:10.622806\n",
      "previous since date: 2023-01-31 08:00:00+00:00\n",
      "previous until date: 2023-03-01 08:00:00+00:00\n",
      "\n",
      "Loading previous saved results; no new API calls required.\n",
      "\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-27_2335_account_insights_df.sav\n",
      "Time completed: 2023-03-27 23:38:10.623768\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/API_response/silvialiftsweights_03-27_2335_account_insights.sav\n",
      "Time completed: 2023-03-27 23:38:10.624767\n"
     ]
    }
   ],
   "source": [
    "since = '2023-01-31'\n",
    "until = '2023-03-01'\n",
    "filename3 = 'silvialiftsweights_03-27_2335'\n",
    "\n",
    "data21, response_json_dict21 = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'data': [{'name': 'impressions',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 45, 'end_time': '2023-01-31T08:00:00+0000'},\n",
       "     {'value': 14, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "     {'value': 169, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "     {'value': 68, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "     {'value': 57, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "     {'value': 78, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "     {'value': 266, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "     {'value': 77, 'end_time': '2023-02-08T08:00:00+0000'},\n",
       "     {'value': 21, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "     {'value': 184, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "     {'value': 48, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "     {'value': 200, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "     {'value': 40, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "     {'value': 10, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "     {'value': 13, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "     {'value': 111, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "     {'value': 19, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "     {'value': 31, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "     {'value': 32, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "     {'value': 109, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "     {'value': 29, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "     {'value': 289, 'end_time': '2023-03-01T08:00:00+0000'}],\n",
       "    'title': 'Impressions',\n",
       "    'description': \"Total number of times the Business Account's media objects have been viewed\",\n",
       "    'id': '17841403231458630/insights/impressions/day'},\n",
       "   {'name': 'reach',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 43, 'end_time': '2023-01-31T08:00:00+0000'},\n",
       "     {'value': 9, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "     {'value': 109, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "     {'value': 51, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "     {'value': 29, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "     {'value': 67, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "     {'value': 208, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "     {'value': 52, 'end_time': '2023-02-08T08:00:00+0000'},\n",
       "     {'value': 21, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "     {'value': 98, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "     {'value': 39, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "     {'value': 131, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "     {'value': 21, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "     {'value': 10, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "     {'value': 96, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "     {'value': 16, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "     {'value': 7, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "     {'value': 9, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "     {'value': 95, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "     {'value': 28, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "     {'value': 170, 'end_time': '2023-03-01T08:00:00+0000'}],\n",
       "    'title': 'Reach',\n",
       "    'description': \"Total number of times the Business Account's media objects have been uniquely viewed\",\n",
       "    'id': '17841403231458630/insights/reach/day'}]}}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_json_dict21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-27 23:39:25.864480\n",
      "Time completed: 2023-03-27 23:39:25.867483\n",
      "previous since date: 2023-01-31 08:00:00+00:00\n",
      "previous until date: 2023-03-01 08:00:00+00:00\n",
      "\n",
      "Fetching newer account insights from 2023-03-01 to 2023-03-11\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1678521600.0&since=1677657600.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-03-01 00:00:00\n",
      "Number of days of data: 11\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\notebooks\\2023-03-09 IG insights metrics.ipynb Cell 49\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-09%20IG%20insights%20metrics.ipynb#Y105sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m until \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m2023-03-11\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-09%20IG%20insights%20metrics.ipynb#Y105sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m filename3 \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msilvialiftsweights_03-27_2335\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-09%20IG%20insights%20metrics.ipynb#Y105sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m data21, response_json_dict21 \u001b[39m=\u001b[39m update_ig_account_insights(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-09%20IG%20insights%20metrics.ipynb#Y105sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     ig_user_id, access_token, since\u001b[39m=\u001b[39;49msince, until\u001b[39m=\u001b[39;49muntil, filename\u001b[39m=\u001b[39;49mfilename3)\n",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\notebooks\\2023-03-09 IG insights metrics.ipynb Cell 49\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-09%20IG%20insights%20metrics.ipynb#Y105sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m     response_json_dict \u001b[39m=\u001b[39m [response_json_dict[i][\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m response_json_dict] \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-09%20IG%20insights%20metrics.ipynb#Y105sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m     new_response_json_dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-09%20IG%20insights%20metrics.ipynb#Y105sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m     \u001b[39mfor\u001b[39;00m page, response \u001b[39min\u001b[39;00m response_json_dict\u001b[39m.\u001b[39;49mitems():\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-09%20IG%20insights%20metrics.ipynb#Y105sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m         new_response_json_dict[page] \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m: response[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m]}\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-03-09%20IG%20insights%20metrics.ipynb#Y105sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m \u001b[39mif\u001b[39;00m (previous_until\u001b[39m.\u001b[39mdate() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m until\u001b[39m.\u001b[39mdate()) \u001b[39m&\u001b[39m (previous_since\u001b[39m.\u001b[39mdate() \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m since\u001b[39m.\u001b[39mdate()):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "since = '2023-01-31'\n",
    "until = '2023-03-11'\n",
    "filename3 = 'silvialiftsweights_03-27_2335'\n",
    "\n",
    "data21, response_json_dict21 = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None, \n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since + timedelta(days=1)\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter > since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "        new_response_json_dict = dict()\n",
    "        for page, response in response_json_dict.items():\n",
    "            new_response_json_dict[page] = {'data': response['data']}\n",
    "    except:\n",
    "        df = df_list \n",
    "    if filename:\n",
    "        filename += '_account_insights'\n",
    "        try:\n",
    "            savepickle(df,filename+'_df','sav',csv_path)\n",
    "            savepickle(new_response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to load prior results; making new API calls for entire date range.\n",
      "https://graph.facebook.com/v15.0/17841403231458630/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1677657600.0&since=1675152000.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  200\n",
      "since_parameter:  2023-01-31 00:00:00\n",
      "Number of days of data: 30\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-27_2340_account_insights_df.sav\n",
      "Time completed: 2023-03-27 23:42:17.604576\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/API_response/silvialiftsweights_03-27_2340_account_insights.sav\n",
      "Time completed: 2023-03-27 23:42:17.605579\n"
     ]
    }
   ],
   "source": [
    "# Make GET request only for dates that have not been saved in previous queries\n",
    "from datetime import time, datetime, timedelta\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    timestamp_column_suffix='end_time', filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\API_response',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-15 1:22\n",
    "    Get the daily impressions and reach a given Instagram account. \n",
    "    Load any results that were previously saved (pull new data if no previously saved results available).\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - timestamp_column_suffix (str): Suffix of the timestamp columns. default is 'end_time'. \n",
    "            Required to parse out the date range of the previously saved outputs.\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    previous_since, previous_until = None, None\n",
    "    if filename:\n",
    "        filename2 = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename2+'_df.sav', csv_path)\n",
    "        df = df.reset_index(drop=True)\n",
    "        timestamp_column = df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]\n",
    "        df = df.sort_values(timestamp_column)\n",
    "        response_json_dict = loadpickle(filename2+'.sav', json_path)\n",
    "        previous_since = datetime.strptime(df.iloc[0][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        previous_until = datetime.strptime(df.iloc[-1][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        print('previous since date:', previous_since)\n",
    "        print('previous until date:', previous_until)\n",
    "    except:\n",
    "        print('Unable to load prior results; making new API calls for entire date range.')\n",
    "    \n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    \n",
    "    if (previous_since == None) & (previous_until == None):\n",
    "        df, response_json_dict = get_ig_account_insights(\n",
    "            ig_user_id, access_token, since=since, until=until, filename=filename,\n",
    "            json_path=json_path)\n",
    "        return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "    elif previous_since == None:\n",
    "        previous_since = since + timedelta (days=1)\n",
    "        print('Previous `since` parameter could not be found; default to since + 1.')\n",
    "    elif previous_until == None:\n",
    "        previous_until = until - timedelta (days=1)\n",
    "        print('Previous `until` parameter could not be found; default to until - 1.')\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'\\nFetching older account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        older_insights_df, older_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token,\n",
    "            json_path=json_path, since=since, until=previous_since)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), older_insights_df])\n",
    "        except:\n",
    "            df = older_insights_df\n",
    "        try:\n",
    "            # Update the keys of *response_json_dict* before merging with older_insights_response_json_dict. That way, final \n",
    "                # response dictionary always has insights from oldest dates first\n",
    "            response_json_dict = dict( \n",
    "                zip([key+len(older_insights_response_json_dict) for key in response_json_dict.keys()], response_json_dict.values())\n",
    "                )\n",
    "            response_json_dict = {**older_insights_response_json_dict, **response_json_dict}\n",
    "        except:\n",
    "            response_json_dict = older_insights_response_json_dict\n",
    "    if (previous_until.date() < until.date()):\n",
    "        print(f'\\nFetching newer account insights from {datetime.strftime(previous_until, \"%Y-%m-%d\")} to {datetime.strftime(until, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            json_path=json_path, since=previous_until, until=until)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), new_insights_df])\n",
    "        except:\n",
    "            df = new_insights_df\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "        # remove items containing API key as this will invalidate access tokens if published to Github\n",
    "        new_response_json_dict = dict()\n",
    "        for page, response in response_json_dict.items():\n",
    "            new_response_json_dict[page] = {'data': response['data']}\n",
    "    \n",
    "    if (previous_until.date() >= until.date()) & (previous_since.date() <= since.date()):\n",
    "        print('\\nLoading previous saved results; no new API calls required.\\n')\n",
    "        new_response_json_dict = response_json_dict\n",
    "        \n",
    "    if filename:\n",
    "        try:\n",
    "            savepickle(df, filename2+'_df', 'sav', csv_path)\n",
    "            savepickle(new_response_json_dict,filename2,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "\n",
    "since = '2023-01-31'\n",
    "until = '2023-03-01'\n",
    "filename3 = 'silvialiftsweights_03-27_2340'\n",
    "\n",
    "data22, response_json_dict22 = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-27 23:43:35.749764\n",
      "Time completed: 2023-03-27 23:43:35.754768\n",
      "previous since date: 2023-01-31 08:00:00+00:00\n",
      "previous until date: 2023-03-01 08:00:00+00:00\n",
      "\n",
      "Loading previous saved results; no new API calls required.\n",
      "\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-27_2340_account_insights_df.sav\n",
      "Time completed: 2023-03-27 23:43:35.757791\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/API_response/silvialiftsweights_03-27_2340_account_insights.sav\n",
      "Time completed: 2023-03-27 23:43:35.760765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: {'data': [{'name': 'impressions',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 45, 'end_time': '2023-01-31T08:00:00+0000'},\n",
       "     {'value': 14, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "     {'value': 169, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "     {'value': 68, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "     {'value': 57, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "     {'value': 78, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "     {'value': 266, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "     {'value': 77, 'end_time': '2023-02-08T08:00:00+0000'},\n",
       "     {'value': 21, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "     {'value': 184, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "     {'value': 48, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "     {'value': 200, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "     {'value': 40, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "     {'value': 10, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "     {'value': 13, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "     {'value': 111, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "     {'value': 19, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "     {'value': 31, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "     {'value': 32, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "     {'value': 109, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "     {'value': 29, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "     {'value': 289, 'end_time': '2023-03-01T08:00:00+0000'}],\n",
       "    'title': 'Impressions',\n",
       "    'description': \"Total number of times the Business Account's media objects have been viewed\",\n",
       "    'id': '17841403231458630/insights/impressions/day'},\n",
       "   {'name': 'reach',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 43, 'end_time': '2023-01-31T08:00:00+0000'},\n",
       "     {'value': 9, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "     {'value': 109, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "     {'value': 51, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "     {'value': 29, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "     {'value': 67, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "     {'value': 208, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "     {'value': 52, 'end_time': '2023-02-08T08:00:00+0000'},\n",
       "     {'value': 21, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "     {'value': 98, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "     {'value': 39, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "     {'value': 131, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "     {'value': 21, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "     {'value': 10, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "     {'value': 96, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "     {'value': 16, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "     {'value': 7, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "     {'value': 9, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "     {'value': 95, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "     {'value': 28, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "     {'value': 170, 'end_time': '2023-03-01T08:00:00+0000'}],\n",
       "    'title': 'Reach',\n",
       "    'description': \"Total number of times the Business Account's media objects have been uniquely viewed\",\n",
       "    'id': '17841403231458630/insights/reach/day'}]}}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "since = '2023-01-31'\n",
    "until = '2023-03-01'\n",
    "filename3 = 'silvialiftsweights_03-27_2340'\n",
    "data22, response_json_dict22 = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)\n",
    "response_json_dict22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = '2023-01-31'\n",
    "until = '2023-03-11'\n",
    "filename3 = 'silvialiftsweights_03-27_2340'\n",
    "data22, response_json_dict22 = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)\n",
    "response_json_dict22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time completed: 2023-03-27 23:44:13.596523\n",
      "Time completed: 2023-03-27 23:44:13.609513\n",
      "previous since date: 2023-01-31 08:00:00+00:00\n",
      "previous until date: 2023-03-11 08:00:00+00:00\n",
      "\n",
      "Loading previous saved results; no new API calls required.\n",
      "\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/interim/silvialiftsweights_03-27_2340_account_insights_df.sav\n",
      "Time completed: 2023-03-27 23:44:13.611520\n",
      "File saved:  C:/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/data/API_response/silvialiftsweights_03-27_2340_account_insights.sav\n",
      "Time completed: 2023-03-27 23:44:13.613515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: {'data': [{'name': 'impressions',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 45, 'end_time': '2023-01-31T08:00:00+0000'},\n",
       "     {'value': 14, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "     {'value': 169, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "     {'value': 68, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "     {'value': 57, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "     {'value': 78, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "     {'value': 266, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "     {'value': 77, 'end_time': '2023-02-08T08:00:00+0000'},\n",
       "     {'value': 21, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "     {'value': 184, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "     {'value': 48, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "     {'value': 200, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "     {'value': 40, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "     {'value': 10, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "     {'value': 13, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "     {'value': 111, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "     {'value': 19, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "     {'value': 31, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "     {'value': 32, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "     {'value': 109, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "     {'value': 29, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "     {'value': 289, 'end_time': '2023-03-01T08:00:00+0000'}],\n",
       "    'title': 'Impressions',\n",
       "    'description': \"Total number of times the Business Account's media objects have been viewed\",\n",
       "    'id': '17841403231458630/insights/impressions/day'},\n",
       "   {'name': 'reach',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 43, 'end_time': '2023-01-31T08:00:00+0000'},\n",
       "     {'value': 9, 'end_time': '2023-02-01T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-02-02T08:00:00+0000'},\n",
       "     {'value': 109, 'end_time': '2023-02-03T08:00:00+0000'},\n",
       "     {'value': 51, 'end_time': '2023-02-04T08:00:00+0000'},\n",
       "     {'value': 29, 'end_time': '2023-02-05T08:00:00+0000'},\n",
       "     {'value': 67, 'end_time': '2023-02-06T08:00:00+0000'},\n",
       "     {'value': 208, 'end_time': '2023-02-07T08:00:00+0000'},\n",
       "     {'value': 52, 'end_time': '2023-02-08T08:00:00+0000'},\n",
       "     {'value': 21, 'end_time': '2023-02-09T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-10T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-11T08:00:00+0000'},\n",
       "     {'value': 98, 'end_time': '2023-02-12T08:00:00+0000'},\n",
       "     {'value': 39, 'end_time': '2023-02-13T08:00:00+0000'},\n",
       "     {'value': 131, 'end_time': '2023-02-14T08:00:00+0000'},\n",
       "     {'value': 21, 'end_time': '2023-02-15T08:00:00+0000'},\n",
       "     {'value': 10, 'end_time': '2023-02-16T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-17T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-18T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-19T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-02-20T08:00:00+0000'},\n",
       "     {'value': 96, 'end_time': '2023-02-21T08:00:00+0000'},\n",
       "     {'value': 16, 'end_time': '2023-02-22T08:00:00+0000'},\n",
       "     {'value': 7, 'end_time': '2023-02-23T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-02-24T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-02-25T08:00:00+0000'},\n",
       "     {'value': 9, 'end_time': '2023-02-26T08:00:00+0000'},\n",
       "     {'value': 95, 'end_time': '2023-02-27T08:00:00+0000'},\n",
       "     {'value': 28, 'end_time': '2023-02-28T08:00:00+0000'},\n",
       "     {'value': 170, 'end_time': '2023-03-01T08:00:00+0000'}],\n",
       "    'title': 'Reach',\n",
       "    'description': \"Total number of times the Business Account's media objects have been uniquely viewed\",\n",
       "    'id': '17841403231458630/insights/reach/day'}]},\n",
       " 2: {'data': [{'name': 'impressions',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 289, 'end_time': '2023-03-01T08:00:00+0000'},\n",
       "     {'value': 51, 'end_time': '2023-03-02T08:00:00+0000'},\n",
       "     {'value': 12, 'end_time': '2023-03-03T08:00:00+0000'},\n",
       "     {'value': 12, 'end_time': '2023-03-04T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-03-05T08:00:00+0000'},\n",
       "     {'value': 27, 'end_time': '2023-03-06T08:00:00+0000'},\n",
       "     {'value': 14, 'end_time': '2023-03-07T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-03-08T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-03-09T08:00:00+0000'},\n",
       "     {'value': 6, 'end_time': '2023-03-10T08:00:00+0000'},\n",
       "     {'value': 8, 'end_time': '2023-03-11T08:00:00+0000'}],\n",
       "    'title': 'Impressions',\n",
       "    'description': \"Total number of times the Business Account's media objects have been viewed\",\n",
       "    'id': '17841403231458630/insights/impressions/day'},\n",
       "   {'name': 'reach',\n",
       "    'period': 'day',\n",
       "    'values': [{'value': 170, 'end_time': '2023-03-01T08:00:00+0000'},\n",
       "     {'value': 30, 'end_time': '2023-03-02T08:00:00+0000'},\n",
       "     {'value': 12, 'end_time': '2023-03-03T08:00:00+0000'},\n",
       "     {'value': 8, 'end_time': '2023-03-04T08:00:00+0000'},\n",
       "     {'value': 3, 'end_time': '2023-03-05T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-03-06T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-03-07T08:00:00+0000'},\n",
       "     {'value': 1, 'end_time': '2023-03-08T08:00:00+0000'},\n",
       "     {'value': 5, 'end_time': '2023-03-09T08:00:00+0000'},\n",
       "     {'value': 4, 'end_time': '2023-03-10T08:00:00+0000'},\n",
       "     {'value': 2, 'end_time': '2023-03-11T08:00:00+0000'}],\n",
       "    'title': 'Reach',\n",
       "    'description': \"Total number of times the Business Account's media objects have been uniquely viewed\",\n",
       "    'id': '17841403231458630/insights/reach/day'}]}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "since = '2023-01-31'\n",
    "until = '2023-03-11'\n",
    "filename3 = 'silvialiftsweights_03-27_2340'\n",
    "data22, response_json_dict22 = update_ig_account_insights(\n",
    "    ig_user_id, access_token, since=since, until=until, filename=filename3)\n",
    "response_json_dict22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
