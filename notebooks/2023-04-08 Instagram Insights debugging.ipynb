{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "[]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import json_normalize  \n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\custom_python\")\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\src\")\n",
    "from silvhua import *\n",
    "from datetime import datetime\n",
    "import json\n",
    "# from plotly.subplots import make_subplots\n",
    "import requests\n",
    "from datetime import time, datetime, timedelta\n",
    "import pickle\n",
    "import streamlit as st\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.json\") as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "# ig_user_id = credentials['am_ig_user_id']\n",
    "# access_token = credentials['am_ig_access_token']\n",
    "ig_user_id = credentials['mf_ig_user_id']\n",
    "access_token = credentials['mf_access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the option to wrap text within cells\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Account issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://graph.facebook.com/v15.0/17841400352707412/insights?metric=impressions%2Creach&metric_type=time_series&period=day&until=1680332400.0&since=1677744000.0\n",
      "Requesting page 1...\n",
      "\tResponse status code:  400\n",
      "{'message': '(#10) Application does not have permission for this action', 'type': 'OAuthException', 'code': 10, 'fbtrace_id': 'AQkRuGBOW0IYE3YSU6M9ipp'}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'type' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\notebooks\\2023-04-08 Instagram Insights debugging.ipynb Cell 5\u001b[0m in \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-04-08%20Instagram%20Insights%20debugging.ipynb#W5sZmlsZQ%3D%3D?line=235'>236</a>\u001b[0m since \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m2023-03-01\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-04-08%20Instagram%20Insights%20debugging.ipynb#W5sZmlsZQ%3D%3D?line=236'>237</a>\u001b[0m until \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m2023-04-01\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-04-08%20Instagram%20Insights%20debugging.ipynb#W5sZmlsZQ%3D%3D?line=237'>238</a>\u001b[0m data[iteration], response[iteration] \u001b[39m=\u001b[39m get_ig_account_insights(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-04-08%20Instagram%20Insights%20debugging.ipynb#W5sZmlsZQ%3D%3D?line=238'>239</a>\u001b[0m     ig_user_id, access_token,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-04-08%20Instagram%20Insights%20debugging.ipynb#W5sZmlsZQ%3D%3D?line=239'>240</a>\u001b[0m     since\u001b[39m=\u001b[39msince, until\u001b[39m=\u001b[39muntil\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/online-PT-social-media-NLP/notebooks/2023-04-08%20Instagram%20Insights%20debugging.ipynb#W5sZmlsZQ%3D%3D?line=240'>241</a>\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: 'type' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "# @st.cache_data\n",
    "def get_ig_account_insights(ig_user_id, access_token, since=None, until=None, \n",
    "    filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\raw',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-02 16:13\n",
    "    Get the daily impressions and reach a given Instagram account.\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    \n",
    "    since_parameter = None\n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    if since_parameter:\n",
    "        url_without_token += f'&since={datetime.timestamp(since_parameter)}'\n",
    "    else:\n",
    "        url_without_token += f'&since={datetime.timestamp(since)}'\n",
    "        since_parameter = since + timedelta(days=1)\n",
    "\n",
    "    url = url_without_token+'&access_token='+access_token\n",
    "    print(url_without_token)\n",
    "    \n",
    "    response_json_dict = dict()\n",
    "    df_list = []\n",
    "    earliest_end_time = None\n",
    "    page = 1\n",
    "    while (since_parameter > since):\n",
    "        response = requests.get(url)\n",
    "        print(f'Requesting page {page}...')\n",
    "        print('\\tResponse status code: ',response.status_code)\n",
    "        response_json_dict[page] = response.json()\n",
    "        if response.status_code//100 != 2: # Stop the function if there is an error in the request\n",
    "            print(response_json_dict[page]['error'])\n",
    "            break\n",
    "        try:\n",
    "            df_list.append(\n",
    "                pd.concat([\n",
    "                json_normalize(response_json_dict[page]['data'][0], record_path='values', record_prefix='impressions_'), # Impressions: \"Total number of times the Business Account's media objects have been viewed\"\n",
    "                json_normalize(response_json_dict[page]['data'][1], record_path='values', record_prefix='reach_') # Reach: \"Total number of times the Business Account's media objects have been uniquely viewed\"\n",
    "                ], axis=1)\n",
    "            )\n",
    "        except:\n",
    "            print('No data in request response for page', page)\n",
    "        earliest_end_time = response_json_dict[page]['data'][0]['values'][0]['end_time']\n",
    "        since_parameter = datetime.strptime(re.sub(r'(.+)T.+', r'\\1', earliest_end_time), \"%Y-%m-%d\")\n",
    "        print('since_parameter: ',since_parameter)\n",
    "\n",
    "        try:\n",
    "            next_endpoint = response_json_dict[page]['paging']['previous']\n",
    "            if next_endpoint+access_token != url:\n",
    "                url = next_endpoint\n",
    "            else:\n",
    "                print('end')\n",
    "                break\n",
    "        except: \n",
    "            break\n",
    "        page +=1\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print('Number of days of data:',len(df))\n",
    "        new_response_json_dict = dict()\n",
    "        for page, response in response_json_dict.items():\n",
    "            new_response_json_dict[page] = {'data': response['data']}\n",
    "    except:\n",
    "        df = df_list \n",
    "    if filename:\n",
    "        filename += '_account_insights'\n",
    "        try:\n",
    "            savepickle(df,filename+'_df','sav',csv_path)\n",
    "            savepickle(new_response_json_dict,filename,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df, response_json_dict\n",
    "\n",
    "# @st.cache_data\n",
    "def update_ig_account_insights(ig_user_id, access_token, since=None, until=None,\n",
    "    timestamp_column_suffix='end_time', filename=None,\n",
    "    json_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\API_response',\n",
    "    csv_path=r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\online-PT-social-media-NLP\\data\\interim'):\n",
    "    \"\"\" \n",
    "    2023-03-15 1:22\n",
    "    Get the daily impressions and reach a given Instagram account. \n",
    "    Load any results that were previously saved (pull new data if no previously saved results available).\n",
    "\n",
    "    Parameters:\n",
    "        - ig_user_id: Can be obtained from Facebook Graph API explorer using this query \n",
    "            (requires business_management permission, possibly others also): \n",
    "             me/accounts?fields=instagram_business_account{id,name,username,profile_picture_url}\n",
    "        - access_token\n",
    "        - since and until (str): Date in 'yyyy-mm-dd format', e.g. '2023-01-01'. \n",
    "            Note: There cannot be more than 30 days (2592000 s) between since and until\n",
    "        - timestamp_column_suffix (str): Suffix of the timestamp columns. default is 'end_time'. \n",
    "            Required to parse out the date range of the previously saved outputs.\n",
    "        - filename (str): Filename (without extension) for saving the outputs. If None, outputs are not saved.\n",
    "            For outputs to be saved, the custom functions save_csv and savepickle must be imported.\n",
    "        - json_path and csv_path (raw string): path to which to save the json and dataframe outputs,\n",
    "            respectively.\n",
    "    \n",
    "    Returns\n",
    "        - df: DataFrame with the following information:\n",
    "            - \n",
    "        - response_json: JSON object with each page number of results as the key (starting with 1)\n",
    "    Example syntax:\n",
    "    \"\"\"\n",
    "    previous_since, previous_until = None, None\n",
    "    if filename:\n",
    "        filename2 = f'{filename}_account_insights'\n",
    "    try:\n",
    "        df = loadpickle(filename2+'_df.sav', csv_path)\n",
    "        df = df.reset_index(drop=True)\n",
    "        timestamp_column = df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]\n",
    "        df = df.sort_values(timestamp_column)\n",
    "        response_json_dict = loadpickle(filename2+'.sav', json_path)\n",
    "        previous_since = datetime.strptime(df.iloc[0][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\") # the %z format code is to indicate timezone as an offset\n",
    "        previous_until = datetime.strptime(df.iloc[-1][timestamp_column], \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        print('previous since date:', previous_since)\n",
    "        print('previous until date:', previous_until)\n",
    "    except:\n",
    "        print('Unable to load prior results; making new API calls for entire date range.')\n",
    "    \n",
    "    url_root = \"https://graph.facebook.com/v15.0/\"\n",
    "    url_without_token = f'{url_root}{ig_user_id}/insights?metric=impressions%2Creach&metric_type=time_series&period=day'\n",
    "    \n",
    "    if since:\n",
    "        if type(since) == str:\n",
    "            since = datetime.strptime(since, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time = time(0,0)\n",
    "            since = datetime.combine(since, default_time)\n",
    "    \n",
    "    if until:\n",
    "        if type(until) == str:\n",
    "            until = datetime.strptime(until, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            default_time=time(0,0)\n",
    "            until = datetime.combine(until, default_time)\n",
    "        if (until != datetime.now()) & (since != datetime.now()) & ((until - since).days > 30):\n",
    "            since_parameter = until - timedelta(days=30)\n",
    "        url_without_token += f'&until={datetime.timestamp(until)}'\n",
    "    \n",
    "    if (previous_since == None) & (previous_until == None):\n",
    "        df, response_json_dict = get_ig_account_insights(\n",
    "            ig_user_id, access_token, since=since, until=until, filename=filename,\n",
    "            json_path=json_path)\n",
    "        return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "    elif previous_since == None:\n",
    "        previous_since = since + timedelta (days=1)\n",
    "        print('Previous `since` parameter could not be found; default to since + 1.')\n",
    "    elif previous_until == None:\n",
    "        previous_until = until - timedelta (days=1)\n",
    "        print('Previous `until` parameter could not be found; default to until - 1.')\n",
    "    if (previous_since.date() > since.date()):\n",
    "        print(f'\\nFetching older account insights from {datetime.strftime(since, \"%Y-%m-%d\")} to {datetime.strftime(previous_since, \"%Y-%m-%d\")}')\n",
    "        older_insights_df, older_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token,\n",
    "            json_path=json_path, since=since, until=previous_since)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), older_insights_df])\n",
    "        except:\n",
    "            df = older_insights_df\n",
    "        try:\n",
    "            # Update the keys of *response_json_dict* before merging with older_insights_response_json_dict. That way, final \n",
    "                # response dictionary always has insights from oldest dates first\n",
    "            response_json_dict = dict( \n",
    "                zip([key+len(older_insights_response_json_dict) for key in response_json_dict.keys()], response_json_dict.values())\n",
    "                )\n",
    "            response_json_dict = {**older_insights_response_json_dict, **response_json_dict}\n",
    "        except:\n",
    "            response_json_dict = older_insights_response_json_dict\n",
    "    if (previous_until.date() < until.date()):\n",
    "        print(f'\\nFetching newer account insights from {datetime.strftime(previous_until, \"%Y-%m-%d\")} to {datetime.strftime(until, \"%Y-%m-%d\")}')\n",
    "        new_insights_df, new_insights_response_json_dict = get_ig_account_insights(ig_user_id, access_token, \n",
    "            json_path=json_path, since=previous_until, until=until)\n",
    "        try:\n",
    "            df = pd.concat([df.copy(), new_insights_df])\n",
    "        except:\n",
    "            df = new_insights_df\n",
    "        new_insights_response_json_dict = dict( # Update the keys of new_insights_response_json_dict before merging with previous dict\n",
    "            zip([key+len(response_json_dict) for key in new_insights_response_json_dict.keys()], new_insights_response_json_dict.values())\n",
    "            )\n",
    "        response_json_dict = {**response_json_dict, **new_insights_response_json_dict}\n",
    "        # remove items containing API key as this will invalidate access tokens if published to Github\n",
    "        new_response_json_dict = dict()\n",
    "        for page, response in response_json_dict.items():\n",
    "            new_response_json_dict[page] = {'data': response['data']}\n",
    "    \n",
    "    if (previous_until.date() >= until.date()) & (previous_since.date() <= since.date()):\n",
    "        print('\\nLoading previous saved results; no new API calls required.\\n')\n",
    "        new_response_json_dict = response_json_dict\n",
    "        \n",
    "    if filename:\n",
    "        try:\n",
    "            savepickle(df, filename2+'_df', 'sav', csv_path)\n",
    "            savepickle(new_response_json_dict,filename2,'sav',json_path)\n",
    "        except:\n",
    "            print('Unable to save outputs')\n",
    "    return df.sort_values(df.columns[df.columns.str.contains('_'+timestamp_column_suffix)][0]).reset_index(drop=True), response_json_dict\n",
    "\n",
    "iteration = 1\n",
    "data = dict()\n",
    "response = dict\n",
    "since = '2023-03-01'\n",
    "until = '2023-04-01'\n",
    "data[iteration], response[iteration] = get_ig_account_insights(\n",
    "    ig_user_id, access_token,\n",
    "    since=since, until=until\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting issues\n",
    "Adjust `plot_account_insights` so there won't be an error message if there are days of the week without any posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_account_insights(\n",
    "        input_df, agg='sum',\n",
    "        metric_column_suffix='value', timezone='Canada/Pacific',\n",
    "        posts_df=None,\n",
    "        streamlit=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    2023-03-02 16:07\n",
    "    \"\"\"\n",
    "    df = process_account_insights(input_df, timezone=timezone)\n",
    "    metrics = df.columns[df.columns.str.contains('_'+metric_column_suffix)].tolist()\n",
    "    try:\n",
    "        metrics += ['posts'] if len(posts_df)>0 else ''\n",
    "    except:\n",
    "        pass\n",
    "    metrics = [metric.replace('_'+metric_column_suffix, '') for metric in metrics]\n",
    "    df.columns = df.columns.str.replace('_'+metric_column_suffix, '')\n",
    "    groupby_options = ['year-month', 'year-week', 'day_of_week', 'date']\n",
    "    subplot_titles = [\n",
    "        f'{metric} per {groupby} ({agg})' for groupby in groupby_options for metric in metrics]\n",
    "    fig = make_subplots(\n",
    "        rows=len(groupby_options)*len(metrics), cols=1, subplot_titles=subplot_titles\n",
    "        )\n",
    "    row = 1\n",
    "    xtick_list = []\n",
    "    df_list = []\n",
    "    for groupby in (groupby_options):\n",
    "        df_grouped = df.filter(items=metrics+[groupby]).groupby(\n",
    "            groupby).agg(agg) if groupby !='date' else df.set_index('date')\n",
    "        if groupby == 'day_of_week':\n",
    "            day_names = df.sort_values('day_of_week')['day_of_week_name'].unique()\n",
    "            df_grouped.index = [day[:3] for day in day_names]\n",
    "            df_grouped = df_grouped/len(df['year-week'].unique())\n",
    "        df_list.append(df_grouped)\n",
    "        for metric in (metrics):\n",
    "            if (metric == 'posts'):               \n",
    "                posts_grouped = posts_df.filter(items=['caption']+[groupby]+['day_of_week_name']).groupby(\n",
    "                    groupby).agg('count') \n",
    "                if groupby == 'date':\n",
    "                    posts_grouped = posts_grouped.asfreq('D').fillna(0)\n",
    "                elif groupby == 'day_of_week':\n",
    "                    # posts_grouped.index = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "                    # print(posts_grouped.columns)\n",
    "                    day_names2 = posts_grouped.sort_values('day_of_week')['day_of_week_name'].unique()\n",
    "                    print('day_names', day_names2)\n",
    "                    posts_grouped.index = [day[:3] for day in day_names]\n",
    "                    posts_grouped = posts_grouped/(len(df['year-week'].unique()) if agg=='mean' else 1)\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    y=df_grouped[metric] if metric !='posts' else posts_grouped['caption'],\n",
    "                    x=df_grouped.index if metric !='posts' else posts_grouped.index, \n",
    "                    showlegend=False,\n",
    "                ),\n",
    "                row=row, col=1\n",
    "            )\n",
    "            row += 1\n",
    "            if len(df_grouped.index) <= 24:\n",
    "                xtick_list.append(df_grouped.index if metric != 'posts' else posts_grouped.index)\n",
    "            else:\n",
    "                xtick_list.append(None)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f'Insights (periods end at {df.loc[0,\"hour\"]}:00 {timezone} time)',\n",
    "        title_xanchor='center', title_x=0.5,\n",
    "        height = len(groupby_options)*len(metrics) * 200,\n",
    "        template='plotly'\n",
    "        )\n",
    "    # Update the xtick labels for each subplot\n",
    "    for ax in fig['layout']:\n",
    "        if ax.startswith('xaxis'):\n",
    "            subplot = int(ax[5:]) if ax[5:] else 1\n",
    "            fig['layout'][ax]['tickvals'] = xtick_list[subplot-1]\n",
    "    if streamlit:\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "    else:\n",
    "        fig.show()\n",
    "    return fig\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_account_insights(\n",
    "        input_df, agg='sum',\n",
    "        metric_column_suffix='value', timezone='Canada/Pacific',\n",
    "        posts_df=None,\n",
    "        streamlit=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    2023-03-02 16:07\n",
    "    \"\"\"\n",
    "    df = process_account_insights(input_df, timezone=timezone)\n",
    "    metrics = df.columns[df.columns.str.contains('_'+metric_column_suffix)].tolist()\n",
    "    try:\n",
    "        metrics += ['posts'] if len(posts_df)>0 else ''\n",
    "    except:\n",
    "        pass\n",
    "    metrics = [metric.replace('_'+metric_column_suffix, '') for metric in metrics]\n",
    "    df.columns = df.columns.str.replace('_'+metric_column_suffix, '')\n",
    "    groupby_options = ['year-month', 'year-week', 'day_of_week', 'date']\n",
    "    subplot_titles = [\n",
    "        f'{metric} per {groupby} ({agg})' for groupby in groupby_options for metric in metrics]\n",
    "    fig = make_subplots(\n",
    "        rows=len(groupby_options)*len(metrics), cols=1, subplot_titles=subplot_titles\n",
    "        )\n",
    "    row = 1\n",
    "    xtick_list = []\n",
    "    df_list = []\n",
    "    for groupby in (groupby_options):\n",
    "        df_grouped = df.filter(items=metrics+[groupby]).groupby(\n",
    "            groupby).agg(agg) if groupby !='date' else df.set_index('date')\n",
    "        if groupby == 'day_of_week':\n",
    "            day_names = df.sort_values('day_of_week')['day_of_week_name'].unique()\n",
    "            df_grouped.index = [day[:3] for day in day_names]\n",
    "            df_grouped = df_grouped/len(df['year-week'].unique())\n",
    "        df_list.append(df_grouped)\n",
    "        for metric in (metrics):\n",
    "            if (metric == 'posts'):\n",
    "                print('unique days of the week:', posts_df['day_of_week_name'].unique())               \n",
    "                posts_grouped = posts_df.filter(items=['caption']+[groupby]).groupby(\n",
    "                    groupby).agg('count') \n",
    "                if groupby == 'date':\n",
    "                    posts_grouped = posts_grouped.asfreq('D').fillna(0)\n",
    "                elif groupby == 'day_of_week':\n",
    "                    day_map = {1: 'Monday', 2: 'Tuesday', 3: 'Wednesday', 4: 'Thursday', 5: 'Friday', 6: 'Saturday', 7: 'Sunday'}\n",
    "                    posts_grouped = posts_grouped.rename(index=day_map)\n",
    "                    posts_grouped = posts_grouped/(len(df['year-week'].unique()) if agg=='mean' else 1)\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    y=df_grouped[metric] if metric !='posts' else posts_grouped['caption'],\n",
    "                    x=df_grouped.index if metric !='posts' else posts_grouped.index, \n",
    "                    showlegend=False,\n",
    "                ),\n",
    "                row=row, col=1\n",
    "            )\n",
    "            row += 1\n",
    "            if len(df_grouped.index) <= 24:\n",
    "                xtick_list.append(df_grouped.index if metric != 'posts' else posts_grouped.index)\n",
    "            else:\n",
    "                xtick_list.append(None)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f'Insights (periods end at {df.loc[0,\"hour\"]}:00 {timezone} time)',\n",
    "        title_xanchor='center', title_x=0.5,\n",
    "        height = len(groupby_options)*len(metrics) * 200,\n",
    "        template='plotly'\n",
    "        )\n",
    "    # Update the xtick labels for each subplot\n",
    "    for ax in fig['layout']:\n",
    "        if ax.startswith('xaxis'):\n",
    "            subplot = int(ax[5:]) if ax[5:] else 1\n",
    "            fig['layout'][ax]['tickvals'] = xtick_list[subplot-1]\n",
    "    if streamlit:\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "    else:\n",
    "        fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
